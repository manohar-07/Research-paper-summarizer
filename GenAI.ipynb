{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ce9ea08be334953b6bf999a6fe6ada5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d0e7aeda1174f13823ea9ef0ff2afe6",
              "IPY_MODEL_c60c53b5a24448b9adbb69ed90a93992",
              "IPY_MODEL_802f9d0e4f53417cae064106140e065e"
            ],
            "layout": "IPY_MODEL_d0cf02904e5b4681a24851809f233a2e"
          }
        },
        "1d0e7aeda1174f13823ea9ef0ff2afe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d10542ca38984ad0a98324b1558fb12f",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_585440e64c9f4597b477e5c7984ce300",
            "value": "tokenizer_config.json:\u2007100%"
          }
        },
        "c60c53b5a24448b9adbb69ed90a93992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64f41a26ebca4532aa9ffe35db3609d3",
            "max": 2324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_398c1fe23e284363b6f015a4b171141f",
            "value": 2324
          }
        },
        "802f9d0e4f53417cae064106140e065e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dbb71d025ea49cd8e46a41239aebc02",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_93b29d6d13fb4b0a9bcdd47cd002b916",
            "value": "\u20072.32k/2.32k\u2007[00:00&lt;00:00,\u2007224kB/s]"
          }
        },
        "d0cf02904e5b4681a24851809f233a2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d10542ca38984ad0a98324b1558fb12f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "585440e64c9f4597b477e5c7984ce300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64f41a26ebca4532aa9ffe35db3609d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "398c1fe23e284363b6f015a4b171141f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7dbb71d025ea49cd8e46a41239aebc02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93b29d6d13fb4b0a9bcdd47cd002b916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97753a604737418395aff4c108ffa887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eda6ee03d2ed43eebcd5e87d09ddc15a",
              "IPY_MODEL_0e917a4b3dd54f90a2ac871614491753",
              "IPY_MODEL_0532cf790f404eefa45007ab1df4e82e"
            ],
            "layout": "IPY_MODEL_5cc5aaba0a534dacb048766dbe57b340"
          }
        },
        "eda6ee03d2ed43eebcd5e87d09ddc15a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e25b88960fe541c6a63e0edf4e28f420",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_8f64ec3713134961b9e40525f1fdcc57",
            "value": "spiece.model:\u2007100%"
          }
        },
        "0e917a4b3dd54f90a2ac871614491753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_777addac3e3541e39ebf652c7f736605",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d65dead427994f84902128f914b7e5ba",
            "value": 791656
          }
        },
        "0532cf790f404eefa45007ab1df4e82e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05938bed8cf04192a912d63776ee3533",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_35267f0476054d1f967de14e5d71371f",
            "value": "\u2007792k/792k\u2007[00:00&lt;00:00,\u20076.51MB/s]"
          }
        },
        "5cc5aaba0a534dacb048766dbe57b340": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e25b88960fe541c6a63e0edf4e28f420": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f64ec3713134961b9e40525f1fdcc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "777addac3e3541e39ebf652c7f736605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d65dead427994f84902128f914b7e5ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05938bed8cf04192a912d63776ee3533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35267f0476054d1f967de14e5d71371f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3476f43db6549b79852ca7dd734f1d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6c36a774c994c0ab49514b7e3047cda",
              "IPY_MODEL_865ab2c6005c4dc197bcc301291822ca",
              "IPY_MODEL_1d5e9616a1be4153820fe01deff772bd"
            ],
            "layout": "IPY_MODEL_015c8b2863b548c88a41b6ee46af3ebe"
          }
        },
        "e6c36a774c994c0ab49514b7e3047cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f5e6ec4f9c2444f895ee57c141ddadb",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_be20ca3ae3b940c2a69fec5205ce1013",
            "value": "tokenizer.json:\u2007100%"
          }
        },
        "865ab2c6005c4dc197bcc301291822ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_897a7ff873224e6b88ab4f7b1c7f0c35",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6a517bf022e4d468265755c145ba96b",
            "value": 1389353
          }
        },
        "1d5e9616a1be4153820fe01deff772bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0295937399fc4d488d2631b51432a3cc",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_e2ef7401d1e34efc824914be4045bd7d",
            "value": "\u20071.39M/1.39M\u2007[00:00&lt;00:00,\u200720.3MB/s]"
          }
        },
        "015c8b2863b548c88a41b6ee46af3ebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f5e6ec4f9c2444f895ee57c141ddadb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be20ca3ae3b940c2a69fec5205ce1013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "897a7ff873224e6b88ab4f7b1c7f0c35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6a517bf022e4d468265755c145ba96b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0295937399fc4d488d2631b51432a3cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2ef7401d1e34efc824914be4045bd7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c1fc49d373743f091da47dddf358e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c887fa8ca05423cb320ae97bfabd3b9",
              "IPY_MODEL_2fbeff637aa24505bcf43abdebaeab66",
              "IPY_MODEL_10ed113627e04e7581d6d367a0dfdffe"
            ],
            "layout": "IPY_MODEL_b0fcf7a8e1214f7b9a30a6ea7775da8b"
          }
        },
        "5c887fa8ca05423cb320ae97bfabd3b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbb0f921670042e1bf26396916be3a4d",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_31cf3c5c03b74e6083e3fc3846041c35",
            "value": "Map:\u2007100%"
          }
        },
        "2fbeff637aa24505bcf43abdebaeab66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f57514db582b48359721521162062e81",
            "max": 900,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07ce8b9cf7d1467783dc0704b1120391",
            "value": 900
          }
        },
        "10ed113627e04e7581d6d367a0dfdffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_724a28e7828b4062a66361b532ae480d",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_16ae5b2c275c4e368aad7371408fa586",
            "value": "\u2007900/900\u2007[00:18&lt;00:00,\u200747.67\u2007examples/s]"
          }
        },
        "b0fcf7a8e1214f7b9a30a6ea7775da8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbb0f921670042e1bf26396916be3a4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31cf3c5c03b74e6083e3fc3846041c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f57514db582b48359721521162062e81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07ce8b9cf7d1467783dc0704b1120391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "724a28e7828b4062a66361b532ae480d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16ae5b2c275c4e368aad7371408fa586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43c2af1429dc43e38ec9c15855be21d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e81912b099745a58bc83763b26fcd68",
              "IPY_MODEL_870eaa0d069047d4ab322457f36cbee8",
              "IPY_MODEL_410d1feb88664bcebdea5de3ca16d3c3"
            ],
            "layout": "IPY_MODEL_5b20b4f029f04327b0b7745b483361d3"
          }
        },
        "5e81912b099745a58bc83763b26fcd68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3877b5c97bd24e52b12dafa4d9b1e80d",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_f0ebc3f9f3a94a9e9543af9e7e6a0cb4",
            "value": "Map:\u2007100%"
          }
        },
        "870eaa0d069047d4ab322457f36cbee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ba8a9673d2a419987d0905b845ace6f",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4da8a3cc68f441818a2da2f967cd7b46",
            "value": 100
          }
        },
        "410d1feb88664bcebdea5de3ca16d3c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d697d8bdebf432aafdcfaf0d1b90fdc",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_b539b3cd4b54487892e101a305c549fc",
            "value": "\u2007100/100\u2007[00:02&lt;00:00,\u200747.19\u2007examples/s]"
          }
        },
        "5b20b4f029f04327b0b7745b483361d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3877b5c97bd24e52b12dafa4d9b1e80d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0ebc3f9f3a94a9e9543af9e7e6a0cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ba8a9673d2a419987d0905b845ace6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4da8a3cc68f441818a2da2f967cd7b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d697d8bdebf432aafdcfaf0d1b90fdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b539b3cd4b54487892e101a305c549fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81b078190f2843e4af37469a63d803c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf5b880f9141418c89efd3667d3b5bb4",
              "IPY_MODEL_a5ffe2f072af4a7dabcca6996af438ee",
              "IPY_MODEL_ab22df832a874879b8517f8153c3a053"
            ],
            "layout": "IPY_MODEL_6c57c76f34e04087bc22f270a91ffaec"
          }
        },
        "cf5b880f9141418c89efd3667d3b5bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7416145b420c422ca8bae797b061fcf7",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_d937c5ad91a94a6186b8b4aaf96bf6f5",
            "value": "Downloading\u2007builder\u2007script:\u2007100%"
          }
        },
        "a5ffe2f072af4a7dabcca6996af438ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cead67a81c74e6b969e697b43133d12",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d201453052b41f184767183a51c97f4",
            "value": 6270
          }
        },
        "ab22df832a874879b8517f8153c3a053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c09511ed62d4acbb96b0f048ae7a649",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_2019bedb07da41019b080ab787625e26",
            "value": "\u20076.27k/6.27k\u2007[00:00&lt;00:00,\u2007219kB/s]"
          }
        },
        "6c57c76f34e04087bc22f270a91ffaec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7416145b420c422ca8bae797b061fcf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d937c5ad91a94a6186b8b4aaf96bf6f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cead67a81c74e6b969e697b43133d12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d201453052b41f184767183a51c97f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c09511ed62d4acbb96b0f048ae7a649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2019bedb07da41019b080ab787625e26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "726c835ada9f477a9e4e85d64dc42eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b14e0a54e624123b9919de6a3ee4047",
              "IPY_MODEL_13dee946b6a44b59a04d468272cfcc51",
              "IPY_MODEL_868ce64d9b1a40a7a5cbd164af2495f0"
            ],
            "layout": "IPY_MODEL_8d36c788740f4eaf811d7adc7722b8a2"
          }
        },
        "4b14e0a54e624123b9919de6a3ee4047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f21b0a0781d4fb19787bf45a07c36fd",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_6aa71f13cd114f53a9502589b3f98b0a",
            "value": "config.json:\u2007100%"
          }
        },
        "13dee946b6a44b59a04d468272cfcc51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a467ca61a19945659501666c658ea576",
            "max": 1206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae6320964231418283f1d31fdb17bdc8",
            "value": 1206
          }
        },
        "868ce64d9b1a40a7a5cbd164af2495f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af2e735997654cce9b5364ba0720318e",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_c024a3b34c674eb4978ad5396c50e3e5",
            "value": "\u20071.21k/1.21k\u2007[00:00&lt;00:00,\u200799.7kB/s]"
          }
        },
        "8d36c788740f4eaf811d7adc7722b8a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f21b0a0781d4fb19787bf45a07c36fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa71f13cd114f53a9502589b3f98b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a467ca61a19945659501666c658ea576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae6320964231418283f1d31fdb17bdc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af2e735997654cce9b5364ba0720318e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c024a3b34c674eb4978ad5396c50e3e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c352a70b75a843d89e86bfc10fe0d7b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82884b2f48b44b5d923636ca0cb3b0d7",
              "IPY_MODEL_be46ea18bea5498e876ecaf9eb66ac65",
              "IPY_MODEL_3cea227cf83e452c83e6c6374a0b7cc1"
            ],
            "layout": "IPY_MODEL_558ff144f41d4965b88d2228fa8df4d0"
          }
        },
        "82884b2f48b44b5d923636ca0cb3b0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb120e2b6f934d4f92bae1fd798e3392",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_0777d5a0a92640f881e18607feb3be8c",
            "value": "model.safetensors:\u2007100%"
          }
        },
        "be46ea18bea5498e876ecaf9eb66ac65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d2928e6bf204542971640dc8e88aa63",
            "max": 242043056,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e8c87501d794fa49dfb914afc088f24",
            "value": 242043056
          }
        },
        "3cea227cf83e452c83e6c6374a0b7cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f55b10e76acc4d508ce16a24a359e760",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_73fd88cf8b3a424681fcc48ceb0be919",
            "value": "\u2007242M/242M\u2007[00:01&lt;00:00,\u2007236MB/s]"
          }
        },
        "558ff144f41d4965b88d2228fa8df4d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb120e2b6f934d4f92bae1fd798e3392": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0777d5a0a92640f881e18607feb3be8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d2928e6bf204542971640dc8e88aa63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e8c87501d794fa49dfb914afc088f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f55b10e76acc4d508ce16a24a359e760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73fd88cf8b3a424681fcc48ceb0be919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab70e70b7a754dee82c42ce4a1546427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb23685b6a3c4886a1d162d646678f00",
              "IPY_MODEL_41a1bc43ae3d41b1a4ca094f3e62194d",
              "IPY_MODEL_6218b2368d90465ba9257d7b86e2942e"
            ],
            "layout": "IPY_MODEL_9c7f77b30fb34b64b35e4011270a735f"
          }
        },
        "eb23685b6a3c4886a1d162d646678f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a04c1aa5e61464692ecaa2b220f66df",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_6c98c63072274136b4f43af7ae4484e6",
            "value": "generation_config.json:\u2007100%"
          }
        },
        "41a1bc43ae3d41b1a4ca094f3e62194d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62131aeb063941afbdc5ef25ac2611ff",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f1cade3f25049caa002d59f7acf2e31",
            "value": 147
          }
        },
        "6218b2368d90465ba9257d7b86e2942e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf5b0f635c7645e89e2df0f47a5169e5",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_c388e4c9cb2049e8a913441e03d007b1",
            "value": "\u2007147/147\u2007[00:00&lt;00:00,\u20079.91kB/s]"
          }
        },
        "9c7f77b30fb34b64b35e4011270a735f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a04c1aa5e61464692ecaa2b220f66df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c98c63072274136b4f43af7ae4484e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62131aeb063941afbdc5ef25ac2611ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f1cade3f25049caa002d59f7acf2e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf5b0f635c7645e89e2df0f47a5169e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c388e4c9cb2049e8a913441e03d007b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "state": {}
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"neelshah18/arxivdataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtuuHDBbJhKH",
        "outputId": "47cafdb4-6060-4da0-9b48-d42d8f192c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/neelshah18/arxivdataset?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18.3M/18.3M [00:01<00:00, 10.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/neelshah18/arxivdataset/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "file_path = path + \"/arxivData.json\"\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "print(data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbKbENcUmsre",
        "outputId": "eb2780ee-563e-49f4-bbba-eb820c4b7a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'author': \"[{'name': 'Ahmed Osman'}, {'name': 'Wojciech Samek'}]\", 'day': 1, 'id': '1802.00209v1', 'link': \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1802.00209v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1802.00209v1', 'type': 'application/pdf', 'title': 'pdf'}]\", 'month': 2, 'summary': 'We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.', 'tag': \"[{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\", 'title': 'Dual Recurrent Attention Units for Visual Question Answering', 'year': 2018}, {'author': \"[{'name': 'Ji Young Lee'}, {'name': 'Franck Dernoncourt'}]\", 'day': 12, 'id': '1603.03827v1', 'link': \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1603.03827v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1603.03827v1', 'type': 'application/pdf', 'title': 'pdf'}]\", 'month': 3, 'summary': 'Recent approaches based on artificial neural networks (ANNs) have shown\\npromising results for short-text classification. However, many short texts\\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\\nand most existing ANN-based systems do not leverage the preceding short texts\\nwhen classifying a subsequent one. In this work, we present a model based on\\nrecurrent neural networks and convolutional neural networks that incorporates\\nthe preceding short texts. Our model achieves state-of-the-art results on three\\ndifferent datasets for dialog act prediction.', 'tag': \"[{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\", 'title': 'Sequential Short-Text Classification with Recurrent and Convolutional\\n  Neural Networks', 'year': 2016}, {'author': \"[{'name': 'Iulian Vlad Serban'}, {'name': 'Tim Klinger'}, {'name': 'Gerald Tesauro'}, {'name': 'Kartik Talamadupula'}, {'name': 'Bowen Zhou'}, {'name': 'Yoshua Bengio'}, {'name': 'Aaron Courville'}]\", 'day': 2, 'id': '1606.00776v2', 'link': \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1606.00776v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1606.00776v2', 'type': 'application/pdf', 'title': 'pdf'}]\", 'month': 6, 'summary': 'We introduce the multiresolution recurrent neural network, which extends the\\nsequence-to-sequence framework to model natural language generation as two\\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\\nand a sequence of natural language tokens. There are many ways to estimate or\\nlearn the high-level coarse tokens, but we argue that a simple extraction\\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\\nSuch procedure allows training the multiresolution recurrent neural network by\\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\\nthe standard log- likelihood objective w.r.t. natural language tokens (word\\nperplexity), optimizing the joint log-likelihood biases the model towards\\nmodeling high-level abstractions. We apply the proposed model to the task of\\ndialogue response generation in two challenging domains: the Ubuntu technical\\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\\ncompeting approaches by a substantial margin, achieving state-of-the-art\\nresults according to both automatic evaluation metrics and a human evaluation\\nstudy. On Twitter, the model appears to generate more relevant and on-topic\\nresponses according to automatic evaluation metrics. Finally, our experiments\\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\\nnatural language and is better able to capture long-term structure.', 'tag': \"[{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.5.1; I.2.7', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\", 'title': 'Multiresolution Recurrent Neural Networks: An Application to Dialogue\\n  Response Generation', 'year': 2016}, {'author': \"[{'name': 'Sebastian Ruder'}, {'name': 'Joachim Bingel'}, {'name': 'Isabelle Augenstein'}, {'name': 'Anders S\u00f8gaard'}]\", 'day': 23, 'id': '1705.08142v2', 'link': \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1705.08142v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1705.08142v2', 'type': 'application/pdf', 'title': 'pdf'}]\", 'month': 5, 'summary': 'Multi-task learning is motivated by the observation that humans bring to bear\\nwhat they know about related problems when solving new ones. Similarly, deep\\nneural networks can profit from related tasks by sharing parameters with other\\nnetworks. However, humans do not consciously decide to transfer knowledge\\nbetween tasks. In Natural Language Processing (NLP), it is hard to predict if\\nsharing will lead to improvements, particularly if tasks are only loosely\\nrelated. To overcome this, we introduce Sluice Networks, a general framework\\nfor multi-task learning where trainable parameters control the amount of\\nsharing. Our framework generalizes previous proposals in enabling sharing of\\nall combinations of subspaces, layers, and skip connections. We perform\\nexperiments on three task pairs, and across seven different domains, using data\\nfrom OntoNotes 5.0, and achieve up to 15% average error reductions over common\\napproaches to multi-task learning. We show that a) label entropy is predictive\\nof gains in sluice networks, confirming findings for hard parameter sharing and\\nb) while sluice networks easily fit noise, they are robust across domains in\\npractice.', 'tag': \"[{'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\", 'title': 'Learning what to share between loosely related tasks', 'year': 2017}, {'author': \"[{'name': 'Iulian V. Serban'}, {'name': 'Chinnadhurai Sankar'}, {'name': 'Mathieu Germain'}, {'name': 'Saizheng Zhang'}, {'name': 'Zhouhan Lin'}, {'name': 'Sandeep Subramanian'}, {'name': 'Taesup Kim'}, {'name': 'Michael Pieper'}, {'name': 'Sarath Chandar'}, {'name': 'Nan Rosemary Ke'}, {'name': 'Sai Rajeshwar'}, {'name': 'Alexandre de Brebisson'}, {'name': 'Jose M. R. Sotelo'}, {'name': 'Dendi Suhubdy'}, {'name': 'Vincent Michalski'}, {'name': 'Alexandre Nguyen'}, {'name': 'Joelle Pineau'}, {'name': 'Yoshua Bengio'}]\", 'day': 7, 'id': '1709.02349v2', 'link': \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1709.02349v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1709.02349v2', 'type': 'application/pdf', 'title': 'pdf'}]\", 'month': 9, 'summary': 'We present MILABOT: a deep reinforcement learning chatbot developed by the\\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\\ncompetition. MILABOT is capable of conversing with humans on popular small talk\\ntopics through both speech and text. The system consists of an ensemble of\\nnatural language generation and retrieval models, including template-based\\nmodels, bag-of-words models, sequence-to-sequence neural network and latent\\nvariable neural network models. By applying reinforcement learning to\\ncrowdsourced data and real-world user interactions, the system has been trained\\nto select an appropriate response from the models in its ensemble. The system\\nhas been evaluated through A/B testing with real-world users, where it\\nperformed significantly better than many competing systems. Due to its machine\\nlearning architecture, the system is likely to improve with additional data.', 'tag': \"[{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.5.1; I.2.7', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\", 'title': 'A Deep Reinforcement Learning Chatbot', 'year': 2017}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d8jc9XxWHCw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "\n",
        "papers = []\n",
        "for paper in data:\n",
        "    papers.append({\n",
        "        \"id\": paper.get(\"id\", \"N/A\"),\n",
        "        \"author\": paper.get(\"author\", \"N/A\"),\n",
        "        \"day\": paper.get(\"day\", \"N/A\"),\n",
        "        \"month\": paper.get(\"month\", \"N/A\"),\n",
        "        \"summary\": paper.get(\"summary\", \"N/A\"),\n",
        "        \"link\": paper.get(\"link\", \"N/A\"),\n",
        "        \"tag\": paper.get(\"tag\", \"N/A\")\n",
        "    })\n",
        "\n",
        "\n",
        "df = pd.DataFrame(papers)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "_WubgftipmDU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "9a2f6ccb-2e18-4e0b-9922-c5421fd2a26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             id                                             author  day  \\\n",
              "0  1802.00209v1  [{'name': 'Ahmed Osman'}, {'name': 'Wojciech S...    1   \n",
              "1  1603.03827v1  [{'name': 'Ji Young Lee'}, {'name': 'Franck De...   12   \n",
              "2  1606.00776v2  [{'name': 'Iulian Vlad Serban'}, {'name': 'Tim...    2   \n",
              "3  1705.08142v2  [{'name': 'Sebastian Ruder'}, {'name': 'Joachi...   23   \n",
              "4  1709.02349v2  [{'name': 'Iulian V. Serban'}, {'name': 'Chinn...    7   \n",
              "\n",
              "   month                                            summary  \\\n",
              "0      2  We propose an architecture for VQA which utili...   \n",
              "1      3  Recent approaches based on artificial neural n...   \n",
              "2      6  We introduce the multiresolution recurrent neu...   \n",
              "3      5  Multi-task learning is motivated by the observ...   \n",
              "4      9  We present MILABOT: a deep reinforcement learn...   \n",
              "\n",
              "                                                link  \\\n",
              "0  [{'rel': 'alternate', 'href': 'http://arxiv.or...   \n",
              "1  [{'rel': 'alternate', 'href': 'http://arxiv.or...   \n",
              "2  [{'rel': 'alternate', 'href': 'http://arxiv.or...   \n",
              "3  [{'rel': 'alternate', 'href': 'http://arxiv.or...   \n",
              "4  [{'rel': 'alternate', 'href': 'http://arxiv.or...   \n",
              "\n",
              "                                                 tag  \n",
              "0  [{'term': 'cs.AI', 'scheme': 'http://arxiv.org...  \n",
              "1  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...  \n",
              "2  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...  \n",
              "3  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...  \n",
              "4  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5796b3d4-a6bc-4153-b68b-746217fa2f7a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1802.00209v1</td>\n",
              "      <td>[{'name': 'Ahmed Osman'}, {'name': 'Wojciech S...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>We propose an architecture for VQA which utili...</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1603.03827v1</td>\n",
              "      <td>[{'name': 'Ji Young Lee'}, {'name': 'Franck De...</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>Recent approaches based on artificial neural n...</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1606.00776v2</td>\n",
              "      <td>[{'name': 'Iulian Vlad Serban'}, {'name': 'Tim...</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>We introduce the multiresolution recurrent neu...</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1705.08142v2</td>\n",
              "      <td>[{'name': 'Sebastian Ruder'}, {'name': 'Joachi...</td>\n",
              "      <td>23</td>\n",
              "      <td>5</td>\n",
              "      <td>Multi-task learning is motivated by the observ...</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1709.02349v2</td>\n",
              "      <td>[{'name': 'Iulian V. Serban'}, {'name': 'Chinn...</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>We present MILABOT: a deep reinforcement learn...</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5796b3d4-a6bc-4153-b68b-746217fa2f7a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5796b3d4-a6bc-4153-b68b-746217fa2f7a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5796b3d4-a6bc-4153-b68b-746217fa2f7a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-381cc591-6412-4329-b9aa-daa7c03376ca\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-381cc591-6412-4329-b9aa-daa7c03376ca')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-381cc591-6412-4329-b9aa-daa7c03376ca button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 41000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41000,\n        \"samples\": [\n          \"0809.0490v2\",\n          \"1503.00036v2\",\n          \"1711.09522v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 36157,\n        \"samples\": [\n          \"[{'name': 'Cosmin Stamate'}, {'name': 'George D. Magoulas'}, {'name': 'Michael S. C. Thomas'}]\",\n          \"[{'name': 'E. R. Vimina'}, {'name': 'K. Poulose Jacob'}]\",\n          \"[{'name': 'Manu Goyal'}, {'name': 'Moi Hoon Yap'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          13,\n          31,\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          7,\n          11,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40967,\n        \"samples\": [\n          \"Many state-of-the-art computer vision algorithms use large scale\\nconvolutional neural networks (CNNs) as basic building blocks. These CNNs are\\nknown for their huge number of parameters, high redundancy in weights, and\\ntremendous computing resource consumptions. This paper presents a learning\\nalgorithm to simplify and speed up these CNNs. Specifically, we introduce a\\n\\\"try-and-learn\\\" algorithm to train pruning agents that remove unnecessary CNN\\nfilters in a data-driven way. With the help of a novel reward function, our\\nagents removes a significant number of filters in CNNs while maintaining\\nperformance at a desired level. Moreover, this method provides an easy control\\nof the tradeoff between network performance and its scale. Per- formance of our\\nalgorithm is validated with comprehensive pruning experiments on several\\npopular CNNs for visual recognition and semantic segmentation tasks.\",\n          \"In this study, we present Swift Linked Data Miner, an interruptible algorithm\\nthat can directly mine an online Linked Data source (e.g., a SPARQL endpoint)\\nfor OWL 2 EL class expressions to extend an ontology with new SubClassOf:\\naxioms. The algorithm works by downloading only a small part of the Linked Data\\nsource at a time, building a smart index in the memory and swiftly iterating\\nover the index to mine axioms. We propose a transformation function from mined\\naxioms to RDF Data Shapes. We show, by means of a crowdsourcing experiment,\\nthat most of the axioms mined by Swift Linked Data Miner are correct and can be\\nadded to an ontology. We provide a ready to use Prot\\\\'eg\\\\'e plugin implementing\\nthe algorithm, to support ontology engineers in their daily modeling work.\",\n          \"We investigate the capacity, convexity and characterization of a general\\nfamily of norm-constrained feed-forward networks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41000,\n        \"samples\": [\n          \"[{'rel': 'related', 'href': 'http://dx.doi.org/10.4018/978-1-60566-766-9', 'type': 'text/html', 'title': 'doi'}, {'rel': 'alternate', 'href': 'http://arxiv.org/abs/0809.0490v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/0809.0490v2', 'type': 'application/pdf', 'title': 'pdf'}]\",\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1503.00036v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1503.00036v2', 'type': 'application/pdf', 'title': 'pdf'}]\",\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1711.09522v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1711.09522v2', 'type': 'application/pdf', 'title': 'pdf'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5874,\n        \"samples\": [\n          \"[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.DS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'F.2.2; I.5; J.3', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n          \"[{'term': 'physics.soc-ph', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'math.CO', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': '05C85, 68R10, 90B15, 90C35', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'G.2.2', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n          \"[{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'q-bio.NC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpXv5vzn7N1P",
        "outputId": "82a5440a-0f5e-4688-ab98-8ff7a671390d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41000, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Preprocessing of the dataset\n"
      ],
      "metadata": {
        "id": "uVhHqTNT30aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def extract_authors(author_list):\n",
        "    try:\n",
        "        authors = ast.literal_eval(author_list)\n",
        "        return \", \".join([author[\"name\"] for author in authors])\n",
        "    except:\n",
        "        return \"Unknown\"\n",
        "\n",
        "def extract_pdf_link(link_list):\n",
        "    try:\n",
        "        links = ast.literal_eval(link_list)\n",
        "        for link in links:\n",
        "            if link.get(\"type\") == \"application/pdf\":\n",
        "                return link.get(\"href\", \"\")\n",
        "        return \"\"\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def extract_tags(tag_list):\n",
        "    try:\n",
        "        tags = ast.literal_eval(tag_list)\n",
        "        return \", \".join([tag[\"term\"] for tag in tags])\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "df[\"author\"] = df[\"author\"].apply(extract_authors)\n",
        "df[\"pdf_link\"] = df[\"link\"].apply(extract_pdf_link)\n",
        "df[\"tags\"] = df[\"tag\"].apply(extract_tags)\n",
        "\n",
        "\n",
        "df.drop(columns=[\"day\", \"month\", \"link\", \"tag\"], inplace=True)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "fMlSvEPo7ak1",
        "outputId": "ccbd4643-dc1c-433b-e83b-8b1aed5aaa4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             id                                             author  \\\n",
              "0  1802.00209v1                        Ahmed Osman, Wojciech Samek   \n",
              "1  1603.03827v1                   Ji Young Lee, Franck Dernoncourt   \n",
              "2  1606.00776v2  Iulian Vlad Serban, Tim Klinger, Gerald Tesaur...   \n",
              "3  1705.08142v2  Sebastian Ruder, Joachim Bingel, Isabelle Auge...   \n",
              "4  1709.02349v2  Iulian V. Serban, Chinnadhurai Sankar, Mathieu...   \n",
              "\n",
              "                                             summary  \\\n",
              "0  We propose an architecture for VQA which utili...   \n",
              "1  Recent approaches based on artificial neural n...   \n",
              "2  We introduce the multiresolution recurrent neu...   \n",
              "3  Multi-task learning is motivated by the observ...   \n",
              "4  We present MILABOT: a deep reinforcement learn...   \n",
              "\n",
              "                            pdf_link  \\\n",
              "0  http://arxiv.org/pdf/1802.00209v1   \n",
              "1  http://arxiv.org/pdf/1603.03827v1   \n",
              "2  http://arxiv.org/pdf/1606.00776v2   \n",
              "3  http://arxiv.org/pdf/1705.08142v2   \n",
              "4  http://arxiv.org/pdf/1709.02349v2   \n",
              "\n",
              "                                                tags  \n",
              "0                cs.AI, cs.CL, cs.CV, cs.NE, stat.ML  \n",
              "1                cs.CL, cs.AI, cs.LG, cs.NE, stat.ML  \n",
              "2  cs.CL, cs.AI, cs.LG, cs.NE, stat.ML, I.5.1; I.2.7  \n",
              "3                stat.ML, cs.AI, cs.CL, cs.LG, cs.NE  \n",
              "4  cs.CL, cs.AI, cs.LG, cs.NE, stat.ML, I.5.1; I.2.7  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c723290-82da-440d-8c03-1aaa8984e1a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>author</th>\n",
              "      <th>summary</th>\n",
              "      <th>pdf_link</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1802.00209v1</td>\n",
              "      <td>Ahmed Osman, Wojciech Samek</td>\n",
              "      <td>We propose an architecture for VQA which utili...</td>\n",
              "      <td>http://arxiv.org/pdf/1802.00209v1</td>\n",
              "      <td>cs.AI, cs.CL, cs.CV, cs.NE, stat.ML</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1603.03827v1</td>\n",
              "      <td>Ji Young Lee, Franck Dernoncourt</td>\n",
              "      <td>Recent approaches based on artificial neural n...</td>\n",
              "      <td>http://arxiv.org/pdf/1603.03827v1</td>\n",
              "      <td>cs.CL, cs.AI, cs.LG, cs.NE, stat.ML</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1606.00776v2</td>\n",
              "      <td>Iulian Vlad Serban, Tim Klinger, Gerald Tesaur...</td>\n",
              "      <td>We introduce the multiresolution recurrent neu...</td>\n",
              "      <td>http://arxiv.org/pdf/1606.00776v2</td>\n",
              "      <td>cs.CL, cs.AI, cs.LG, cs.NE, stat.ML, I.5.1; I.2.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1705.08142v2</td>\n",
              "      <td>Sebastian Ruder, Joachim Bingel, Isabelle Auge...</td>\n",
              "      <td>Multi-task learning is motivated by the observ...</td>\n",
              "      <td>http://arxiv.org/pdf/1705.08142v2</td>\n",
              "      <td>stat.ML, cs.AI, cs.CL, cs.LG, cs.NE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1709.02349v2</td>\n",
              "      <td>Iulian V. Serban, Chinnadhurai Sankar, Mathieu...</td>\n",
              "      <td>We present MILABOT: a deep reinforcement learn...</td>\n",
              "      <td>http://arxiv.org/pdf/1709.02349v2</td>\n",
              "      <td>cs.CL, cs.AI, cs.LG, cs.NE, stat.ML, I.5.1; I.2.7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c723290-82da-440d-8c03-1aaa8984e1a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c723290-82da-440d-8c03-1aaa8984e1a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c723290-82da-440d-8c03-1aaa8984e1a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dd976529-9455-46e6-982b-7eb21f8a982e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd976529-9455-46e6-982b-7eb21f8a982e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dd976529-9455-46e6-982b-7eb21f8a982e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 41000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41000,\n        \"samples\": [\n          \"0809.0490v2\",\n          \"1503.00036v2\",\n          \"1711.09522v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 36157,\n        \"samples\": [\n          \"Cosmin Stamate, George D. Magoulas, Michael S. C. Thomas\",\n          \"E. R. Vimina, K. Poulose Jacob\",\n          \"Manu Goyal, Moi Hoon Yap\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40967,\n        \"samples\": [\n          \"Many state-of-the-art computer vision algorithms use large scale\\nconvolutional neural networks (CNNs) as basic building blocks. These CNNs are\\nknown for their huge number of parameters, high redundancy in weights, and\\ntremendous computing resource consumptions. This paper presents a learning\\nalgorithm to simplify and speed up these CNNs. Specifically, we introduce a\\n\\\"try-and-learn\\\" algorithm to train pruning agents that remove unnecessary CNN\\nfilters in a data-driven way. With the help of a novel reward function, our\\nagents removes a significant number of filters in CNNs while maintaining\\nperformance at a desired level. Moreover, this method provides an easy control\\nof the tradeoff between network performance and its scale. Per- formance of our\\nalgorithm is validated with comprehensive pruning experiments on several\\npopular CNNs for visual recognition and semantic segmentation tasks.\",\n          \"In this study, we present Swift Linked Data Miner, an interruptible algorithm\\nthat can directly mine an online Linked Data source (e.g., a SPARQL endpoint)\\nfor OWL 2 EL class expressions to extend an ontology with new SubClassOf:\\naxioms. The algorithm works by downloading only a small part of the Linked Data\\nsource at a time, building a smart index in the memory and swiftly iterating\\nover the index to mine axioms. We propose a transformation function from mined\\naxioms to RDF Data Shapes. We show, by means of a crowdsourcing experiment,\\nthat most of the axioms mined by Swift Linked Data Miner are correct and can be\\nadded to an ontology. We provide a ready to use Prot\\\\'eg\\\\'e plugin implementing\\nthe algorithm, to support ontology engineers in their daily modeling work.\",\n          \"We investigate the capacity, convexity and characterization of a general\\nfamily of norm-constrained feed-forward networks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41000,\n        \"samples\": [\n          \"http://arxiv.org/pdf/0809.0490v2\",\n          \"http://arxiv.org/pdf/1503.00036v2\",\n          \"http://arxiv.org/pdf/1711.09522v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5874,\n        \"samples\": [\n          \"cs.CV, cs.DS, F.2.2; I.5; J.3\",\n          \"physics.soc-ph, cs.NE, cs.SI, math.CO, 05C85, 68R10, 90B15, 90C35, G.2.2\",\n          \"cs.AI, cs.CV, cs.NE, q-bio.NC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missing_links = df[df[\"pdf_link\"].isna() | (df[\"pdf_link\"] == \"\")]\n",
        "print(f\"papers with no links: {len(missing_links)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWsqxOUo88fr",
        "outputId": "3b5ac213-7b32-44f2-e769-fdb9161e4aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "papers with no links: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"arxiv_dataset_preprocessed.csv\")"
      ],
      "metadata": {
        "id": "ctP4mk5v9xin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Took a sample of the dataset\n"
      ],
      "metadata": {
        "id": "uWoZOJPp4ADy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = df.sample(n=5000, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "5wmW9k9dwK5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests pymupdf pandas tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-aK1p3n1SkE",
        "outputId": "24abe5d6-1136-4e23-de92-b6305a60f3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removed invalid links\n"
      ],
      "metadata": {
        "id": "VKwTEFCS4Gph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import fitz  # PyMuPDF\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Step 2: Filter out invalid PDF links\n",
        "def is_valid_pdf(pdf_url):\n",
        "    try:\n",
        "        response = requests.head(pdf_url, timeout=10)\n",
        "        return response.status_code == 200\n",
        "    except requests.exceptions.RequestException:\n",
        "        return False\n",
        "\n",
        "def filter_invalid_pdfs_parallel(df):\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        results = list(executor.map(is_valid_pdf, df[\"pdf_link\"]))\n",
        "    return df[results].reset_index(drop=True)\n",
        "\n",
        "df_valid = filter_invalid_pdfs_parallel(df_sample)\n",
        "print(f\"Valid papers after checking links: {len(df_valid)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H3AYrMq04fL",
        "outputId": "6de795a6-e17c-43d6-9f08-63e7ade6df0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid papers after checking links: 4945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracted text"
      ],
      "metadata": {
        "id": "pdqEwi7j4M_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_url):\n",
        "    try:\n",
        "        response = requests.get(pdf_url, timeout=20)\n",
        "        response.raise_for_status()\n",
        "        pdf_document = fitz.open(stream=BytesIO(response.content), filetype=\"pdf\")\n",
        "        return \"\\n\".join([pdf_document[i].get_text() for i in range(len(pdf_document))])\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract text from {pdf_url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_texts_concurrently(df):\n",
        "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
        "        texts = list(executor.map(extract_text_from_pdf, df[\"pdf_link\"]))\n",
        "    df[\"paper_text\"] = texts\n",
        "    return df\n",
        "\n",
        "df_with_text = extract_texts_concurrently(df_valid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gv5Y7Pl1His",
        "outputId": "517c9a27-1c37-4812-90ec-d798c242a482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
            "\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (133 0 R)\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '597.50787pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '845.04684pt'\n",
            "\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '614.295pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '794.96999pt'\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (161 0 R)\n",
            "\n",
            "MuPDF error: syntax error: expected object number\n",
            "\n",
            "MuPDF error: format error: object is not a stream\n",
            "\n",
            "MuPDF error: format error: object is not a stream\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (365 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (540 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (302 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (437 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (529 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (667 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (191 0 R)\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '597.50787pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '845.04675pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '614.295pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '794.96999pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '597.50787pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '845.04684pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '597.50787pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '845.04684pt'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_text.to_csv(\"sample_5000_with_text.csv\", index=False)\n",
        "print(\"\u2705 Extraction completed. Saved to sample_5000_with_text.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6jkcLw918fj",
        "outputId": "9ad34386-1240-4599-b9a4-960c75d134d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Extraction completed. Saved to sample_5000_with_text.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_with_text.head(1)[\"paper_text\"].iloc[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRJ4mn2z5ihS",
        "outputId": "6fe11a3b-b0a8-480e-9370-73c924aeef07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            " \n",
            "Principal Graphs and Manifolds \n",
            " \n",
            " \n",
            "Alexander N. Gorban \n",
            "University of Leicester, United Kingdom \n",
            " \n",
            "Andrei Y. Zinovyev \n",
            "Institut Curie, Paris, France \n",
            " \n",
            " \n",
            "ABSTRACT \n",
            "In \n",
            "many \n",
            "physical, \n",
            "statistical, \n",
            "biological \n",
            "and \n",
            "other \n",
            "investigations it is desirable to approximate a system of \n",
            "points by objects of lower dimension and/or complexity. For \n",
            "this purpose, Karl Pearson invented principal component \n",
            "analysis in 1901 and found \u2018lines and planes of closest fit \n",
            "to system of points\u2019. The famous k-means algorithm solves the \n",
            "approximation problem too, but by finite sets instead of \n",
            "lines and planes. This chapter gives a brief practical \n",
            "introduction into the methods of construction of general \n",
            "principal objects, i.e. objects embedded in the \u2018middle\u2019 of \n",
            "the multidimensional data set. As a basis, the unifying \n",
            "framework of mean squared distance approximation of finite \n",
            "datasets is selected. Principal graphs and manifolds are \n",
            "constructed as generalisations of principal components and k-\n",
            "means principal points. For this purpose, the family of \n",
            "expectation/maximisation \n",
            "algorithms \n",
            "with \n",
            "nearest \n",
            "generalisations is presented. Construction of principal \n",
            "graphs with controlled complexity is based on the graph \n",
            "grammar approach. \n",
            " \n",
            " \n",
            "INTRODUCTION \n",
            " \n",
            "In many fields of science, one meets with multivariate (multidimensional) \n",
            "distributions of vectors representing some observations. These distributions are \n",
            "often difficult to analyse and make sense of due to the very nature of human brain \n",
            "which is able to visually manipulate only with the objects of dimension no more \n",
            "than three. \n",
            " \n",
            "This makes actual the problem of approximating the multidimensional vector \n",
            "distributions by objects of lower dimension and/or complexity while retaining the \n",
            "most important information and structures contained in the initial full and \n",
            "complex data point cloud. \n",
            " \n",
            "The most trivial and coarse approximation is collapsing the whole set of vectors \n",
            "into its mean point. The mean point represents the \u2017most typical\u2018 properties of the \n",
            "system, completely forgetting variability of observations. \n",
            " \n",
            "The notion of the mean point can be generalized for approximating data by more \n",
            "complex types of objects. In 1901 Pearson proposed to approximate multivariate \n",
            "distributions by lines and planes (Pearson, 1901). In this way the Principal \n",
            "\n",
            " \n",
            " \n",
            "Component Analysis (PCA) was invented, nowadays a basic statistical tool. \n",
            "Principal lines and planes go through the \u2017middle\u2018 of multivariate data distribution \n",
            "and correspond to the first few modes of the multivariate Gaussian distribution \n",
            "approximating the data.  \n",
            " \n",
            "Starting from 1950s (Steinhaus, 1956; Lloyd, 1957; and MacQueen, 1967), it was \n",
            "proposed to approximate the complex multidimensional dataset by several \u2017mean\u2018 \n",
            "points. Thus k-means algorithm was suggested and nowadays it is one of the most \n",
            "used clustering methods in machine learning (see a review presented by Xu & \n",
            "Wunsch, 2008). \n",
            " \n",
            "Both these directions (PCA and K-Means) were further developed during last \n",
            "decades following two major directions: 1) linear manifolds were generalised for \n",
            "non-linear ones (in simple words, initial lines and planes were bended and \n",
            "twisted), and 2) some links between the \u2017mean\u2018 points were introduced. This led \n",
            "to appearance of several large families of new statistical methods; the most \n",
            "famous from them are Principal Curves, Principal Manifolds and Self-Organising \n",
            "Maps (SOM). It was quickly realized that the objects that are constructed by these \n",
            "methods are tightly connected theoretically.  This observation allows now to \n",
            "develop a common framework called \u2015Construction of Principal Objects\u2016. The \n",
            "geometrical nature of these objects can be very different but all of them serve as \n",
            "data approximators of controllable complexity. It allows using them in the tasks \n",
            "of dimension and complexity reduction. In Machine Learning this direction is \n",
            "connected with terms \u2017Unsupervised Learning\u2018 and \u2017Manifold Learning.\u2018 \n",
            " \n",
            "In this chapter we will overview the major directions in the field of principal \n",
            "objects construction. We will formulate the problem and the classical approaches \n",
            "such as PCA and k-means in a unifying framework, and show how it is naturally \n",
            "generalised for the Principal Graphs and Manifolds and the most general types of \n",
            "principal objects, Principal Cubic Complexes. We will systematically introduce \n",
            "the most used ideas and algorithms developed in this field.  \n",
            " \n",
            "APPROXIMATIONS OF FINITE DATASETS \n",
            " \n",
            "Definition. Dataset is a finite set X of objects representing N multivariate \n",
            "(multidimensional) observations. These objects xi\uf0ceX, i =1\u2026N, are embedded in \n",
            "Rm and in the case of complete data are vectors xi\uf0ceRm. We will also refer to the \n",
            "individual components of xi as \n",
            "i\n",
            "kx  such that \n",
            ")\n",
            ",...,\n",
            ",\n",
            "(\n",
            "2\n",
            "1\n",
            "i\n",
            "m\n",
            "i\n",
            "i\n",
            "i\n",
            "x\n",
            "x\n",
            "x\n",
            "x\n",
            "; we can also \n",
            "represent dataset as a data matrix \n",
            "}\n",
            "{\n",
            "i\n",
            "jx\n",
            "X\n",
            ". \n",
            " \n",
            "Definition. Distance function dist(x,y) is defined for any pair of objects x, y from \n",
            "X such that three usual axioms are satisfied: dist(x,x) = 0, dist(x,y) = dist(y,x), \n",
            "dist(x,y)+dist(y,z) \u2264 dist(x,z). \n",
            " \n",
            "Definition. Mean point MF(X) for X is a vector MF\uf0ceRm such \n",
            "that \n",
            "2\n",
            "..\n",
            "1\n",
            "))\n",
            ",\n",
            "(\n",
            "dist\n",
            "(\n",
            "min\n",
            "arg\n",
            ")\n",
            "(\n",
            "N\n",
            "i\n",
            "i\n",
            "R\n",
            "F\n",
            "m\n",
            "X\n",
            "x\n",
            "y\n",
            "M\n",
            "y\n",
            ".  \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "In this form the definition of the mean point goes back to Fr\u00e9chet (1948). Notice \n",
            "that in this definition the mean point by Fr\u00e9chet can be non-unique. However, this \n",
            "definition allows multiple useful generalisations including using it in the abstract \n",
            "metric spaces. It is easy to show that in the case of complete data and the \n",
            "Euclidean distance function \n",
            "m\n",
            "i\n",
            "i\n",
            "i\n",
            "1\n",
            "2)\n",
            "(\n",
            ")\n",
            ",\n",
            "(\n",
            "dist\n",
            "y\n",
            "x\n",
            "y\n",
            "x\n",
            ", or, more generally, in the \n",
            "case of any quadratic distance function (for example, Mahalanobis distance), the \n",
            "mean point is the standard expectation\n",
            ")\n",
            "(\n",
            "1\n",
            ")\n",
            "(\n",
            "1\n",
            "X\n",
            "N\n",
            "X\n",
            "N\n",
            "i\n",
            "i\n",
            "F\n",
            "E\n",
            "x\n",
            "\u039c\n",
            ".  \n",
            " \n",
            "Definition. Orthogonal projection P(x,Y) is defined for an object x and a set (not \n",
            "necessarily finite) of vectors Y as a vector in Y such that \n",
            ")\n",
            ",\n",
            "(\n",
            "min\n",
            "arg\n",
            ")\n",
            ",\n",
            "(\n",
            "y\n",
            "x\n",
            "x\n",
            "y\n",
            "dist\n",
            "Y\n",
            "P\n",
            "Y\n",
            ". Notice that in principle, one can have non-unique \n",
            "and even infinitely many projections of x on Y. \n",
            " \n",
            "Definition. Mean squared distance MSD(X,Y) between a dataset X  and a set of \n",
            "vectors Y is defined as \n",
            "N\n",
            "i\n",
            "i\n",
            "i\n",
            "Y\n",
            "P\n",
            "N\n",
            "Y\n",
            "X\n",
            "1\n",
            "2\n",
            "))\n",
            ",\n",
            "(\n",
            ",\n",
            "(\n",
            "dist\n",
            "1\n",
            ")\n",
            ",\n",
            "MSD(\n",
            "x\n",
            "x\n",
            ". We will also \n",
            "consider a simple generalisation of MSD: weighted mean squared \n",
            "distance \n",
            "N\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "N\n",
            "i\n",
            "i\n",
            "W\n",
            "Y\n",
            "P\n",
            "w\n",
            "w\n",
            "Y\n",
            "X\n",
            "1\n",
            "2\n",
            "1\n",
            "))\n",
            ",\n",
            "(\n",
            ",\n",
            "(\n",
            "dist\n",
            "1\n",
            ")\n",
            ",\n",
            "(\n",
            "MSD\n",
            "x\n",
            "x\n",
            ", where wi > 0 is a \n",
            "weight for the object xi. \n",
            " \n",
            "Our objective in the rest of the chapter is to briefly describe the methods for \n",
            "constructing various approximations (principal objects) for a dataset X. In almost \n",
            "all cases the principal objects will be represented as a finite or infinite set of \n",
            "vectors Y\uf0ceRm such that 1) it approximates the finite dataset X in the sense of \n",
            "minimisation of MSD(X,Y), and 2) it answers some regularity conditions that will \n",
            "be discussed below. \n",
            " \n",
            "PROBABILISTIC INTERPRETATION OF STATISTICS AND NOTION OF SELF-\n",
            "CONSISTENCY \n",
            " \n",
            "In his original works, Pearson followed the principle that the only reality in data \n",
            "analysis is the dataset, embedded in a multidimensional metric space. This \n",
            "approach can be called geometrical. During the 20th century, probabilistic \n",
            "interpretation of statistics was actively developed. Accordingly to this \n",
            "interpretation, a dataset X is one particular of i.i.d. sample from a \n",
            "multidimensional probability distribution F(x) which defines a probability of \n",
            "appearance of a sample in the point x\uf0ceRm.  \n",
            " \n",
            "The probability distribution, if can be estimated, provides a very useful auxiliary \n",
            "object allowing to define many notions in the theory of statistical data analysis. In \n",
            "particular, it allows us to define principal manifolds as self-consistent objects.  \n",
            "\n",
            " \n",
            " \n",
            "The notion of self-consistency in this context was first introduced by Efron (1967) \n",
            "and developed in the works of Flury (Tarpey & Flury, 1996), where it is claimed \n",
            "to be one of the most fundamental in statistical theory. \n",
            " \n",
            "Definition. Given probability distribution F(x) and a set of vectors Y we say that \n",
            "Y is self-consistent with respect to F(x) if \n",
            ")\n",
            ")\n",
            ",\n",
            "(\n",
            "(\n",
            "y\n",
            "x\n",
            "x\n",
            "E\n",
            "y\n",
            "Y\n",
            "P\n",
            "F\n",
            " for every vector \n",
            "y\uf0ceY. In words, it means that any vector y\uf0ceY is a conditional mean expectation of \n",
            "point x under condition that x is orthogonally projected in y. \n",
            " \n",
            "The disadvantage of this definition for finite datasets is that it is not always \n",
            "possible to calculate the conditional mean, since typically for points y\uf0ceY it is only \n",
            "one or zero point projected from X. This means that for finite datasets we should \n",
            "develop coarse-grained self-consistency notion. Usually it means that for every \n",
            "point y\uf0ceY one defines some kind of neighbourhood and introduces a modified \n",
            "self-consistency with respect to this neighbourhood instead of y itself. Concrete \n",
            "implementations of this idea are described further in this chapter. In all cases, the \n",
            "effective size of the neighbourhood is a fundamental parameter in controlling the \n",
            "complexity of the resulting approximator Y. \n",
            " \n",
            " \n",
            "FOUR APPROACHES TO CLASSICAL PCA  \n",
            " \n",
            "We can define linear principal manifolds as mean squared distance data \n",
            "approximators, constructed from linear manifolds embedded in Rm. In fact, this \n",
            "corresponds to the original definition of principal lines and planes by Pearson \n",
            "(Pearson, 1901). However, PCA method was re-invented in other fields and even \n",
            "obtained different names (Karhunen-Lo\u00e8ve or KL decomposition \n",
            "(Karhunen, 1946; Lo\u00e8ve, 1955), Hotteling transform (Hotelling, 1933), Proper \n",
            "Orthogonal Decomposition (Lumley, 1967)) and others. Here we formulate four \n",
            "equivalent ways to define principal components that the user can meet in different \n",
            "applications.  \n",
            "Let us consider a linear manifold Lk of dimension k in the parametric form \n",
            "Lk = {a0 + \uf062\n",
            "1a1 + \u2026 + \uf062\n",
            "kak | \uf062\n",
            "i\uf0ceR }, a0\uf0ceRm and {a1,\u2026, ak} is a set of \n",
            "orthonormal vectors in Rm.  \n",
            " \n",
            "Definition of PCA problem #1 (data approximation by lines and planes):   \n",
            "PCA problem consists in finding such sequence Lk (k=1,2,\u2026,m-1) that the sum of \n",
            "squared distances from data points to their orthogonal projections on Lk  is \n",
            "minimal over all linear manifolds of dimension k embedded in Rm: \n",
            " \n",
            "min\n",
            ")\n",
            ",\n",
            "MSD(\n",
            "kL\n",
            "X\n",
            " (k=1,2,\u2026,m-1). \n",
            " \n",
            "Definition of PCA problem #2 (variance maximisation):  \n",
            "For a set of vectors X and for a given ai, let us construct a one-dimensional \n",
            "distribution \uf042\n",
            "i = {\uf062: \uf062\uf020\n",
            "= (x,ai), x\uf0ceX} where (\u00b7,\u00b7) denotes scalar vector product. \n",
            "Then let us define empirical variance of X along ai as Var(Bi), where Var( ) is the \n",
            "standard empirical variance. PCA problem consists in finding such Lk that the sum \n",
            "of empirical variances of X along a1,\u2026, ak would be maximal over all linear \n",
            "manifolds of dimension k embedded in Rm: \n",
            "max\n",
            ")\n",
            "(\n",
            "Var\n",
            "..\n",
            "1 k\n",
            "i\n",
            "i\n",
            ". Let us also \n",
            "\n",
            " \n",
            " \n",
            "consider an orthogonal complement {ak+1 , \u2026, am} of the basis {a1 , \u2026, ak}. Then \n",
            "an equivalent definition (minimization of residue variance) is  \n",
            "min\n",
            ")\n",
            "(\n",
            "Var\n",
            "1\n",
            "m\n",
            "k\n",
            "i\n",
            "i\n",
            ". \n",
            " \n",
            "Definition of PCA problem #3 (mean point-to-point squared distance \n",
            "maximisation): \n",
            "PCA problem consists in finding such sequence Lk that the mean point-to-point \n",
            "squared distance between the orthogonal projections of data points on Lk is \n",
            "maximal over all linear manifolds of dimension k embedded in Rm: \n",
            "max\n",
            "))\n",
            ",\n",
            "(\n",
            "),\n",
            ",\n",
            "(\n",
            "(\n",
            "dist\n",
            "1\n",
            "1\n",
            ",\n",
            "2\n",
            "N\n",
            "j\n",
            "i\n",
            "k\n",
            "j\n",
            "k\n",
            "i\n",
            "L\n",
            "P\n",
            "L\n",
            "P\n",
            "N\n",
            "x\n",
            "x\n",
            ". Having in mind that all orthogonal \n",
            "projections onto lower-dimensional space lead to contraction of all point-to-point \n",
            "distances (except for some that do not change), this is equivalent to minimisation \n",
            "of mean squared distance distortion:  \n",
            "min\n",
            "))]\n",
            ",\n",
            "(\n",
            "),\n",
            ",\n",
            "(\n",
            "(\n",
            "dist\n",
            ")\n",
            ",\n",
            "(\n",
            "[dist\n",
            "1\n",
            ",\n",
            "2\n",
            "2\n",
            "N\n",
            "j\n",
            "i\n",
            "k\n",
            "j\n",
            "k\n",
            "i\n",
            "j\n",
            "i\n",
            "L\n",
            "P\n",
            "L\n",
            "P\n",
            "x\n",
            "x\n",
            "x\n",
            "x\n",
            ". \n",
            " \n",
            "In the three above mentioned definitions, the basis vectors are defined up to an \n",
            "arbitrary rotation that does not change the manifold. To make the choice less \n",
            "ambiguous, in the PCA method the following principle is applied: given \n",
            "{a0, a1,\u2026, ak}, any \u2017embedded\u2018 linear manifold of smaller dimension s in the form \n",
            "Ls = {a0 + \uf062\n",
            "1a1 + \u2026+ \uf062\n",
            "sas| \uf062\n",
            "i\uf0ceR, s < k}, must be itself a linear principal manifold \n",
            "of dimension s for X (a flag of principal subspaces).  \n",
            "Definition of PCA problem #4 (correlation cancellation): \n",
            "Find such an orthonormal basis (a1,\u2026, as) in which the covariance matrix for x is \n",
            "diagonal. Evidently, in this basis the distributions (ai,x) and (aj,x), for i \u2260 j, have \n",
            "zero correlation. \n",
            "Definitions 1-3 were given for finite datasets while definition 4 is sensible both \n",
            "for finite datasets and random vector x.  For finite datasets the empiric correlation \n",
            "should be cancelled. The empiric principal components which annul empiric \n",
            "correlations could be considered as an approximation to the principal components \n",
            "of the random vector. \n",
            "Equivalence of the above-mentioned definitions in the case of complete data and \n",
            "Euclidean space follows from Pythagorean Theorem and elementary algebra. \n",
            "However, in practice this or that definition can be more useful for computations or \n",
            "generalisations of the PCA approach. Thus, only definitions #1 and #3 are suitable \n",
            "for working with incomplete data since they are defined with use of only distance \n",
            "function that can be easily calculated for the \u2017gapped\u2018 data vectors (see further). \n",
            "The definition #1 can be generalized by weighting data points (Cochran & Horne, \n",
            "1977), while the definition #3 can be generalized by weighting pairs of data points \n",
            "(Gabriel & Zamir, 1979). More details about PCA and generalisations could be \n",
            "found in the fundamental book by Jollliffe (2002). \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "BASIC EXPECTATION/MAXIMISATION ITERATIVE \n",
            "ALGORITHM \n",
            "FOR \n",
            "FINDING \n",
            "PRINCIPAL OBJECTS \n",
            " \n",
            "Most of the algorithms for finding principal objects for a given dataset X are \n",
            "constructed accordingly to the classical expectation/maximisation (EM) splitting \n",
            "scheme that was first formulated as a generic method by Dempster et al (1977): \n",
            " \n",
            "Generic Expectation-Maximisation algorithm for estimating principal objects \n",
            " \n",
            "1) Initialisation step. Some initial configuration of the principal object Y is \n",
            "generated;  \n",
            "2) Expectation (projection) step. Given configuration of Y, calculate \n",
            "orthogonal projections P(x,Y), for all x\uf0ceX; \n",
            "3) Maximisation step. Given the calculated projections, find more optimal \n",
            "configuration of Y with respect to X. \n",
            "4) (Optional) adaptation step. Using some strategy, change the properties \n",
            "of Y (typically, add or remove points to Y). \n",
            "5) Repeat steps 2-4 until some convergence criteria would be satisfied. \n",
            " \n",
            "For example, for the principal line, we have the following implementation of the \n",
            "above mentioned bi-iteration scheme (Bauer, 1957; for generalisations see works \n",
            "of Roweis (1998) and Gorban & Rossiev (1999)). \n",
            " \n",
            "Iterative algorithm for calculating the first principal component \n",
            " \n",
            "1) Set a0 = MF(X) (i.e., zero order principal component is the mean \n",
            "point of X); \n",
            "2) Choose randomly a1; \n",
            "3) Calculate \n",
            "2\n",
            "1\n",
            "1\n",
            "0\n",
            ")\n",
            ",\n",
            "(\n",
            "a\n",
            "a\n",
            "a\n",
            "xi\n",
            "ib\n",
            ", i =1\u2026N ; \n",
            "4) Given bi, find new a1, such that \n",
            "min\n",
            ")\n",
            "(\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "0\n",
            "a\n",
            "a\n",
            "a\n",
            "x\n",
            "N\n",
            "i\n",
            "i\n",
            "i\n",
            "b\n",
            ", i.e. \n",
            "N\n",
            "i\n",
            "i\n",
            "N\n",
            "i\n",
            "i\n",
            "N\n",
            "i\n",
            "i\n",
            "i\n",
            "b\n",
            "b\n",
            "b\n",
            "..\n",
            "1\n",
            "2\n",
            "..\n",
            "1\n",
            "0\n",
            "..\n",
            "1\n",
            "1\n",
            "a\n",
            "x\n",
            "a\n",
            ";  \n",
            "5) Re-normalize \n",
            "||\n",
            "||\n",
            ":\n",
            "1\n",
            "1\n",
            "1\n",
            "a\n",
            "a\n",
            "a\n",
            ". \n",
            "6) Repeat steps 3-5 until the direction of a1 do not change more than on some \n",
            "small angle \uf065. \n",
            " \n",
            "Remark. To calculate all other principal components, deflation approach is \n",
            "applied: after finding a1, one calculates new X(1) = X - a0 - a1(x,a1), and the \n",
            "procedure is repeated for X(1). \n",
            " \n",
            "Remark. The basic EM procedure has good convergence properties only if the \n",
            "first eigenvalues of the empirical covariance matrix XTX are sufficiently well \n",
            "separated. If this is not the case, more sophisticated approaches are needed \n",
            "(Bau & Trefethen, 1997). \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "The PCA method can be treated as spectral decomposition of the symmetric and \n",
            "positively defined empirical covariance data matrix (defined in the case of \n",
            "complete data) \n",
            "X\n",
            "X\n",
            "N\n",
            "C\n",
            "T\n",
            "1\n",
            "1\n",
            "  or \n",
            "N\n",
            "k\n",
            "k\n",
            "j\n",
            "k\n",
            "i\n",
            "ij\n",
            "x\n",
            "x\n",
            "N\n",
            "C\n",
            "1\n",
            "1\n",
            "1\n",
            ", where without loss of \n",
            "generality we suppose that the data are centered. \n",
            " \n",
            "Definition. We call \uf073 \uf03e \uf030 a singular value for the data matrix X iff there exist two \n",
            "vectors of unit length a\uf073 and b\uf073 such that \n",
            "T\n",
            "X\n",
            "b\n",
            "a\n",
            " and \n",
            "T\n",
            "X\n",
            "a\n",
            "b\n",
            ". Then the \n",
            "vectors a\uf073\uf020\uf03d\n",
            " {\n",
            ")\n",
            "(\n",
            ")\n",
            "(\n",
            "1\n",
            ",\n",
            ",\n",
            "m\n",
            "a\n",
            "a\n",
            "\uf04c\n",
            "} and b\uf073\uf020\uf03d\n",
            " {\n",
            ")\n",
            "(\n",
            ")\n",
            "(\n",
            "1\n",
            ",\n",
            ",\n",
            "N\n",
            "b\n",
            "b\n",
            "\uf04c\n",
            "} are called left and right \n",
            "singular vectors for the singular value \uf073. \n",
            " \n",
            "If we know all p singular values of X, where p = rank(X) \u2264 min(N, m), then we can \n",
            "represent X as \n",
            "p\n",
            "l\n",
            "l\n",
            "l\n",
            "l\n",
            "X\n",
            "1\n",
            ")\n",
            "(\n",
            ")\n",
            "( a\n",
            "b\n",
            "or \n",
            "p\n",
            "l\n",
            "l\n",
            "i\n",
            "l\n",
            "k\n",
            "l\n",
            "k\n",
            "i\n",
            "a\n",
            "b\n",
            "x\n",
            "1\n",
            ")\n",
            "(\n",
            ")\n",
            "(\n",
            ". It is called the singular value \n",
            "decomposition (SVD) of X. It is easy to check that the vectors a\uf028l\uf029\uf020\uf020correspond to \n",
            "the principal vectors of X and the eigenvectors of the empirical covariance matrix \n",
            "C, whereas b\uf028l\uf029 contain projections of N points onto the corresponding principal \n",
            "vector. Eigenvalues \uf06c\n",
            "l of C and singular values \uf073\n",
            "l of X and are connected by\n",
            "2)\n",
            "(\n",
            "1\n",
            "1\n",
            "l\n",
            "l\n",
            "N\n",
            ". \n",
            "The mathematical basis for SVD was introduced by Sylvester (1889) and it \n",
            "represents a solid mathematical foundation for PCA (Strang, 1993). Although \n",
            "formally the problems of spectral decomposition of X and eigen decomposition of \n",
            "C are equivalent, the algorithms for performing singular decomposition directly \n",
            "(without explicit calculation of C) can be more efficient and robust \n",
            "(Bau III & Trefethen, 1997). Thus, the iterative EM algorithm for calculating the \n",
            "first principal component described in the previous chapter indeed performs \n",
            "singular decomposition (for centered data we simply put a0 = 0) and finds right \n",
            "singular (principal) and left singular vectors one by one. \n",
            "K-MEANS AND PRINCIPAL POINTS  \n",
            " \n",
            "K-means clustering goes back to 1950s (Steinhaus (1956); Lloyd (1957); and \n",
            "MacQueen (1967)). It is another extreme in its simplicity case of finding a \n",
            "principal object. In this case it is simply an unstructured finite (and usually, much \n",
            "smaller than the number of points N in the dataset X) set of vectors (centroids). \n",
            "One can say that the solution searched by the k-means algorithm is a set of k \n",
            "principal points (Flury, 1990). \n",
            "Definition. A set of k points Y={y1,..,yk}, yi\uf0ceRm is called a set of principal points \n",
            "for dataset X if it approximates X with minimal mean squared distance error over \n",
            "all sets of k-points in Rm (distortion):\n",
            "min\n",
            "))\n",
            ",\n",
            "(\n",
            ",\n",
            "(\n",
            "dist 2\n",
            "X\n",
            "Y\n",
            "P\n",
            "x\n",
            "x\n",
            "x\n",
            ", where P(x,Y) is \n",
            "the point from Y closest to x. Note that the set of principal points can be not \n",
            "unique. \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "The simplest implementation of the k-means procedure follows the classical EM \n",
            "scheme: \n",
            " \n",
            "Basic k-means algorithm \n",
            " \n",
            "1) Choose initial position of y1,..,yk randomly from xi\uf0ceX (with equal \n",
            "probabilities); \n",
            "2) Partition X into subsets Ki, i=1..k of data points by their proximity to yk: \n",
            ")}\n",
            ",\n",
            "(\n",
            "dist\n",
            "min\n",
            "arg\n",
            ":\n",
            "{\n",
            "j\n",
            "Y\n",
            "i\n",
            "i\n",
            "j\n",
            "K\n",
            "y\n",
            "x\n",
            "y\n",
            "x\n",
            "y\n",
            "; \n",
            "3) Re-estimate \n",
            "i\n",
            "K\n",
            "i\n",
            "i\n",
            "K\n",
            "x\n",
            "x\n",
            "y\n",
            "|\n",
            "|\n",
            "1\n",
            ", i = 1..k; \n",
            "4) Repeat steps 2-3 until complete convergence. \n",
            " \n",
            "The method is sensitive to the initial choice of y1,..,yk . Arthur & Vassilvitskii \n",
            "(2007) demonstrated that the special construction of probabilities instead of \n",
            "equidistribution gives serious advantages. The first centre, y1, they select \n",
            "equiprobable from X. Let the centres y1,..,yj are chosen (j < k) and D(x) be the \n",
            "squared shortest distance from a data point x to the closest centre we have already \n",
            "chosen. Then, we select the next centre, yj+1, from xi\uf0ceX with probability  \n",
            ")\n",
            "(\n",
            ")\n",
            "(\n",
            ")\n",
            "(\n",
            "x\n",
            "x\n",
            "x\n",
            "x X\n",
            "i\n",
            "i\n",
            "D\n",
            "D\n",
            "p\n",
            ".  \n",
            " \n",
            "Evidently, any solution of k-means procedure converges to a self-consistent set of \n",
            "points Y={y1,..,yk} (because Y = E[P(X,Y)]), but this solution may give a local \n",
            "minimum of distortion and is not necessary the set of principal points (which is \n",
            "the globally optimal approximator from all possible k-means solutions).  \n",
            " \n",
            "Multiple generalisations of k-means scheme have been developed (see, for \n",
            "example, a book of Mirkin (2005) based on the idea of \u2017data recovering\u2018). The \n",
            "most computationally expensive step of the algorithm, partitioning the dataset by \n",
            "proximity to the centroids, can be significantly accelerated using kd-tree data \n",
            "structure (Pelleg & Moore, 1999). Analysis of the effectiveness of EM algorithm \n",
            "for the k-means problem was given by Ostrovsky et al. (2006).  \n",
            " \n",
            "Notice that the case of principal points is the only in this chapter when self-\n",
            "consistency and coarse-grained self-consistency coincide: centroid yk is the \n",
            "conditional mean point for the data points belonging to the Voronoi region \n",
            "associated with yk. \n",
            " \n",
            "LOCAL PCA  \n",
            " \n",
            "The term \u2017Local PCA\u2018 was first used by Braverman (1970) and \n",
            "Fukunaga & Olsen (1971) to denote the simplest cluster-wise PCA approach \n",
            "which consists in 1) applying k-means or other type of clustering to a dataset and \n",
            "2) calculating the principal components for each cluster separately. However, this \n",
            "simple idea performs rather poorly in applications, and more interesting approach \n",
            "consists in generalizing k-means by introducing principal hyperplane segments \n",
            "proposed by Diday (1979) and called \u2017k-segments\u2018 or local subspace analysis in a \n",
            "\n",
            " \n",
            " \n",
            "more advanced version (Liu, 2003). The algorithm for their estimation follows the \n",
            "classical EM scheme.  \n",
            " \n",
            "Further development of the local PCA idea went in two main directions. First, \n",
            "Verbeek (2002) proposed a variant of the \u2017k-segment\u2018 approach for one-\n",
            "dimensional segments accompanied by a strategy to assemble disconnected line \n",
            "segments into the global piecewise linear principal curve. Einbeck et al (2008) \n",
            "proposed an iterative cluster splitting and joining approach (recursive local PCA) \n",
            "which helps to select the optimal number and configuration of disjoined segments.  \n",
            " \n",
            "Second direction is associated with a different understanding of \u2017locality\u2018. It \n",
            "consists in calculating local mean points and local principal directions and \n",
            "following them starting from (may be multiple) seed points. Locality is introduced \n",
            "using kernel functions defining the effective radius of neighborhood in the data \n",
            "space. Thus, Delicado (2001) introduced principal oriented points (POP) based on \n",
            "the variance maximisation-based definition of PCA (#2 in our chapter). POPs are \n",
            "different from the principal points introduced above because they are defined \n",
            "independently one from another, while the principal points are defined globally, as \n",
            "a set. POPs can be assembled into the principal curves of oriented points (PCOP). \n",
            "Einbeck (2005) proposed a simpler approach based on local tracing of principal \n",
            "curves by calculating local centers of mass and the local first principal \n",
            "components. \n",
            " \n",
            " \n",
            "SOM APPROACH FOR PRINCIPAL MANIFOLD APPROXIMATION AND ITS  \n",
            " GENERALISATIONS  \n",
            " \n",
            "Kohonen in his seminal paper (Kohonen, 1982) proposed to modify the k-means \n",
            "approach by introducing connections between centroids such that a change in the \n",
            "position of one centroid would also change the configuration of some neighboring \n",
            "centroids. Thus Self-Organizing Maps (SOM) algorithm was developed. \n",
            " \n",
            "With the SOM algorithm (Kohonen, 1982) we take a finite metric space V with \n",
            "metric \u03c1 and try to map it into Rm with combinations of two criteria: (1) the best \n",
            "preservation of initial structure in the image of V and (2) the best approximation \n",
            "of the dataset X. In this way, SOMs give the most popular approximations for \n",
            "principal manifolds: we can take for V a fragment of a regular s-dimensional grid \n",
            "and consider the resulting SOM as the approximation to the s-dimensional \n",
            "principal manifold (Mulier & Cherkassky, 1995; Ritter et al, 1992; Yin H. 2008). \n",
            " \n",
            "The SOM algorithm has several setup variables to regulate the compromise \n",
            "between these goals. In the original formulation by Kohonen, we start from some \n",
            "initial approximation of the map, \uf0661: V \uf0ae\n",
            " Rm. Usually this approximation lies on \n",
            "the s-dimensional linear principal manifold. On each k-th step of the algorithm we \n",
            "have a chosen datapoint x\uf0ceX and a current approximation \uf066k: V \uf0ae\n",
            " Rm. For these x \n",
            "and \uf066k we define an \u2017owner\u2018 of x in V: \n",
            ")\n",
            "(\n",
            "min\n",
            "arg\n",
            "v\n",
            "v\n",
            "k\n",
            "V\n",
            "v\n",
            "x\n",
            "x\n",
            ". The next \n",
            "approximation, \uf066k+1, is \uf066k+1(v) = hk\uf0b4w(\u03c1(v,vx))(x \u2212 \uf066k(v)). Here hk is a step size, 0 \u2264 \n",
            "w(\u03c1(v,vx)) \u2264 1 is a monotonically decreasing neighborhood function. This process \n",
            "proceeds in several epochs, with neighborhood radius decreasing during each next \n",
            "epoch. \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "The idea of SOM is flexible, was applied in many domains of science, and it lead \n",
            "to multiple generalizations (see the review paper by Yin (2008)). Some of the \n",
            "algorithms for constructing SOMs are of EM type described above, such as the \n",
            "Batch SOM Algorithm (Kohonen, 1997): it includes projecting step exactly the \n",
            "same as in k-means and the maximization step at which all \uf066k(v) are modified \n",
            "simultaneously. \n",
            " \n",
            "One source of theoretical dissatisfaction with SOM is that it is not possible to \n",
            "define an optimality criterion (Erwin et al, 1992): SOM is a result of the algorithm \n",
            "at work and there does not exist any objective function that is minimized by the \n",
            "training process. \n",
            " \n",
            "In attempt to resolve this issue, Bishop et al. (1998) developed the optimization-\n",
            "based Generative Topographic Mapping (GTM) method. In this setting, it is \n",
            "supposed that the observed data is i.i.d. sample from a mixture of Gaussian \n",
            "distributions with the centers aligned along a two-dimensional grid, embedded in \n",
            "the data space. Parameters of this mixture are determined by EM-based \n",
            "maximization of the likelihood function (probability of observing X within this \n",
            "data model).  \n",
            " \n",
            "PRINCIPAL MANIFOLDS BY HASTIE AND STUELZE \n",
            " \n",
            "Principal curves and principal two-dimensional surfaces for a probability \n",
            "distribution F(x) were introduced in the PhD thesis by Trevor Hastie (1984) as a \n",
            "self-consistent (non-linear) one- and two-dimensional globally parametrisable \n",
            "smooth manifolds without self-intersections. \n",
            " \n",
            "Definition. Let G be the class of differentiable 1-dimensional curves in Rm, \n",
            "parameterized by \uf06c\uf0ce\n",
            "R1 and without self-intersections. The Principal Curve of the \n",
            "probability distribution F(x) is such a Y(\uf06c)\uf0ceG that is self-consistent. \n",
            " \n",
            "Remark. Usually, a compact subset of Rm and a compact interval of parameters \uf06c\uf0ce\n",
            "R1 are considered. To discuss unbounded regions, it is necessary to add a \n",
            "condition that Y(\uf06c) has finite length inside any bounded subset of Rm \n",
            "(K\u00e9gl, 1999).   \n",
            " \n",
            "Definition. Let G2 be the class of differentiable 2-dimensional surfaces in Rm, \n",
            "parameterized by \uf06c\uf0ce\n",
            "R2 and without self-intersections. The Principal Surface of \n",
            "the probability distribution F(x) is such a Y(\uf06c)\uf0ceG2 that is self-consistent. (Again, \n",
            "for unbounded regions it is necessary to assume that for any bounded set B from \n",
            "Rm the set of parameters \uf06c\uf020\n",
            "for which Y(\uf06c)\uf0ceB is also bounded.) \n",
            " \n",
            "First, Hastie and Stuelze proposed an algorithm for finding the principal curves \n",
            "and principal surfaces for a probability distribution F(x), using the classical EM \n",
            "splitting. We do not provide this algorithm here because for a finite dataset X it \n",
            "can not be directly applied because in a typical point on Y(\uf06c) only zero or one data \n",
            "point is projected, hence, one can not calculate  the expectation. As mentioned \n",
            "above, in this case we should use some kind of coarse-grained self-consistency. In \n",
            "the original approach by Hastie (1984), this is done through introducing \n",
            "\n",
            " \n",
            " \n",
            "smoothers. This gives the practical formulation of the HS algorithm for estimating \n",
            "the principal manifolds from a finite dataset X: \n",
            " \n",
            "Hastie and Stuelze algorithm for finding principal curve for finite dataset \n",
            " \n",
            "1) Initialize Y(\uf06c) = a0+\uf06ca\uf031\uf020\uf02c\uf020\n",
            "where a0 is a mean point and a1 is the first \n",
            "principal component; \n",
            "2) Project every data point xi onto Y(\uf06c): i.e., for each xi find \uf06c\n",
            "i such that \n",
            "2\n",
            "\u03bb\n",
            "||\n",
            ")\n",
            "(\n",
            "||\n",
            "inf\n",
            "arg\n",
            ")\n",
            "(\n",
            "i\n",
            "i\n",
            "Y\n",
            "Y\n",
            "x\n",
            ". In practice it requires interpolation \n",
            "procedure because Y(\uf06c) is determined in a finite number of points \n",
            "{\uf06c\uf031\uf02c\uf02e\uf02e\uf02e\uf02c\uf06c\n",
            "N}. The simplest is the piecewise interpolation procedure, but more \n",
            "sophisticated procedures can be proposed (Hastie, 1984); \n",
            "3) Calculate new Y\u2032(\uf06c) in the finite number of internal coordinates {\uf06c\uf031\uf02c\uf02e\uf02e\uf02e\uf02c\uf06c\n",
            "N} \n",
            "(found at the previous step) as the local average of points xi and some \n",
            "other points, that have close to \uf06c\n",
            "i projections onto Y. To do this, 1) a span \n",
            "[w\uf0b4N] is defined ( [.] here is integer part ), where 0 < w << 1 is a \n",
            "parameter of the method (coarse-grained self-consistency neighbourhood \n",
            "radius); 2) for [w\uf0b4N] internal coordinates \n",
            "}\n",
            ",...,\n",
            "{\n",
            "N]\n",
            "[w\n",
            "1\n",
            "i\n",
            "i\n",
            " closest to \uf06c\n",
            "i and \n",
            "the corresponding \n",
            "}\n",
            ",...,\n",
            "{\n",
            "N]\n",
            "[w\n",
            "1\n",
            "i\n",
            "i\n",
            "x\n",
            "x\n",
            " calculate weighted least squares linear \n",
            "regression y(\uf06c) = a(i)\uf06c\uf02b\n",
            "b(i); 3) define Y\u2032(\uf06c\n",
            "i) as the value of the linear \n",
            "regression in \uf06c\n",
            "i: Y\u2032(\uf06c\n",
            "i) = a(i)\uf06c\n",
            "i\uf02bb(i).  \n",
            "4) Reassign Y(\uf06c) \u2190 Y\u2032(\uf06c) \n",
            "5) Repeat steps 2)-4) until Y does not change (approximately). \n",
            "Remark. For the weights in the regression at the step 3) Hastie proposed to use \n",
            "some symmetric kernel function that vanishes on the borders of the \n",
            "neighbourhood. For example, for xi let us denote as \n",
            "N]\n",
            "[w\n",
            "i\n",
            "the most distant value \n",
            "of the internal coordinate from [w\uf0b4N] ones closest to \uf06c\n",
            "i. Then we can define \n",
            "weight for the pair (\n",
            "j\n",
            "j ,\n",
            "i\n",
            "i x ) as \n",
            "otherwise.\n",
            ",0\n",
            ")\n",
            ")\n",
            "(|\n",
            "-\n",
            "(1\n",
            "3\n",
            "1\n",
            "3\n",
            "|,\n",
            "\u03bb\n",
            "|\u03bb\n",
            "|\n",
            "\u03bb\n",
            "|\u03bb\n",
            "if\n",
            ",\n",
            "|\n",
            "\u03bb\n",
            "|/|\u03bb\n",
            "\u03bb\n",
            "\u03bb\n",
            "N\n",
            "j\n",
            "j\n",
            "N\n",
            "j\n",
            "j\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "/\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "j\n",
            " \n",
            "Remark. At the step 3) an alternative approach was also proposed with use of \n",
            "cubic splines to approximate the smooth function Y\u2032(\uf06c) from all pairs \n",
            "(\uf06c\n",
            "i,xi), i = 1..N. \n",
            "Non-linear Principal Manifolds constructed by this algorithm are usually called \n",
            "Hastie-Stuelze (HS) principal manifolds. However, the global optimality of HS \n",
            "principal manifolds is not guaranteed (only self-consistency in the case of \n",
            "distribution or coarse-grained self-consistency in the case of dataset is guaranteed \n",
            "by construction). For example, the second principal component of a sample X \n",
            "from a normal distribution is self-consistent and will be correct HS principal curve \n",
            "but of course not the optimal one. \n",
            "We should also underline that our view on what is the object constructed by the \n",
            "HS algorithm for a dataset X depends on 1) probabilistic interpretation of the \n",
            "nature of X, and 2) the chosen heuristic approach to coarse-grained self-\n",
            "\n",
            " \n",
            " \n",
            "consistency. If we do not suppose that the dataset is generated by i.i.d. sampling \n",
            "from F(x) then the definition of HS principal manifold is purely operational: HS \n",
            "principal manifold for X is the result of application of HS algorithm for finite \n",
            "datasets. Analogous remark is applicable for all principal manifold approximators \n",
            "constructed for finite datasets and described further in this chapter.  \n",
            "In his PhD thesis Hastie noticed that the HS principal curve does not coincide \n",
            "with the generating curve in a very simple additive data generation model  \n",
            " \n",
            "X = f(\uf06c)+\uf065,  \n",
            " \n",
            " \n",
            " \n",
            "(1) \n",
            " \n",
            "where f(\uf06c) is some curve embedded in data space and \uf065\uf020is noise distribution \n",
            "independent on \uf06c\uf02e\uf020\n",
            "Because of the fact that if f(\uf06c) is not a straight line then it is not \n",
            "self-consistent, HS principal curves were claimed to be \u2017biased\u2018. This inspired \n",
            "Tibshirani (1992) to introduce an alternative definition of the principal curve, \n",
            "based directly on a continuous mixture model (1) and maximising regularized \n",
            "likelihood. \n",
            " \n",
            "K\u00c9GL-KRYZHAK IMPROVEMENT  \n",
            " \n",
            "K\u00e9gl in his PhD thesis supervised by Kryzhak (K\u00e9gl, 1999) revised the existing \n",
            "methods for estimating the principal curves. In particular, this led to the definition \n",
            "of principal curves with limited length. \n",
            " \n",
            "Definition. Principal curve YL(\uf06c) of length L is such a curve that the mean \n",
            "squared distance from the dataset X to the curve YL(\uf06c) is minimal over all curves \n",
            "of length less than or equal to L: \n",
            "min\n",
            "))\n",
            ",\n",
            "(\n",
            ",\n",
            "(\n",
            "dist\n",
            "1\n",
            "2\n",
            "N\n",
            "i\n",
            "L\n",
            "i\n",
            "i\n",
            "Y\n",
            "P x\n",
            "x\n",
            ". \n",
            "Theorem. Assume that X has finite second moments, i.e. \n",
            "N\n",
            "i\n",
            "T\n",
            "i\n",
            "i\n",
            "1\n",
            ")\n",
            "(x\n",
            "x\n",
            ". Then for \n",
            "any L > 0 there exists a principal curve of length L. \n",
            " \n",
            "Principal curves of length L as defined by K\u00e9gl, are globally optimal \n",
            "approximators as opposite to the HS principal curves that are only self-consistent. \n",
            "However, all attempts to construct a practical algorithm for finding globally \n",
            "optimal principal curves of length L were not successful. Instead K\u00e9gl developed \n",
            "an efficient heuristic Polygonal line algorithm for constructing piecewise linear \n",
            "principal curves.  \n",
            " \n",
            "Let us consider a piecewise curve Y composed from vertices located in points \n",
            "{y1,\u2026,yk+1} and k segments connecting pairs of vertices {yj,yj+1}, j=1..k. K\u00e9gl\u2018s \n",
            "algorithm searches for a (local) optimum of the penalised mean squared distance \n",
            "error function: \n",
            " \n",
            "1\n",
            "1\n",
            ")\n",
            "CP(\n",
            "1\n",
            ")\n",
            ",\n",
            "MSD(\n",
            ")\n",
            ",\n",
            "(\n",
            "k\n",
            "i\n",
            "i\n",
            "k\n",
            "Y\n",
            "X\n",
            "Y\n",
            "X\n",
            "U\n",
            ", \n",
            " \n",
            " \n",
            "(2) \n",
            " \n",
            "where CP(i) is a curvature penalty function for a vertex i chosen as \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "1\n",
            "if\n",
            "1\n",
            "1\n",
            "if\n",
            "))\n",
            "(\n",
            "cos\n",
            "1(\n",
            "1\n",
            "if\n",
            ")\n",
            "(\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "k\n",
            "i\n",
            "k\n",
            "i\n",
            "i\n",
            "r\n",
            "i\n",
            "i\n",
            "CP\n",
            "k\n",
            "k\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            "    , \n",
            " \n",
            "where \n",
            "||\n",
            "||||\n",
            "||\n",
            ")\n",
            ",\n",
            "(\n",
            ")\n",
            "(\n",
            "cos\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "i\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            " is the cosines of the angle between two \n",
            "neighbouring segments at the vertex i, \n",
            "))\n",
            "(\n",
            ",\n",
            "dist(\n",
            "max\n",
            "X\n",
            "r\n",
            "X\n",
            "F\n",
            "x\n",
            "M\n",
            "x\n",
            " is the \u2017radius\u2018 \n",
            "of the dataset X, and \uf06c is a parameter controlling the curve global smoothness. \n",
            " \n",
            "The Polygonal line algorithm (K\u00e9gl, 1999) follows the standard EM splitting \n",
            "scheme: \n",
            " \n",
            "Polygonal line algorithm for estimating piece-wise linear principal curve \n",
            " \n",
            "1) The initial approximation is constructed as a segment of principal line. The  \n",
            "length of the segment is the difference between the maximal and the \n",
            "minimal projection value of X onto the first principal component. The \n",
            "segment is positioned such that it contains all of the projected data points. \n",
            "Thus in the initial approximation one has two vertices {y1,y2} and one \n",
            "segment between them (k = 1). \n",
            "2) Projection step. The dataset X is partitioned into 2k+1 \n",
            ")}\n",
            ",\n",
            "(\n",
            "dist\n",
            "min\n",
            "arg\n",
            ":\n",
            "{\n",
            "segments\n",
            "vertices\n",
            "z\n",
            "z\n",
            "K\n",
            "z\n",
            "z\n",
            "x\n",
            "x\n",
            " subsets constructed by their \n",
            "proximity to k+1 vertices and k segments. If a segment i and a vertex j are \n",
            "equally distant from x then x is placed into Kj only. \n",
            "3) Optimisation step. Given partitioning obtained at the step 2, the functional \n",
            "U(X,Y) is optimised by use of a gradient technique. Fixing partitioning into \n",
            "Ki  is needed to calculate the gradient of U(X,Y) because otherwise it is not \n",
            "a differentiable function with respect to the position of vertices {yi}. \n",
            "4) Adaptation step. Choose the segment with the largest number of points \n",
            "projected onto it. If more than one such segment exists then the longest \n",
            "one is chosen. The new vertex is inserted in the midpoint of this segment; \n",
            "all other segments are renumerated accordingly. \n",
            "5) Stopping criterion. The algorithm stops when the number of segments \n",
            "exceeds \n",
            ")\n",
            ",\n",
            "MSD(\n",
            "3\n",
            "/\n",
            "1\n",
            "Y\n",
            "X\n",
            "r\n",
            "N\n",
            ". \n",
            " \n",
            "Heuristically, the default parameters of the method have been proposed \uf062 = 0.3, \n",
            "r\n",
            "Y\n",
            "X\n",
            "N\n",
            "k\n",
            ")\n",
            ",\n",
            "MSD(\n",
            "'\n",
            "3\n",
            "/\n",
            "1\n",
            ", \uf06c\u2032 = 0.13. The details of implementation together with \n",
            "convergence and computational complexity study are provided elsewhere \n",
            "(K\u00e9gl, 1999).  \n",
            "Smola et al. (2001) proposed a regularized principal manifolds framework, based \n",
            "on minimization of quantization error functional with a large class of regularizers \n",
            "that can be used and a universal EM-type algorithm. For this algorithm, the \n",
            "convergence rates were analyzed and it was showed that for some regularizing \n",
            "\n",
            " \n",
            " \n",
            "terms the convergence can be optimized with respect to the Kegl\u2018s polygonal line \n",
            "algorithm. \n",
            " \n",
            "ELASTIC MAPS APPROACH  \n",
            " \n",
            "In a series of works (Gorban & Rossiev, 1999; Gorban et al., 2001, 2003; \n",
            "Gorban & Zinovyev, 2005, 2008a; Gorban et al., 2007, 2008), the authors of this \n",
            "chapter used metaphor of elastic membrane and plate to construct one-, two- and \n",
            "three-dimensional principal manifold approximations of various topologies. Mean \n",
            "squared distance approximation error combined with the elastic energy of the \n",
            "membrane serves as a functional to be optimised. The elastic map algorithm is \n",
            "extremely fast at the optimisation step due to the simplest form of the smoothness \n",
            "penalty. It is implemented in several programming languages as software libraries \n",
            "or front-end user graphical interfaces freely available from the web-site \n",
            "http://bioinfo.curie.fr/projects/vidaexpert. The software found applications in \n",
            "microarray data analysis, visualization of genetic texts, visualization of \n",
            "economical and sociological data and other fields (Gorban et al, 2001, 2003; \n",
            "Gorban & Zinovyev 2005, 2008a; Gorban et al, 2007, 2008). \n",
            " \n",
            "Let G be a simple undirected graph with set of vertices V and set of edges E. \n",
            " \n",
            "Definition. k-star in a graph G is a subgraph with k + 1 vertices v0,1,...,k \uf0ce V and k \n",
            "edges {(v0, vi)|i = 1, .., k} \uf0ce E. The rib is by definition a 2-star. \n",
            " \n",
            "Definition. Suppose that for each k \u2265 2, a family Sk of k-stars in G has been \n",
            "selected. Then we define an elastic graph as a graph with selected families of \n",
            "k-stars Sk and for which for all E(i) \uf0ce E and \n",
            ")\n",
            "( j\n",
            "k\n",
            "S\uf0ce\n",
            " Sk, the corresponding elasticity \n",
            "moduli \u03bbi > 0 and \u03bckj > 0 are defined. \n",
            " \n",
            "Definition. Primitive elastic graph is an elastic graph in which every non-terminal \n",
            "node (with the number of neighbours more than one) is associated with a k-star \n",
            "formed by all neighbours of the node. All k-stars in the primitive elastic graph are \n",
            "selected, i.e. the Sk sets are completely determined by the graph structure. \n",
            " \n",
            "Definition. Let E(i)(0), E(i)(1) denote two vertices of the graph edge E(i) and \n",
            ")\n",
            "( j\n",
            "k\n",
            "S\n",
            "(0), ..., \n",
            ")\n",
            "( j\n",
            "k\n",
            "S\n",
            "(k) denote vertices of a k-star \n",
            ")\n",
            "( j\n",
            "k\n",
            "S\n",
            " (where \n",
            ")\n",
            "( j\n",
            "k\n",
            "S\n",
            "(0) is the central vertex, \n",
            "to which all other vertices are connected). Let us consider a map \uf066:V \u2192 Rm which \n",
            "describes an embedding of the graph into a multidimensional space. The elastic \n",
            "energy of the graph embedding in the Euclidean space is defined as \n",
            ")\n",
            "(\n",
            ")\n",
            "(\n",
            ":)\n",
            "(\n",
            "G\n",
            "U\n",
            "G\n",
            "U\n",
            "G\n",
            "U\n",
            "R\n",
            "E\n",
            ", \n",
            " \n",
            " \n",
            " \n",
            "(3) \n",
            ")\n",
            "(\n",
            "2\n",
            ")\n",
            "(\n",
            ")\n",
            "(\n",
            "))\n",
            "1(\n",
            "(\n",
            "))\n",
            "0\n",
            "(\n",
            "(\n",
            ":)\n",
            "(\n",
            "i\n",
            "E\n",
            "i\n",
            "i\n",
            "i\n",
            "E\n",
            "E\n",
            "E\n",
            "G\n",
            "U\n",
            ", \n",
            " \n",
            " \n",
            "(4) \n",
            ")\n",
            "(\n",
            "2\n",
            "1\n",
            ")\n",
            "(\n",
            ")\n",
            "(\n",
            "||\n",
            "))\n",
            "(\n",
            "(\n",
            "1\n",
            "))\n",
            "0\n",
            "(\n",
            "(\n",
            "||\n",
            ":)\n",
            "(\n",
            "j\n",
            "k\n",
            "S\n",
            "k\n",
            "i\n",
            "j\n",
            "k\n",
            "j\n",
            "k\n",
            "kj\n",
            "E\n",
            "i\n",
            "S\n",
            "k\n",
            "S\n",
            "G\n",
            "U\n",
            ". \n",
            " \n",
            " \n",
            "(5) \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "Fig. 1. Elastic nets used in practice. \n",
            " \n",
            "Definition. Elastic net is a particular case of elastic graph which (1) contains only \n",
            "ribs (2-stars) (the family Sk are empty for all k>2); and (2) the vertices of this \n",
            "graph form a regular small-dimensional grid (Fig.1).  \n",
            " \n",
            "The elastic net is characterised by internal dimension dim(G). Every node vi in the \n",
            "elastic net is indexed by the discrete values of internal coordinates \n",
            "}\n",
            ",...,\n",
            "{\n",
            ")\n",
            "dim(\n",
            "1\n",
            "i\n",
            "G\n",
            "i\n",
            " \n",
            "in such a way that the nodes close on the graph have similar internal coordinates.  \n",
            " \n",
            "The purpose of the elastic net is to introduce point approximations to manifolds. \n",
            "Historically it was first explored and used in applications. To avoid confusion, one \n",
            "should notice that the term elastic net was independently introduced by several \n",
            "groups:  for solving the traveling salesman problem (Durbin &Willshaw, 1987), in \n",
            "the context of principal manifolds (Gorban et al, 2001) and recently in the context \n",
            "of regularized regression problem (Zhou & Hastie, 2005). These three notions are \n",
            "completely independent and denote different things. \n",
            " \n",
            "Definition. Elastic map is a continuous manifold Y\uf0ceRm constructed from the \n",
            "elastic net as its grid approximation using some between-node interpolation \n",
            "procedure. This interpolation procedure constructs a continuous mapping \uf066\n",
            "c:{\uf06c\n",
            "1,\u2026, \uf06c\n",
            "dim(G)} \u2192 Rm from the discrete map \uf066\uf03a\n",
            "V \u2192 Rm\uf020\uf02c\uf020used to embed the \n",
            "graph in Rm, and the discrete values of node indices \n",
            "}\n",
            ",...,\n",
            "{\n",
            ")\n",
            "dim(\n",
            "1\n",
            "i\n",
            "G\n",
            "i\n",
            ", i = 1...|V|. For \n",
            "example, the simplest piecewise linear elastic map is built by piecewise linear \n",
            "map \uf066c. \n",
            " \n",
            "Definition. Elastic principal manifold of dimension s for a dataset X is an elastic \n",
            "map, constructed from an elastic net Y of dimension s embedded in Rm using such \n",
            "a map \uf066opt:Y \u2192 Rm\uf020\uf020that corresponds to the minimal value of the functional \n",
            " \n",
            ")\n",
            "(\n",
            ")\n",
            ",\n",
            "(\n",
            "MSD\n",
            ")\n",
            ",\n",
            "(\n",
            "W\n",
            "G\n",
            "U\n",
            "Y\n",
            "X\n",
            "Y\n",
            "X\n",
            "U\n",
            ",    \n",
            " \n",
            " \n",
            "(6) \n",
            " \n",
            "where the weighted mean squared distance from the dataset X to the elastic net Y \n",
            "is calculated as the distance to the finite set of vertices {y1=\uf066\uf028\n",
            "v1\uf029\uf02c\uf02e\uf02e\uf02e\uf02c\uf020\n",
            "yk=\uf066\uf028\n",
            "vk\uf029}.  \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "In the Euclidean space one can apply an EM algorithm for estimating the elastic \n",
            "principal manifold for a finite dataset. It is based in turn on the general algorithm \n",
            "for estimating the locally optimal embedding map \uf066\uf020for an arbitrary elastic graph \n",
            "G, described below. \n",
            " \n",
            "Optimisation of the elastic graph algorithm: \n",
            " \n",
            "1) Choose some initial position of nodes of the elastic graph \n",
            "{y1=\uf066\uf028\n",
            "v1\uf029\uf02c\uf02e\uf02e\uf02e\uf02c\uf020\n",
            "yk=\uf066\uf028\n",
            "vk\uf029}, where k is the number of graph nodes k = |V|; \n",
            "2) Calculate two matrices eij and sij , using the following sub-algorithm: \n",
            "i. Initialize the sij matrix to zero; \n",
            "ii. For each k-star \n",
            ")\n",
            "(i\n",
            "k\n",
            "S\n",
            " with elasticity module \u03bcki, outer nodes \n",
            "vN1 , ... ,vNk and the central node vN0, the sij matrix is updated \n",
            "as follows (1 \u2264 l,m \u2264 k): \n",
            "k\n",
            "s\n",
            "s\n",
            "k\n",
            "s\n",
            "s\n",
            "k\n",
            "s\n",
            "s\n",
            "s\n",
            "s\n",
            "ki\n",
            "N\n",
            "N\n",
            "N\n",
            "N\n",
            "ki\n",
            "N\n",
            "N\n",
            "N\n",
            "N\n",
            "ki\n",
            "N\n",
            "N\n",
            "N\n",
            "N\n",
            "ki\n",
            "N\n",
            "N\n",
            "N\n",
            "N\n",
            "l\n",
            "l\n",
            "l\n",
            "l\n",
            "m\n",
            "l\n",
            "m\n",
            "l\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            ",\n",
            ",\n",
            "2\n",
            " \n",
            "iii. Initialize the eij matrix to zero; \n",
            "iv. For each edge E(i) with weight \u03bbi, one vertex vk1 and the \n",
            "other vertex vk2, the ejk matrix is updated as follows: \n",
            "i\n",
            "k\n",
            "k\n",
            "k\n",
            "k\n",
            "i\n",
            "k\n",
            "k\n",
            "k\n",
            "k\n",
            "i\n",
            "k\n",
            "k\n",
            "k\n",
            "k\n",
            "i\n",
            "k\n",
            "k\n",
            "k\n",
            "k\n",
            "e\n",
            "e\n",
            "e\n",
            "e\n",
            "e\n",
            "e\n",
            "e\n",
            "e\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            ",\n",
            ",\n",
            " \n",
            " \n",
            "3) Partition X into subsets Ki, i=1..k of data points by their proximity to \n",
            "yk: \n",
            ")}\n",
            ",\n",
            "(\n",
            "dist\n",
            "min\n",
            "arg\n",
            ":\n",
            "{\n",
            "j\n",
            "Y\n",
            "i\n",
            "i\n",
            "j\n",
            "K\n",
            "y\n",
            "x\n",
            "y\n",
            "x\n",
            "y\n",
            "; \n",
            "4) Given Ki , calculate matrix \n",
            "js\n",
            "js\n",
            "N\n",
            "i\n",
            "i\n",
            "js\n",
            "j\n",
            "js\n",
            "s\n",
            "e\n",
            "w\n",
            "n\n",
            "a\n",
            "1\n",
            ", where \n",
            "j\n",
            "i K\n",
            "x\n",
            "i\n",
            "j\n",
            "w\n",
            "n\n",
            ", \uf064\n",
            "js is the Kronecker\u2018s symbol. \n",
            " \n",
            "5) Find new position of {y1\uf02c\uf02e\uf02e\uf02e\uf02c\uf020\n",
            "yk} by solving the system of linear \n",
            "equations  \n",
            "j\n",
            "i K\n",
            "i\n",
            "i\n",
            "N\n",
            "i\n",
            "i\n",
            "k\n",
            "s\n",
            "s\n",
            "js\n",
            "w\n",
            "w\n",
            "a\n",
            "x\n",
            "x\n",
            "y\n",
            "1\n",
            "1\n",
            "1\n",
            " \n",
            "6) Repeat steps 3-5 until complete or approximate convergence of node \n",
            "positions {y1\uf02c\uf02e\uf02e\uf02e\uf02c\uf020\n",
            "yk}. \n",
            " \n",
            "As usual, the EM algorithm described above gives only locally optimal solution. \n",
            "One can expect that the number of local minima of the energy function U grows \n",
            "with increasing the \u2017softness\u2018 of the elastic graph (decreasing \uf06d\n",
            "kj parameters). \n",
            "Because of this, in order to obtain a solution closer to the global optimum, the \n",
            "softening strategy has been proposed, used in the algorithm for estimating the \n",
            "elastic principal manifold. \n",
            " \n",
            "Algorithm for estimating the elastic principal manifold \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "1) Define a decreasing set of numbers {m1,\u2026,mp}, mp=1 (for example, {103, \n",
            "102, 10, 1}), defining p epochs for softening;  \n",
            "2) Define the base values of the elastic moduli \n",
            ")\n",
            "(base\n",
            "i\u03bb\n",
            " and \n",
            ")\n",
            "(base\n",
            "i\u03bc\n",
            "; \n",
            "3) Initialize positions of the elastic net nodes {y1\uf02c\uf02e\uf02e\uf02e\uf02c\uf020\n",
            "yk} on the linear \n",
            "principal manifold spanned by first dim(G) principal components;  \n",
            "4) Set epoch_counter = 1 \n",
            "5) Set the elastic moduli \n",
            ")\n",
            "(\n",
            "_\n",
            "base\n",
            "i\n",
            "counter\n",
            "epoch\n",
            "i\n",
            "\u03bb\n",
            "m\n",
            "\u03bb\n",
            " and \n",
            ")\n",
            "(\n",
            "_\n",
            "base\n",
            "i\n",
            "counter\n",
            "epoch\n",
            "i\n",
            "\u03bc\n",
            "m\n",
            "\u03bc\n",
            " ; \n",
            "6) Modify the elastic net using the algorithm for optimisation of the elastic \n",
            "graph; \n",
            "7) Repeat steps 5-6 for all values of epoch_counter = 2, \u2026 , p. \n",
            " \n",
            "Remark. The values \u03bbi and \u03bcj are the coefficients of stretching elasticity of every \n",
            "edge E(i) and of bending elasticity of every rib \n",
            ")\n",
            "(\n",
            "2\n",
            "j\n",
            "S\n",
            ". In the simplest case \n",
            "\u03bb1 = \u03bb2 = ... = \u03bbs = \u03bb(s), \u03bc1 = \u03bc2 = ... = \u03bcr = \u03bc(r), where s and r are the numbers of \n",
            "edges and ribs correspondingly. Approximately dependence on graph \u2017resolution\u2018 \n",
            "is given by Gorban & Zinovyev (2007): \n",
            ")\n",
            "dim(\n",
            ")\n",
            "dim(\n",
            "2\n",
            "0\n",
            ")\n",
            "dim(\n",
            ")\n",
            "dim(\n",
            "2\n",
            "0\n",
            ")\n",
            "(\n",
            ",\n",
            ")\n",
            "(\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "r\n",
            "s\n",
            "s\n",
            "s\n",
            ". \n",
            "This formula is applicable, of course, only for the elastic nets. In general a case \u03bbi \n",
            "and \u03bci are often made variable in different parts of the graph accordingly to some \n",
            "adaptation strategy (Gorban & Zinovyev, 2005).  \n",
            " \n",
            "Remark. \n",
            ")\n",
            "(G\n",
            "U E\n",
            " penalizes the total length (or, indirectly, \u2017square\u2018, \u2017volume, etc.) \n",
            "of the constructed manifold and provides regularization of distances between node \n",
            "positions at the initial steps of the softening. At the final stage of the softening \u03bbi \n",
            "can be put to zero with little effect on the manifold configuration. \n",
            " \n",
            "Elastic map post-processing such as map extrapolation can be applied to increase \n",
            "its usability and avoid the \u2017border effect\u2018, for details see (Gorban & \n",
            "Zinovyev, 2008a). \n",
            " \n",
            "PLURIHARMONIC GRAPHS AS IDEAL APPROXIMATORS \n",
            " \n",
            "Approximating datasets by one dimensional principal curves is not satisfactory in \n",
            "the case of datasets that can be intuitively characterized as branched. A principal \n",
            "object which naturally passes through the \u2017middle\u2018 of such a data distribution \n",
            "should also have branching points that are missing in the simple structure of \n",
            "principal curves. Introducing such branching points converts principal curves into \n",
            "principal graphs.  \n",
            " \n",
            "Principal graphs were introduced by K\u00e9gl & Krzyzak (2002) as a natural \n",
            "extension of one-dimensional principal curves in the context of skeletonisation of \n",
            "hand-written symbols. The most important part of this definition is the form of the \n",
            "penalty imposed onto deviation of the configuration of the branching points \n",
            "embedment from their \u2017ideal\u2018 configuration (end, line, corner, T-, Y- and X-\n",
            "configuration). Assigning types for all vertices serves for definition of the penalty \n",
            "on the total deviation from the graph \u2017ideal\u2018 configuration (K\u00e9gl, 1999). Other \n",
            "\n",
            " \n",
            " \n",
            "types of vertices were not considered, and outside the field of symbol \n",
            "skeletonization applicability of such a definition of principal graph remains \n",
            "limited. \n",
            " \n",
            "Gorban & Zinovyev (2005), Gorban et al. (2007), and Gorban et al. (2008) \n",
            "proposed to use a universal form of non-linearity penalty for the branching points. \n",
            "The form of this penalty is defined in the previous chapter for the elastic energy of \n",
            "graph embedment. It naturally generalizes the simplest three-point second \n",
            "derivative approximation squared:  \n",
            "for a 2-star (or rib) the penalty equals \n",
            "2\n",
            ")\n",
            "(\n",
            "2\n",
            ")\n",
            "(\n",
            "2\n",
            ")\n",
            "(\n",
            "2\n",
            "||\n",
            ")))\n",
            "2\n",
            "(\n",
            "(\n",
            "))\n",
            "1(\n",
            "(\n",
            "(\n",
            "2\n",
            "1\n",
            "))\n",
            "0\n",
            "(\n",
            "(\n",
            "||\n",
            "j\n",
            "j\n",
            "j\n",
            "S\n",
            "S\n",
            "S\n",
            ",  \n",
            "for a 3-star it is \n",
            "2\n",
            ")\n",
            "(\n",
            "3\n",
            ")\n",
            "(\n",
            "3\n",
            ")\n",
            "(\n",
            "3\n",
            ")\n",
            "(\n",
            "3\n",
            "||\n",
            ")))\n",
            "3\n",
            "(\n",
            "(\n",
            "))\n",
            "2\n",
            "(\n",
            "(\n",
            "))\n",
            "1(\n",
            "(\n",
            "(\n",
            "3\n",
            "1\n",
            "))\n",
            "0\n",
            "(\n",
            "(\n",
            "||\n",
            "j\n",
            "j\n",
            "j\n",
            "j\n",
            "S\n",
            "S\n",
            "S\n",
            "S\n",
            ", etc.  \n",
            "For a k-star this penalty equals to zero iff the position of the central node \n",
            "coincides with the mean point of its neighbors. An embedment \uf066(G) is \u2017ideal\u2018 if \n",
            "all such penalties equal to zero. For a primitive elastic graph this means that this \n",
            "embedment is a harmonic function on graph: its value in each non-terminal vertex \n",
            "is a mean of the value in the closest neighbors of this vertex.  \n",
            " \n",
            "For non-primitive graphs we can consider stars which include not all neighbors of \n",
            "their centers. For example, for a square lattice we create elastic graph (elastic net) \n",
            "using 2-stars (ribs): all vertical 2-stars and all horizontal 2-stars. For such elastic \n",
            "net, each non-boundary vertex belongs to two stars. For a general elastic graph G \n",
            "with sets of k-stars \n",
            "k\n",
            "S  we introduce the following notion of pluriharmoning \n",
            "function. \n",
            " \n",
            "Definition. A map \uf066\uf03aV\u2192 Rm defined on vertices of G is pluriharmonic iff for any \n",
            "k-star \n",
            "k\n",
            "j\n",
            "k\n",
            "S\n",
            "S\n",
            ")\n",
            "(\n",
            " with the central vertex \n",
            ")\n",
            "( j\n",
            "k\n",
            "S\n",
            "(0) and the neighbouring vertices \n",
            ")\n",
            "( j\n",
            "k\n",
            "S\n",
            "(i), i = 1...k, the equality holds:  \n",
            "k\n",
            "i\n",
            "j\n",
            "k\n",
            "j\n",
            "k\n",
            "i\n",
            "S\n",
            "k\n",
            "S\n",
            "1\n",
            ")\n",
            "(\n",
            ")\n",
            "(\n",
            "))\n",
            "(\n",
            "(\n",
            "1\n",
            "))\n",
            "0\n",
            "(\n",
            "(\n",
            ".    \n",
            " \n",
            " \n",
            "(7) \n",
            " \n",
            "Pluriharmonic maps generalize the notion of linear map and of harmonic map, \n",
            "simultaneously. For example: \n",
            "1) \n",
            "1D harmonic functions are linear; \n",
            "2) \n",
            "If we consider an nD cubic lattice as a primitive graph (with 2n-stars for all \n",
            "non-boundary vertices), then the correspondent pluriharmonic functions are \n",
            "just harmonic ones; \n",
            "3) \n",
            "If we create from nD cubic lattice a standard nD elastic net with 2-stars \n",
            "(each non-boundary vertex is a center of n 2-stars, one 2-stars for each \n",
            "coordinate direction), then pluriharmonic functions are linear. \n",
            " \n",
            "Pluriharmonic functions have many attractive properties, for example, they satisfy \n",
            "the following maximum principle. A vertex v of an elastic graph is called a corner \n",
            "point or an extreme point of G iff v is not a centre of any k-star from \n",
            "k\n",
            "S  for all \n",
            "k>0. \n",
            "\n",
            " \n",
            " \n",
            "Theorem. Let \uf066\uf03a\n",
            "V\u2192 Rm   be a pluriharmonic map, F be a convex function on Rm, \n",
            "and a = maxx\uf0ceVF(\uf066(x)). Then there is a corner point v of G such that F(\uf066\uf028\n",
            "v))=a. \n",
            " \n",
            "Convex functions achieve their maxima in corner points. Even a particular case of \n",
            "this theorem with linear functions F is quite useful. Linear functions achieve their \n",
            "maxima and minima in corner points. \n",
            " \n",
            "In the theory of principal curves and manifolds the penalty functions were \n",
            "introduced to penalise deviation from linear manifolds (straight lines or planes). \n",
            "We proposed to use pluriharmonic embeddings (\u2017pluriharmonic graphs\u2018) as \u2017ideal \n",
            "objects\u2018 instead of manifolds and to introduce penalty (5) for deviation from this \n",
            "ideal form.  \n",
            " \n",
            "GRAPH GRAMMARS AND THREE TYPES OF COMPLEXITY FOR PRINCIPAL GRAPHS \n",
            " \n",
            "Principal graphs can be called data approximators of controllable complexity. By \n",
            "complexity of the principal objects we mean the following three notions: \n",
            " \n",
            "1) \n",
            "Geometric complexity: how far a principal object deviates from its ideal \n",
            "configuration; for the elastic principal graphs we explicitly measure \n",
            "deviation from the \u2017ideal\u2018 pluriharmonic graph by the elastic energy U\uf066(G) \n",
            "(3) (this complexity may be considered as a measure of non-linearity);  \n",
            "2) \n",
            "Structural complexity measure: it is some non-decreasing function of the \n",
            "number of vertices, edges and k-stars of different orders \n",
            "SC(G)=SC(|V|,|E|,|S2|,\u2026,|Sm|); this function penalises for number of \n",
            "structural elements; \n",
            "3) \n",
            "Construction complexity is defined with respect to a graph grammar as a \n",
            "number of applications of elementary transformations necessary to construct \n",
            "given G from the simplest graph (one vertex, zero edges).  \n",
            " \n",
            "The construction complexity is defined with respect to a grammar of elementary \n",
            "transformation. The graph grammars (L\u00f6we, 1993; Nagl, 1976) provide a well-\n",
            "developed formalism for the description of elementary transformations. An elastic \n",
            "graph grammar is presented as a set of production (or substitution) rules. Each \n",
            "rule has a form A \u2192 B, where A and B are elastic graphs. When this rule is applied \n",
            "to an elastic graph, a copy of A is removed from the graph together with all its \n",
            "incident edges and is replaced with a copy of B with edges that connect B to the \n",
            "graph. For a full description of this language we need the notion of a labeled \n",
            "graph. Labels are necessary to provide the proper connection between B and the \n",
            "graph (Nagl, 1976). An approach based on graph grammars to constructing \n",
            "effective approximations of an elastic principal graph has been recently proposed \n",
            "(Gorban et al, 2007). \n",
            " \n",
            "Let us define graph grammar O as a set of graph grammar operations \n",
            "O={o1,..,os}. All possible applications of a graph grammar operation oi to a graph \n",
            "G gives a set of transformations of the initial graph oi(G) = {G1, G2, \u2026, Gp}, \n",
            "where p is the number of all possible applications of oi to G. Let us also define a \n",
            "sequence of r different graph grammars  \n",
            "}}\n",
            ",...,\n",
            "{\n",
            ",\n",
            ",\n",
            "}\n",
            ",...,\n",
            "{\n",
            "{\n",
            ")\n",
            "(\n",
            ")\n",
            "(\n",
            "1\n",
            ")\n",
            "(\n",
            ")\n",
            "1\n",
            "(\n",
            ")\n",
            "1\n",
            "(\n",
            "1\n",
            ")\n",
            "1\n",
            "(\n",
            "1\n",
            "r\n",
            "s\n",
            "r\n",
            "r\n",
            "s\n",
            "r\n",
            "o\n",
            "o\n",
            "O\n",
            "o\n",
            "o\n",
            "O\n",
            "\uf04c\n",
            ". \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "Let us choose a grammar of elementary transformations, predefined boundaries of \n",
            "structural complexity SCmax and construction complexity CCmax , and elasticity \n",
            "coefficients \u03bbi and \u03bckj .  \n",
            " \n",
            "Definition. Elastic principal graph for a dataset X is such an elastic graph G \n",
            "embedded in the Euclidean space by the map \uf066\uf03a\n",
            "V\u2192 Rm  that SC(G) \u2264 SCmax ,  \n",
            "CC(G) \u2264 CCmax , and U\uf066(G) \u2192 min over all possible elastic graphs G embeddings \n",
            "in Rm . \n",
            " \n",
            "Algorithm for estimating the elastic principal graph \n",
            " \n",
            "1) \n",
            "Initialize the elastic graph G by 2 vertices v1 and v2 connected by an edge. \n",
            "The initial map \uf066 is chosen in such a way that \uf066(v1) and \uf066(v2) belong to the \n",
            "first principal line in such a way that all the data points are projected onto \n",
            "the principal line segment defined by \uf066(v1), \uf066(v2); \n",
            "2) \n",
            "For all j=1\u2026r repeat steps 3-6: \n",
            "3) \n",
            "Apply all grammar operations from O(j) to G in all possible ways; this gives \n",
            "a collection of candidate graph transformations {G1, G2, \u2026};  \n",
            "4) \n",
            "Separate {G1, G2, \u2026} into permissible and forbidden transformations; \n",
            "permissible transformation Gk is such that SC(Gk) \u2264 SCmax , where SCmax is \n",
            "some predefined structural complexity ceiling; \n",
            "5) \n",
            "Optimize the embedment \uf066 and calculate the elastic energy U\uf066(G) of graph \n",
            "embedment for every permissible candidate transformation, and choose such \n",
            "a graph Gopt that gives the minimal value of the elastic functional: \n",
            ")\n",
            "(\n",
            "inf\n",
            "arg\n",
            "k\n",
            "set\n",
            "e\n",
            "permissibl\n",
            "G\n",
            "opt\n",
            "G\n",
            "U\n",
            "G\n",
            "k\n",
            "; \n",
            "6) \n",
            "Substitute G \u2192Gopt ; \n",
            "7) \n",
            "Repeat steps 2-6 until the set of permissible transformations is empty or the \n",
            "number of operations exceeds a predefined number \u2013 the construction \n",
            "complexity. \n",
            " \n",
            "PRINCIPAL TREES AND METRO MAPS \n",
            " \n",
            "Let us construct the simplest non-trivial type of the principal graphs, called \n",
            "principal trees. For this purpose let us introduce a simple \u2017Add a node, bisect an \n",
            "edge\u2018 graph grammar (see Fig. 2) applied for the class of primitive elastic graphs. \n",
            " \n",
            "Definition. Principal tree is an acyclic primitive elastic principal graph. \n",
            " \n",
            "Definition. \u201eRemove a leaf, remove an edge\u201f graph grammar O(shrink) applicable \n",
            "for the class of primitive elastic graphs consists of two operations: 1) The \n",
            "transformation \u2017remove a leaf\u2018 can be applied to any vertex v of G with \n",
            "connectivity degree equal to 1: remove v and remove the edge (v,v\u2018) connecting v \n",
            "to the tree; 2) The transformation \u2017remove an edge\u2018 is applicable to any pair of \n",
            "graph vertices v, v\u201f connected by an edge (v, v\u201f): delete edge (v, v\u201f), delete vertex \n",
            "v\u2018, merge the k-stars for which v and v\u201f are the central nodes and make a new \n",
            "k-star for which v is the central node with a set of neighbors which is the union of \n",
            "the neighbors from the  k-stars of v and v\u201f. \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Fig. 2. Illustration of the simple \u201cadd node to a node\u201d or \u201cbisect an edge\u201d graph \n",
            "grammar. a) We start with a simple 2-star from which one can generate three distinct \n",
            "graphs shown. The \u201cOp1\u201d operation is adding a node to a node, operations \u201cOp1\u201d and \n",
            "\u201cOp2\u201d are edge bisections (here they are topologically equivalent to adding a node to a \n",
            "terminal node of the initial 2-star). For illustration let us suppose that the \u201cOp2\u201d \n",
            "operation gives the biggest elastic energy decrement, thus it is the \u201coptimal\u201d operation. \n",
            "b) From the graph obtained one can generate 5 distinct graphs and choose the optimal \n",
            "one. c) The process is continued until a definite number of nodes are inserted. \n",
            " \n",
            "Definition. \u201eAdd a node, bisect an edge\u201f graph grammar O(grow) applicable for the \n",
            "class of primitive elastic graphs consists of two operations: 1) The transformation \n",
            "\u201cadd a node\u201d can be applied to any vertex v of G: add a new node z and a new \n",
            "edge (v, z); 2) The transformation \u201cbisect an edge\u201d is applicable to any pair of \n",
            "graph vertices v, v\u201f connected by an edge (v, v\u201f): delete edge (v, v\u201f), add a vertex z \n",
            "and two edges, (v, z) and (z, v\u201f). The transformation of the elastic structure \n",
            "(change in the star list) is induced by the change of topology, because the elastic \n",
            "graph is primitive. Consecutive application of the operations from this grammar \n",
            "generates trees, i.e. graphs without cycles. \n",
            " \n",
            "Also we should define the structural complexity measure \n",
            "SC(G)=SC(|V|,|E|,|S2|,\u2026,|Sm|). Its concrete form depends on the application field. \n",
            "Here are some simple examples: \n",
            " \n",
            "1) SC(G) = |V| : i.e., the graph is considered more complex if it has more \n",
            "vertices; \n",
            "2) \n",
            "otherwise\n",
            ",\n",
            "0\n",
            "and\n",
            "|\n",
            "|\n",
            "if\n",
            "|,\n",
            "|\n",
            " =\n",
            " )\n",
            "SC(\n",
            "4\n",
            "max\n",
            "3\n",
            "3\n",
            "m\n",
            "k\n",
            "kS\n",
            "b\n",
            "S\n",
            "S\n",
            "G\n",
            ", \n",
            "i.e., only bmax simple branches (3-stars) are allowed in the principal tree.  \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "h) \n",
            " \n",
            " \n",
            "Fig. 3. Principal manifold and principal tree for the Iris dataset. a) View of the principal \n",
            "manifold projected on the first two principal components, the data points are shown \n",
            "projected into the closest vertex of the elastic net; b) visualization of data points in the \n",
            "internal coordinates, here classes are represented in the form of Hinton diagrams: the \n",
            "size of the diagram is proportional to the number of points projected, the shape of the \n",
            "diagram denote three different point classes; c) same as a), but the data points are shown \n",
            "projected into the closest point of the piecewise linearly interpolated elastic map; d) same \n",
            "as b), but based on projection shown in c); e)-g) First 50 iterations of the principal tree \n",
            "algorithm, the tree is shown projected onto the principal plane; h) metro map \n",
            "representation of the Iris dataset. \n",
            " \n",
            "Using the sequence {O(grow), O(grow), O(shrink)} in the above-described algorithm for \n",
            "estimating the elastic principal graph gives an approximation to the principal \n",
            "trees. Introducing the \u2017tree trimming\u2018 grammar O(shrink) allows to produce principal \n",
            "trees closer to the global optimum, trimming excessive tree branching and fusing \n",
            "k-stars separated by small \u2017bridges\u2018. \n",
            " \n",
            "Principal trees can have applications in data visualization. A principal tree is \n",
            "embedded into a multidimensional data space. It approximates the data so that one \n",
            "\n",
            " \n",
            " \n",
            "can project points from the multidimensional space into the closest node of the \n",
            "tree. The tree by its construction is a one-dimensional object, so this projection \n",
            "performs dimension reduction of the multidimensional data. The question is how \n",
            "to produce a planar tree layout? Of course, there are many ways to layout a tree on \n",
            "a plane without edge intersection. But it would be useful if both local tree \n",
            "properties and global distance relations would be represented using the layout. We \n",
            "can require that \n",
            "1) \n",
            "In a two-dimensional layout, all k-stars should be represented equiangular; \n",
            "this is the small penalty configuration; \n",
            "2) \n",
            "The edge lengths should be proportional to their length in the \n",
            "multidimensional embedding; thus one can represent between-node \n",
            "distances. \n",
            "This defines a tree layout up to global rotation and scaling and also up to changing \n",
            "the order of leaves in every k-star. We can change this order to eliminate edge \n",
            "intersections, but the result can not be guaranteed. In order to represent the global \n",
            "distance structure, it was found (Gorban et al., 2008) that a good approximation \n",
            "for the order of k-star leaves can be taken from the projection of every k-star on \n",
            "the linear principal plane calculated for all data points, or on the local principal \n",
            "plane in the vicinity of the k-star, calculated only for the points close to this star. \n",
            "The resulting layout can be further optimized using some greedy optimization \n",
            "methods. \n",
            " \n",
            "The point projections are then represented as pie diagrams, where the size of the \n",
            "diagram reflects the number of points projected into the corresponding tree node. \n",
            "The sectors of the diagram allow us to show proportions of points of different \n",
            "classes projected into the node (see an example on Fig. 3). \n",
            " \n",
            "This data display was called a \u201cmetro map\u201d since it is a schematic and \u2015idealized\u2016 \n",
            "representation of the tree and the data distribution with inevitable distortions made \n",
            "to produce a 2D layout. However, using this map one can still estimate the \n",
            "distance from a point (tree node) to a point passing through other points. This map \n",
            "is inherently unrooted (as a real metro map). It is useful to compare this metaphor \n",
            "with trees produced by hierarchical clustering where the metaphor is closer to a \n",
            "\u2015genealogy tree\u2016. \n",
            " \n",
            "PRINCIPAL CUBIC COMPLEXES \n",
            " \n",
            "Elastic nets introduced above are characterized by their internal dimension \n",
            "dim(G). The way to generalize these characteristics on other elastic graphs is to \n",
            "utilize the notion of cubic complex (Gorban et al, 2007). \n",
            " \n",
            "Definition. Elastic cubic complex K of internal dimension r is a Cartesian product \n",
            "G1 \u00d7\u2026\u00d7 Gr of elastic graphs G1, . . .Gr . It has the vertex set V1 \u00d7 . . . \u00d7 Vr. Let \n",
            "1 \u2264 i \u2264 r and vj \uf0ce Vj (j \u2260 i). For this set of vertices, {vj}j \u2260 i, a copy of Gi in \n",
            "G1 \u00d7 ... \u00d7Gr is defined with vertices (v1, \u2026, vi\u22121, v, vi+1, \u2026, vr) (v \uf0ce Vi), edges  \n",
            " \n",
            "((v1, \u2026, vi\u22121, v, vi+1, \u2026, vr), (v1, \u2026, vi\u22121, v\u201f, vi+1, \u2026, vr)), (v, v\u201f) \uf0ce Ei ,  \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "and, similarly, k-stars of the form (v1, \u2026, vi\u22121, Sk, vi+1, \u2026, vr), where Sk is a k-star \n",
            "in Gi. For any Gi there are \n",
            "i\n",
            "j\n",
            "j\n",
            "V |\n",
            "|\n",
            " copies of Gi in G. Sets of edges and k-stars for \n",
            "Cartesian product are unions of that set through all copies of all factors. A map \n",
            "\u03c6 : V1 \u00d7 . . . \u00d7 Vr \uf0ae\n",
            " Rm maps all the copies of factors into Rm too.  \n",
            " \n",
            "Remark. By construction, the energy of the elastic graph product is the energy \n",
            "sum of all factor copies. It is, of course, a quadratic functional of \uf066. \n",
            " \n",
            "If we approximate multidimensional data by an r-dimensional object, the number \n",
            "of points (or, more generally, elements) in this object grows with r exponentially. \n",
            "This is an obstacle for grammar\u2013based algorithms even for modest r, because for \n",
            "analysis of the rule A \uf0ae\n",
            " B applications we should investigate all isomorphic \n",
            "copies of A in G. Introduction of a cubic complex is useful factorization of the \n",
            "principal object which allows to avoid this problem. \n",
            " \n",
            "The only difference between the construction of general elastic graphs and \n",
            "factorized graphs is in the application of the transformations. For factorized \n",
            "graphs, we apply them to factors. This approach significantly reduces the amount \n",
            "of trials in selection of the optimal application. The simple grammar with two \n",
            "rules, \u2015add a node to a node, or bisect an edge,\u2016 is also powerful here, it produces \n",
            "products of primitive elastic trees. For such a product, the elastic structure is \n",
            "defined by the topology of the factors. \n",
            " \n",
            "INCOMPLETE DATA \n",
            " \n",
            "Some of the methods described above allow us to use incomplete data in a natural \n",
            "way. Let us represent an incomplete observation by \n",
            ")\n",
            "@,...,\n",
            "@,...,\n",
            ",...,\n",
            "( 1\n",
            "m\n",
            "x\n",
            "x\n",
            "x\n",
            ", \n",
            "where the \u2017@\u2018 symbol denotes a missing value.  \n",
            " \n",
            "Definition. Scalar product between two incomplete observations x and y is\n",
            "m\n",
            "i\n",
            "i\n",
            "i y\n",
            "x\n",
            "@\n",
            ")\n",
            ",\n",
            "(\n",
            "y\n",
            "x\n",
            ". Then the Euclidean distance is \n",
            "m\n",
            "i\n",
            "i\n",
            "i\n",
            "y\n",
            "x\n",
            "@\n",
            "2\n",
            "2\n",
            ")\n",
            "(\n",
            ")\n",
            "(\n",
            "y\n",
            "x\n",
            ". \n",
            " \n",
            "Remark. This definition has a very natural geometrical interpretation: an \n",
            "incomplete observation with k missing values is represented by a k\u2013dimensional \n",
            "linear manifold Lk, parallel to k coordinate axes corresponding to the missing data. \n",
            " \n",
            "Thus, any method which uses only scalar products or/and Euclidean distances can \n",
            "be applied for incomplete data with some minimal modifications subject to \n",
            "random and not too dense distribution of missing values in X. For example, the \n",
            "iterative method for SVD for incomplete data matrix was developed (Roweis \n",
            "(1998); Gorban & Rossiev, 1999).  \n",
            " \n",
            "There are, of course, other approaches to incomplete data in unsupervised learning \n",
            "(for example, those presented by Little & Rubin (1987)). \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "IMPLICIT METHODS \n",
            " \n",
            "Most of the principal objects introduced in this paper are constructed as explicit \n",
            "geometrical objects embedded in Rm to which we can calculate the distance from \n",
            "any object in X. In this way, they generalize the \u2015data approximation\u2016-based (#1) \n",
            "and the \u2015variation-maximization\u2016-based (#2) definitions of linear PCA. There also \n",
            "exists the whole family of methods, which we only briefly mention here, that \n",
            "generalize the \u2015distance distortion minimization\u2016 definition of PCA (#3).  \n",
            " \n",
            "First, some methods take as input a pairwise distance (or, more generally, \n",
            "dissimilarity) matrix D and construct such a configuration of points in a low-\n",
            "dimensional Euclidean space that the distance matrix D\u201f in this space reproduce D \n",
            "with maximal precision. The most fundamental in this series is the metric \n",
            "multidimensional scaling (Kruskal, 1964). The next is the Kernel PCA approach \n",
            "(Sch\u00f6lkopf et al., 1997) which takes advantage of the fact that for the linear PCA \n",
            "algorithm one needs only the matrix of pairwise scalar products (Gramm matrix) \n",
            "but not the explicit values of coordinates of X. It allows to apply the kernel trick \n",
            "(Aizerman et al., 1964) and substitute the Gramm matrix by the scalar products \n",
            "calculated with use of some kernel functions. Kernel PCA method is tightly \n",
            "related to the classical multidimensional scaling (Williams, 2002). \n",
            " \n",
            "Local Linear Embedding or LLE (Roweis & Saul, 2000) searches for such a N\u00d7N \n",
            "matrix A that approximates given xi by a linear combination of n vectors-\n",
            "neighbours of xi: \n",
            "min\n",
            "||\n",
            "||\n",
            "2\n",
            "1\n",
            "1\n",
            "N\n",
            "i\n",
            "N\n",
            "k\n",
            "k\n",
            "i\n",
            "k\n",
            "i\n",
            "A x\n",
            "x\n",
            ", where only such \n",
            "0\n",
            "i\n",
            "k\n",
            "A\n",
            ", if k is one \n",
            "of the n closest to xi vectors. After one constructs such a configuration of points in \n",
            "Rs, s << m, that \n",
            "N\n",
            "k\n",
            "k\n",
            "i\n",
            "k\n",
            "i\n",
            "A\n",
            "1\n",
            "y\n",
            "y\n",
            ", yi\uf0ceRs , for all i = 1\u2026N.  The coordinates of such \n",
            "embedding are given by the eigenvectors of the matrix (1-A)T(1-A). \n",
            " \n",
            "ISOMAP (Tenenbaum et al., 2000) and Laplacian eigenmap \n",
            "(Belkin & Niyogi, 2003; Nadler et al., 2008) methods start with construction of \n",
            "the neighbourhood graph, i.e. the graph in which close in some sense data points \n",
            "are connected by (weighted) edges. This weighted graph can be represented in the \n",
            "form of a weighted adjacency matrix W= {Wij}. From this graph, ISOMAP \n",
            "constructs a new distance matrix D(ISOMAP), based on the path lengths between two \n",
            "points in the neighbourhood graph, and the multidimensional scaling is applied to \n",
            "D(ISOMAP). The Laplacian map solves the eigenproblem \n",
            "f\n",
            "f\n",
            "S\n",
            "L\n",
            ", where \n",
            "}\n",
            ",\n",
            ",\n",
            "{\n",
            "1\n",
            "1\n",
            "0\n",
            "N\n",
            "j\n",
            "Nj\n",
            "N\n",
            "j\n",
            "j\n",
            "W\n",
            "W\n",
            "diag\n",
            "S\n",
            "\uf04c\n",
            ", L = S \u2013 W is the Laplacian matrix. The trivial \n",
            "constant solution corresponding to the smallest eigenvalue \uf06c\uf030\n",
            " = 0 is discarded, \n",
            "while the elements of the eigenvectors \n",
            "s\n",
            "f\n",
            "f\n",
            "f\n",
            ",\n",
            ",\n",
            ",\n",
            "2\n",
            "1\n",
            "\uf04c\n",
            ", where \n",
            "s\n",
            "...\n",
            "2\n",
            "1\n",
            ", \n",
            "give the s-dimensional projection of xi, i.e. P(xi)= {\n",
            ")\n",
            "(\n",
            ",\n",
            "),\n",
            "(\n",
            "),\n",
            "(\n",
            "2\n",
            "1\n",
            "i\n",
            "i\n",
            "i\n",
            "s\n",
            "f\n",
            "f\n",
            "f\n",
            "\uf04c\n",
            "}. \n",
            " \n",
            "Finally, one can implicitly construct projections into smaller dimensional spaces \n",
            "by training auto-associative neural networks with narrow hidden layer. An \n",
            "\n",
            " \n",
            " \n",
            "overview of the existing Neural PCA methods can be found in the recent \n",
            "collection of review papers (Gorban et al, 2008). \n",
            " \n",
            "EXAMPLE: PRINCIPAL OBJECTS FOR THE IRIS DATASET  \n",
            " \n",
            "On Fig. 3 we show application of the elastic principal manifolds and principal \n",
            "trees algorithms to the standard Iris dataset (Fisher, 1936). As expected, two-\n",
            "dimensional approximation of the principal manifold in this case is close to the \n",
            "linear principal plane. One can also see that the principal tree illustrates well the \n",
            "fact of almost complete separation of classes in data space. \n",
            " \n",
            "EXAMPLE: PRINCIPAL OBJECTS FOR MOLECULAR SURFACES  \n",
            " \n",
            "A molecular surface defines the effective region of space which is occupied by a \n",
            "molecule. For example, the Van-der-Waals molecular surface is formed by \n",
            "surrounding every atom in the molecule by a sphere of radius equal to the \n",
            "characteristic radius of the Van-der-Waals force. After all the interior points are \n",
            "eliminated, this forms a complicated non-smooth surface in 3D. In practice, this \n",
            "surface is sampled by a finite number of points. \n",
            " \n",
            "Using principal manifolds methodology, we constructed a smooth approximation \n",
            "of such molecular surface for a small piece of a DNA molecule (several \n",
            "nucleotides long). First, we have made an approximation of this dataset by a 1D \n",
            "principal curve. Interestingly, this curve followed the backbone of the molecule, \n",
            "forming a helix (see Fig. 4). Second, we approximated the molecular surface by a \n",
            "2D manifold. The topology of the surface is expected to be spherical, so we \n",
            "applied spherical topology of the elastic net for optimisation.  \n",
            " \n",
            " \n",
            " \n",
            "Fig. 4. Principal objects approximating molecular surface of a short stretch of DNA \n",
            "molecule. a) stick-and-balls model of the DNA stretch and the initial molecular surface \n",
            "(black points); b) one- and two-dimensional spherical principal manifolds for the \n",
            "molecular surface; c) simple principal cubic complex (product of principal trees) which \n",
            "does not have any branching in this case. \n",
            " \n",
            "We should notice that since it is impossible to make the lengths of all edges equal \n",
            "for the spherical grid, corrections were performed for the edge elasticities during \n",
            "the grid initialization (shorter edges are given larger \uf06c\n",
            "is). Third, we applied the \n",
            "method for constructing principal cubic complexes, namely, graph product of \n",
            "\n",
            " \n",
            " \n",
            "principal trees, which produced somewhat trivial construction (because no \n",
            "branching was energetically optimal): product of two short elastic principal \n",
            "curves, forming a double helix. \n",
            " \n",
            " \n",
            " \n",
            "Fig. 5. Seven cluster structures presented for 4 selected genomes. A genome is \n",
            "represented as a collection of points (text fragments represented by their triplet \n",
            "frequencies) in the 64-multidimensional space. Color codes denote point classes \n",
            "corresponding to 6 possible frameshifts when a random fragment overlaps with a coding \n",
            "gene (3 in the forward and 3 in the backward direction of the gene), and the black color \n",
            "corresponds to non-coding regions. For every genome a principal tree (\u201cmetro map\u201d \n",
            "layout) is shown together with 2D PCA projection of the data distribution. Note that the \n",
            "clusters that appear to be mixed on the PCA plot for Escherichia coli (they remain mixed \n",
            "in 3D PCA as well) are well separated on the \u201cmetro map\u201d. This proves that they are \n",
            "well-separated in R64. \n",
            " \n",
            "EXAMPLE: PRINCIPAL OBJECTS DECIPHER GENOME  \n",
            " \n",
            "A dataset X can be constructed for a string sequence using a short word frequency \n",
            "dictionary approach in the following way: 1) the notion of word is defined; 2) the \n",
            "set of all possible short words is defined, let us say that we have m of them; 3) a \n",
            "number N of text fragments of certain width is sampled from the text; 4) in each \n",
            "\n",
            " \n",
            " \n",
            "fragment the frequency of occurrences of all possible short words is calculated \n",
            "and, thus, each fragment is represented as a vector in multidimensional space Rm. \n",
            "The whole text then is represented as a dataset of N vectors embedded in Rm. \n",
            " \n",
            "We systematically applied this approach to available bacterial genomic sequences \n",
            "(Gorban & Zinovyev, 2008b). In our case we defined: 1) a word is a sequence of \n",
            "three letters from the {A,C,G,T} alphabet (triplet); 2) evidently, there are 64 \n",
            "possible triplets in the {A,C,G,T} alphabet; 3) we sampled 5000-10000 fragments \n",
            "of width 300 from a genomic sequence; 4)  we calculated the frequencies of non-\n",
            "overlapping triplets for every fragment.  \n",
            " \n",
            "The constructed datasets are interesting objects for data-mining, because 1) they \n",
            "have a non-trivial cluster structure which usually contains various configurations \n",
            "of 7 clusters (see Fig. 5); 2) class labels can be assigned to points accordingly to \n",
            "available genome annotations; in our case we put information about presence (in \n",
            "one of six possible frameshifts) or absence of the coding information in the \n",
            "current position of a genome; 3) using data mining techniques here has immediate \n",
            "applications in the field of automatic gene recognition and in others, see, for \n",
            "example, (Carbone et al, 2003). On Fig. 5 we show application of both classical \n",
            "PCA and the metro map methods for several bacterial genomes. Look at \n",
            "http://www.ihes.fr/~zinovyev/7clusters web-site for further information.  \n",
            " \n",
            "EXAMPLE: NON-LINEAR PRINCIPAL MANIFOLDS FOR MICROARRAY DATA \n",
            "VISUALIZATION \n",
            " \n",
            "DNA microarray data is a rich source of information for molecular biology (an \n",
            "expository overview is provided by Leung & Cavalieri (2003)). This technology \n",
            "found numerous applications in understanding various biological processes \n",
            "including cancer. It allows to screen simultaneously the expression of all genes in \n",
            "a cell exposed to some specific conditions (for example, stress, cancer, treatment, \n",
            "normal conditions). Obtaining a sufficient number of observations (chips), one \n",
            "can construct a table of \"samples vs genes\", containing logarithms of the \n",
            "expression levels of, typically several thousands (n) of genes, in typically several \n",
            "tens (m) of samples.  \n",
            "On Fig. 6 we provide a comparison of data visualization scatters after projection \n",
            "of the breast cancer dataset, provided by Wang et al. (2003), onto the linear two- \n",
            "and non-linear two-dimensional principal manifold. The latter one is constructed \n",
            "by the elastic maps approach. Each point here represents a patient treated from \n",
            "cancer. Before dimension reduction it is represented as a vector in Rn, containing \n",
            "the expression values for all n genes in the tumor sample. Linear and non-linear \n",
            "2D principal manifolds provide mappings Rn \uf0ae\n",
            " R2, drastically reducing vector \n",
            "dimensions and allowing data visualization. The form, the shape and the size of \n",
            "the point on the Fig.6 represent various clinical data (class labels) extracted from \n",
            "the patient\u2018s disease records.  \n",
            "Practical experience from bioinformatics studies shows that two-dimensional data \n",
            "visualization using non-linear projections allow to catch more signals from data \n",
            "(in the form of clusters or specific regions of higher point density) than linear \n",
            "projections, see Fig. 6 and a good example by Ivakhno & Armstrong (2008).  \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "Figure 6. Visualization of breast cancer microarray dataset using elastic maps. Ab initio \n",
            "classifications are shown using points size (ER, estrogen receptor status), shape (Group \n",
            "A \u2013 patients with aggressive cancer, Group B \u2013 patients with non-aggressive cancer) and \n",
            "color (TYPE, molecular type of breast cancer). a) Configuration of nodes projected into \n",
            "the three-dimensional principal linear manifold. One clear feature is that the dataset is \n",
            "curved such that it can not be mapped adequately onto a two-dimensional principal \n",
            "plane. b) The distribution of points in the internal non-linear manifold coordinates is \n",
            "shown together with estimation of the two-dimensional density of points. c) The same as \n",
            "b) but for the linear two-dimensional manifold. One can notice that the ``basal'' breast \n",
            "cancer subtype is much better separated on the non-linear mapping and some features of \n",
            "the distribution become better resolved. \n",
            " \n",
            "In addition to that, Gorban & Zinovyev (2008a) performed a systematic \n",
            "comparison of performance of low-dimensional linear and non-linear principal \n",
            "manifolds for microarray data visualization, using the following four criteria: \n",
            "1) mean-square distance error; 2) distortions in mapping the big distances between \n",
            "points; 3) local point neighbourhood preservation; 4) compactness of point class \n",
            "\n",
            " \n",
            " \n",
            "labels after projection. It was demonstrated that non-linear two-dimensional \n",
            "principal manifolds provide systematically better results accordingly to all these \n",
            "criteria, achieving the performance of three- and four- dimensional linear principal \n",
            "manifolds (principal components). \n",
            "The interactive ViMiDa (Visualization of Microarray Data) and ViDaExpert \n",
            "software allowing microarray data visualization with use of non-linear principal \n",
            "manifolds are available on the web-site of Institut Curie (Paris): \n",
            "http://bioinfo.curie.fr/projects/vidaexpert and http://bioinfo.curie.fr/projects/vimida. \n",
            " \n",
            "CONCLUSION \n",
            " \n",
            "In this chapter we gave a brief practical introduction into the methods of \n",
            "construction of principal objects, i.e. objects embedded in the \u2017middle\u2018 of the \n",
            "multidimensional data set. As a basis, we took the unifying framework of mean \n",
            "squared distance approximation of finite datasets which allowed us to look at the \n",
            "principal graphs and manifolds as generalizations of the mean point notion. \n",
            " \n",
            "REFERENCES \n",
            " \n",
            "1) \n",
            "Aizerman M., Braverman E., and Rozonoer L. (1964). Theoretical \n",
            "foundations of the potential function method in pattern recognition learning. \n",
            "Automation and Remote Control 25: 821\u2013837. \n",
            "2) \n",
            "Arthur D., Vassilvitskii S. (2007). K-means++ The Advantages of Careful \n",
            "Seeding. In: Proceedings of the Eighteenth Annual ACM-SIAM Symposium \n",
            "on Discrete Algorithms, SODA 2007 (pp. 1027-1035). New Orleans, \n",
            "Louisiana, USA, January 7-9. \n",
            "3) \n",
            "Aluja-Banet, T. and Nonell-Torrent, R. (1991). Local principal component \n",
            "analysis. Q\u03cbestio\u03cc 3, 267\u2013278. \n",
            "4) \n",
            "Bau III D., Trefethen L.N. (1997). Numerical linear algebra. SIAM Press, \n",
            "Philadelphia. (Lecture 31) \n",
            "5) \n",
            "Bauer F. L. (1957). Das verfahren der Treppeniteration und verwandte \n",
            "verfahren zur losung algebraischer eigenwert probleme, Z. Angew. Math. \n",
            "Phys. 8, 214\u2013235. \n",
            "6) \n",
            "Belkin M. and Niyogi P. (2006). Laplacian Eigenmaps for Dimensionality \n",
            "Reduction and Data Representation. Neural Computation 15(6): 1373-1396. \n",
            "7) \n",
            "Bishop C.M., Svens\u00e9n M., and Williams C.K.I. (1998). GTM: The \n",
            "generative topographic mapping. Neural Computation 10(1), 215\u2013234  \n",
            "8) \n",
            "Braverman E.M. (1970). Methods of extremal grouping of parameters and \n",
            "problem of apportionment of essential factors. Automation and Remote \n",
            "Control (1), 108\u2013116 \n",
            "9) \n",
            "Cochran, R. N., Horne F. H. (1977). Statistically weighted principal \n",
            "component analysis of rapid scanning wavelength kinetics experiments. \n",
            "Anal. Chem. 49, 846-853. \n",
            "10) Delicado P. (2001). Another look at principal curves and surfaces. Journal \n",
            "of Multivariate Analysis 77, 84\u2013116  \n",
            "11) Dempster A., Laird N., and Rubin D. (1977). Maximum likelihood from \n",
            "incomplete data via the EM algorithm. Journal of the Royal Statistical \n",
            "Society, Series B, 39(1):1-38. \n",
            "\n",
            " \n",
            " \n",
            "12) Diday E. (1979). Optimisation en classification automatique, Tome 1,2. \n",
            "INRIA, Rocquencourt (in French)  \n",
            "13) Durbin R., Willshaw D. (1987). An analogue approach to the travelling \n",
            "salesman problem using an elastic net method. Nature 326, 689\u2013691. \n",
            "14) Efron, B. (1967). The two sample problem with censored data. Proc. Fifth \n",
            "Berkeley Simp. Math. Statist. Probab. 4, 831-853. Univ.California Press, \n",
            "Berkeley. \n",
            "15) Einbeck, J., Tutz, G., and Evers, L. (2005). Local principal curves. Statistics \n",
            "and Computing 15, 301\u2013313. \n",
            "16) Einbeck J., Evers L., and Bailer-Jones C. (2008). Representing Complex \n",
            "Data Using Localized Principal Components With Application To \n",
            "Astronomical Data. In Gorban A., K\u00e9gl B., Wunch D., Zinovyev A. (Ed.) \n",
            "Principal Manifolds for Data Visualization and Dimension Reduction, \n",
            "Lecture Notes in Computational Science and Engineering 58 (pp. 178-201). \n",
            "Springer, Berlin-Heidelberg. \n",
            "17) Erwin E., Obermayer K., and Schulten K. (1992). Self-organizing maps: \n",
            "ordering, convergence properties and energy functions. Biological \n",
            "Cybernetics 67, 47\u201355. \n",
            "18) Fisher R.A. (1936). The Use of Multiple Measurements in Taxonomic \n",
            "Problems. Annals of Eugenics 7, 179\u2013188. \n",
            "19) Flury B. (1990). Principal points. Biometrika, 77, 33-41. \n",
            "20) Fr\u00e9chet, M. (1948). Les \u00e9lements al\u00e9atoires de nature quelconque dans un \n",
            "espace distanci\u00e9. Ann. Inst. H. Poincar\u00e9 10, 215\u2013310. \n",
            "21) Fukunaga K., Olsen D. (1971). An algorithm for finding intrinsic \n",
            "dimensionality of data. IEEE Transactions on Computers 20 (2), 176\u2013183 \n",
            "22) Gabriel K. R. and Zamir S. (1979), Lower rank approximation of matrices \n",
            "by least squares with any choices of weights, Technometrics, 21, pp. 298-\n",
            "489.  \n",
            "23) Gorban A.N., Rossiev A.A.. (1999). Neural network iterative method of \n",
            "principal curves for data with gaps.  J Comput Sys Sc Int 38 (5), 825-830. \n",
            "24) Gorban A.N., Pitenko A.A., Zinov'ev A.Y., Wunsch D.C. (2001). \n",
            "Vizualization of any data using elastic map method. Smart Engineering \n",
            "System Design 11, 363-368. \n",
            "25) Gorban, A.N., Zinovyev, A.Yu., and Wunsch, D.C. (2003). Application of \n",
            "the method of elastic maps in analysis of genetic texts. In Proceedings of \n",
            "International Joint Conference on Neural Networks. IJCNN Portland, \n",
            "Oregon, July 20-24. \n",
            "26) Gorban A., Zinovyev A. (2005). Elastic Principal Graphs and Manifolds and \n",
            "their Practical Applications. Computing 75,359 -379 \n",
            "27) Gorban A., Sumner N., Zinovyev A. (2007). Topological grammars for data \n",
            "approximation. Applied Mathematics Letters 20(4), 382-386. \n",
            "28) Gorban A., K\u00e9gl B., Wunsch D., Zinovyev A. (Ed.) (2008). Principal \n",
            "Manifolds for Data Visualization and Dimension Reduction. Lecture Notes \n",
            "in Computational Science and Engineering, Vol. 58:  Berlin-Heidelberg, \n",
            "Springer. \n",
            "29) Gorban A., Sumner N.R., Zinovyev A. (2008). Beyond The Concept of \n",
            "Manifolds: Principal Trees, Metro Maps, and Elastic Cubic Complexes. In \n",
            "Gorban A., K\u00e9gl B., Wunsch D., Zinovyev A. (Ed.) Principal Manifolds for \n",
            "Data Visualization and Dimension Reduction, Lecture Notes in \n",
            "\n",
            " \n",
            " \n",
            "Computational Science and Engineering 58 (pp. 219-237). Springer, Berlin-\n",
            "Heidelberg. \n",
            "30) Gorban A., Zinovyev A. (2008a). Elastic Maps and Nets for Approximating \n",
            "Principal Manifolds and Their Application to Microarray Data \n",
            "Visualization. In Gorban A., K\u00e9gl B., Wunsch D., Zinovyev A. (Ed.) \n",
            "Principal Manifolds for Data Visualization and Dimension Reduction, \n",
            "Lecture Notes in Computational Science and Engineering 58 (pp. 96-130). \n",
            "Springer, Berlin-Heidelberg. \n",
            "31) Gorban A., Zinovyev A. (2008b). PCA and K-means decipher genome. In \n",
            "Gorban A., K\u00e9gl B., Wunsch D., Zinovyev A. (Ed.) Principal Manifolds for \n",
            "Data Visualization and Dimension Reduction, Lecture Notes in \n",
            "Computational Science and Engineering 58 (pp. 309-323). Springer, Berlin-\n",
            "Heidelberg. \n",
            "32) Hartigan, J.A. (1975). Clustering Algorithms. Wiley, New-York. \n",
            "33) Hastie T. (1984). Principal Curves and Surfaces. PhD Thesis, Stanford \n",
            "University, California. \n",
            "34) Hotelling H. (1933). Analisys of a complex of statistical variables into \n",
            "principal components. Journal of Educational Psychology 24, 417-441. \n",
            "35) Ivakhno S. and Armstrong J.D. Non-linear dimensionality reduction of \n",
            "signalling networks. BMC Systems Biology 2007, 1:27. \n",
            "36) Jolliffe I.T. (2002). Principal Component Analysis, Series: Springer Series \n",
            "in Statistics, 2nd ed., XXIX. Springer, NY.  \n",
            "37) Kambhatla, N. and Leen, T. K. (1997). Dimension reduction by local PCA. \n",
            "Neural Computation 9, 1493\u20131516  \n",
            "38) Karhunen K. (1946). Zur Spektraltheorie Stochastischer Prozesse. \n",
            "Ann.Acad. Sci. Fennicae, 37. \n",
            "39) K\u00e9gl B. (1999). Principal curves: learning, design, and applications. Ph.D. \n",
            "Thesis, Concordia University, Canada. \n",
            "40) K\u00e9gl B., Krzyzak A. (2002). Piecewise linear skeletonization using principal \n",
            "curves. IEEE Transactions on Pattern Analysis and Machine Intelligence \n",
            "24(1), 59-74. \n",
            "41) Kohonen, T. (1982). Self-organized formation of topologically correct \n",
            "feature maps. Biological Cybernetics 43, 59\u201369  \n",
            "42) Kohonen T. (1997). Self-Organizing Maps. Springer: Berlin \u2013 Heidelberg. \n",
            "43) Koren Y., & Carmel L. (2004). Robust linear dimensionality \n",
            "reduction. IEEE Transactions on Visualisation and Computer Graphics, 10 \n",
            "(4), 459\u2014470. \n",
            "44) Kruskal J.B. (1964). Multidimensional scaling by optimizing goodness of fit \n",
            "to a nonmetric hypothesis. Psychometrika 29: 1\u201327. \n",
            "45) Leung Y.F. and Cavalieri D. (2003). Fundamentals of cDNA microarray \n",
            "data analysis. Trends Genet. 19(11), 649:659. \n",
            "46) Little, R.J.A. and Rubin, D.B. (1987). Statistical Analysis with Missing \n",
            "Data. New York: John Wiley. \n",
            "47) Liu Z.-Y., Chiu K.-C., and Xu L. (2003). Improved system for object \n",
            "detection and star/galaxy classification via local subspace analysis. Neural \n",
            "Networks 16, 437\u2013451 \n",
            "48) Lloyd S. (1957). Least square quantization in PCM's. Bell Telephone \n",
            "Laboratories Paper. \n",
            "49) Lo\u00e8ve M.M. (1955). Probability Theory. Princeton, N.J.: VanNostrand. \n",
            "\n",
            " \n",
            " \n",
            "50) L\u00f6we, M. (1993). Algebraic approach to single\u2013pushout graph \n",
            "transformation. Theor. Comp. Sci. 109, 181\u2013224. \n",
            "51) Lumley J.L. (1967). The Structure of Inhomogeneous Turbulent Flows. In \n",
            "Yaglom A.M., Tatarski V.I (Eds.) Atmospheric turbulence and radio \n",
            "propagation (pp. 166-178). Moscow: Nauka.  \n",
            "52) MacQueen J. (1967). Some methods for classification and analysis of \n",
            "multivariate observations. In Proc. 5th Berkeley Symp. on Math. Statistics \n",
            "and Probability, pages 281-297. \n",
            "53) Mirkin, B. (2005). Clustering for Data Mining: A Data Recovery Approach. \n",
            "Chapman and Hall, Boca Raton. \n",
            "54) Mulier F., Cherkassky V. (1995). Self-organization as an iterative kernel \n",
            "smoothing process. Neural Computation 7, 1165\u20131177. \n",
            "55) Nadler B., Lafon S., Coifman R., Kevrekidis I. G. (2008). Diffusion Maps - \n",
            "a Probabilistic Interpretation for Spectral Embedding and Clustering \n",
            "Algorithms. In Gorban A., K\u00e9gl B., Wunch D., Zinovyev A. (Ed.) Principal \n",
            "Manifolds for Data Visualization and Dimension Reduction, Lecture Notes \n",
            "in Computational Science and Engineering 58 (pp. 69-96). Springer, Berlin-\n",
            "Heidelberg. \n",
            "56) Nagl, M. (1976). Formal languages of labelled graphs. Computing 16, 113\u2013\n",
            "137. \n",
            "57) Ostrovsky R., Rabani Y., Schulman L.J., Swamy C. (2006). The \n",
            "Effectiveness of Lloyd-Type Methods for the k-Means Problem. In \n",
            "Proceedings of the 47th Annual IEEE Symposium on Foundations of \n",
            "Computer Science  (pp. 165-176). IEEE Computer Society,  Washington, \n",
            "DC, USA. \n",
            "58)  Pearson K. (1901). On lines and planes of closest fit to systems of points in \n",
            "space. Philosophical Magazine, series 6 (2), 559\u2013572 \n",
            "59) Pelleg D., Moore A. (1999). Accelerating Exact k-means Algorithms with \n",
            "Geometric Reasoning. Proceedings of the Fifth International Conference on \n",
            "Knowledge Discovery in Databases, 277-281. AAAI Press \n",
            "60) Ritter H., Martinetz T. and Schulten K. (1992). Neural Computation and \n",
            "Self-Organizing Maps: An Introduction. Addison-Wesley Reading, \n",
            "Massachusetts. \n",
            "61) Roweis S. (1998). EM algorithms for PCA and SPCA. In Advances in \n",
            "Neural Information Processing Systems (pp. 626-632). MIT Press, \n",
            "Cambridge, MA (USA).  \n",
            "62) Roweis S., Saul L. (2000). Nonlinear dimensionality reduction by locally \n",
            "linear embedding. Science 290: 2323-2326. \n",
            "63) Sch\u00f6lkopf B., Smola A.J., M\u00fcller K.-R. (1997). Kernel Principal \n",
            "Component Analysis. ICANN: 583-588.  \n",
            "64) Smola A.J., Mika S., Sch\u00f6lkopf B., Williamson R.C. (2001). Regularized \n",
            "Principal Manifolds. Journal of Machine Learning Research 1: 179-209. \n",
            "65) Steinhaus H. (1956). Sur la division des corps materiels en parties. Bull. \n",
            "Acad. Polon. Sci., C1. III vol IV: 801-804. \n",
            "66) Strang G. (1993). The Fundamental Theorem of Linear Algebra. The \n",
            "American Mathematical Monthly 100 (9), pp. 848-855 \n",
            "67) Sylvester J.J. (1889). On the reduction of a bilinear quantic of the nth order \n",
            "to the form of a sum of n products by a double orthogonal substitution. \n",
            "Messenger of Mathematics 19, 42\u201446. \n",
            "\n",
            " \n",
            " \n",
            "68) Tarpey T., & Flury B. (1996). Self-consistency: a fundamental concept in \n",
            "statistics. Statistical Science, 11(3), 229-243. \n",
            "69) Tenenbaum J. B., de Silva V. and Langford J. C. (2000). A global geometric \n",
            "framework for nonlinear dimensionality reduction. Science 290: 2319-2323. \n",
            "70) Tibshirani R. (1992). Principal curves revisited. Statistics and \n",
            "Computation 2, 183-190. \n",
            "71) Verbeek J. J., Vlassis N., Kr\u00f6se B. (2002). A k-segments algorithm for \n",
            "finding principal curves. Pattern Recognition Letters 23 (8), 1009-1017. \n",
            "72) Williams C.K.I. (2002). On a connection between Kernel PCA and metric \n",
            "multidimensional scaling. Machine Learning 46, 11-19.  \n",
            "73) Wang Y., Klijn J.G., Zhang Y. et al. (2005). Gene expression profiles to \n",
            "predict distant metastasis of lymph-node-negative primary breast cancer. \n",
            "Lancet 365: 671:679. \n",
            "74) Xu R., Wunsch D. (2008). Clustering, IEEE Press Series on Computational \n",
            "Intelligence, John Wiley & Sons, NY. \n",
            "75) Yin H. (2008). Learning Nonlinear Principal Manifolds by Self-Organising \n",
            "Maps, In Gorban A., K\u00e9gl B., Wunsch D., Zinovyev A. (Ed.) Principal \n",
            "Manifolds for Data Visualization and Dimension Reduction, Lecture Notes \n",
            "in Computational Science and Engineering 58 (pp. 69-96). Springer, Berlin-\n",
            "Heidelberg. \n",
            "76) Zou H., Hastie T. (2005). Regularization and variable selection via the \n",
            "elastic net. J. R. Statist. Soc. B 67, Part 2, 301\u2013320. \n",
            "BIOGRAPHY \n",
            " \n",
            "Prof. Dr. Alexander Gorban obtained his PhD degree in differential equations and \n",
            "mathematical physics in 1980, and Dr. Sc. degree in biophysics in 1990. He holds \n",
            "now a chair of Applied Mathematics at the University of Leicester, UK, and he is \n",
            "the Chief Scientist at the Institute for Computational Modelling Russian Academy \n",
            "of Sciences (Krasnoyarsk, Russia). His scientific interest include interdisciplinary \n",
            "problem of model reduction, topological dynamics, physical and chemical \n",
            "kinetics, mathematical biology and data mining. \n",
            " \n",
            "Dr. Andrei Zinovyev obtained his university formation in theoretical physics \n",
            "(cosmology). In 2001 he obtained his PhD degree in Computer Science at the \n",
            "Institute for Computational Modelling of Russian Academy of Sciences \n",
            "(Krasnoyarsk, Russia). After defending his PhD, he moved to France, at Institut \n",
            "des Hautes Etudes Scientifiques in Bures-sur-Yvette to work as a post-doc during \n",
            "3 years in the mathematical biology group of Professor Misha Gromov. Since \n",
            "January 2005, he is the head of the computational systems biology of cancer team \n",
            "at Institut Curie (INSERM Unit U900 \u2015Bioinformatics and Computational \n",
            "Systems Biology of Cancer\u2016) in Paris. His main area of scientific expertise is \n",
            "bioinformatics, systems biology of cancer, dimension reduction in high-\n",
            "throughput data analysis and model reduction in the dynamical models. \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "KEY TERMS AND THEIR DEFINITIONS \n",
            " \n",
            "Principal components: such an orthonormal basis in which the covariance matrix \n",
            "is diagonal. \n",
            "Principal manifold: intuitively, a smooth manifold going through the middle of \n",
            "data cloud; formally, there exist several definitions for the case of data \n",
            "distributions: 1) Hastie and Stuelze\u2018s principal manifolds are self-consistent \n",
            "curves and surfaces; 2) Kegl\u2018s principal curves provide the minimal mean squared \n",
            "error given the limited curve length; 3) Tibshirani\u2018s principal curves maximize the \n",
            "likelihood of the additive noise data model; 4) Gorban and Zinovyev elastic \n",
            "principal manifolds minimize a mean square error functional regularized by \n",
            "addition of energy of manifold stretching and bending; 5) Smola\u2018s regularized \n",
            "principal manifolds minimize some form of a regularized quantization error \n",
            "functional;   and some other definitions. \n",
            "Principal graph: a graph embedded in the multidimensional data space, \n",
            "providing the minimal mean squared distance to the dataset combined with \n",
            "deviation from an \u2015ideal\u2016 configuration (for example, from pluriharmonic graph) \n",
            "and not exceeding some limits on complexity (in terms of the number of structural \n",
            "elements and the number of graph grammar transformations needed for obtaining \n",
            "the principal graph from some minimal graph). \n",
            "Self-consistent approximation: approximation of a dataset by a set of vectors \n",
            "such that every point y in the vector set is a conditional mean of all points from  \n",
            "dataset that are projected in y. \n",
            "Expectation/Maximisation algorithm: generic splitting algorithmic scheme with \n",
            "use of which almost all algorithms for estimating principal objects are \n",
            "constructed; it consists of two basic steps: 1) projection step, at which the data is \n",
            "projected onto the approximator, and 2) maximization step, at which the \n",
            "approximator is optimized given the projections obtained at the previous step. \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "Principal Graphs and Manifolds \n",
            " \n",
            "Alexander Gorban \n",
            "Department of Mathematics, University of Leicester \n",
            "University Road, Leicester LE1 7RH, United Kingdom \n",
            "+44 (0) 116 223 14 33 \n",
            "ag153@le.ac.uk \n",
            " \n",
            "Andrei Zinovyev \n",
            "Institut Curie \n",
            "26 rue d\u2018Ulm, Paris 75248 \n",
            "+33 (0) 56 24 69 89 \n",
            "andrei.zinovyev@curie.fr \n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_text.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "kPG1QgvX7BtO",
        "outputId": "7bab3bde-8ab7-4ecc-f984-bc2507e57b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            id                        author  \\\n",
              "0  0809.0490v2  A. N. Gorban, A. Y. Zinovyev   \n",
              "\n",
              "                                             summary  \\\n",
              "0  In many physical, statistical, biological and ...   \n",
              "\n",
              "                           pdf_link                   tags  \\\n",
              "0  http://arxiv.org/pdf/0809.0490v2  cs.LG, cs.NE, stat.ML   \n",
              "\n",
              "                                          paper_text  \n",
              "0   \\n \\nPrincipal Graphs and Manifolds \\n \\n \\nA...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7aaed057-2b12-4d98-a63f-5828955b485d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>author</th>\n",
              "      <th>summary</th>\n",
              "      <th>pdf_link</th>\n",
              "      <th>tags</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0809.0490v2</td>\n",
              "      <td>A. N. Gorban, A. Y. Zinovyev</td>\n",
              "      <td>In many physical, statistical, biological and ...</td>\n",
              "      <td>http://arxiv.org/pdf/0809.0490v2</td>\n",
              "      <td>cs.LG, cs.NE, stat.ML</td>\n",
              "      <td>\\n \\nPrincipal Graphs and Manifolds \\n \\n \\nA...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7aaed057-2b12-4d98-a63f-5828955b485d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7aaed057-2b12-4d98-a63f-5828955b485d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7aaed057-2b12-4d98-a63f-5828955b485d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_with_text",
              "summary": "{\n  \"name\": \"df_with_text\",\n  \"rows\": 4945,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4945,\n        \"samples\": [\n          \"1208.3530v1\",\n          \"1704.03162v2\",\n          \"1309.6933v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4847,\n        \"samples\": [\n          \"Wolfgang Gatterbauer, Dan Suciu\",\n          \"Abhishek Kumar, Suresh Chandra Gupta\",\n          \"Guillaume Noyel, Michel Jourlin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4945,\n        \"samples\": [\n          \"The New York Public Library is participating in the Chronicling America\\ninitiative to develop an online searchable database of historically significant\\nnewspaper articles. Microfilm copies of the newspapers are scanned and high\\nresolution Optical Character Recognition (OCR) software is run on them. The\\ntext from the OCR provides a wealth of data and opinion for researchers and\\nhistorians. However, categorization of articles provided by the OCR engine is\\nrudimentary and a large number of the articles are labeled editorial without\\nfurther grouping. Manually sorting articles into fine-grained categories is\\ntime consuming if not impossible given the size of the corpus. This paper\\nstudies techniques for automatic categorization of newspaper articles so as to\\nenhance search and retrieval on the archive. We explore unsupervised (e.g.\\nKMeans) and semi-supervised (e.g. constrained clustering) learning algorithms\\nto develop article categorization schemes geared towards the needs of\\nend-users. A pilot study was designed to understand whether there was unanimous\\nagreement amongst patrons regarding how articles can be categorized. It was\\nfound that the task was very subjective and consequently automated algorithms\\nthat could deal with subjective labels were used. While the small scale pilot\\nstudy was extremely helpful in designing machine learning algorithms, a much\\nlarger system needs to be developed to collect annotations from users of the\\narchive. The \\\"BODHI\\\" system currently being developed is a step in that\\ndirection, allowing users to correct wrongly scanned OCR and providing keywords\\nand tags for newspaper articles used frequently. On successful implementation\\nof the beta version of this system, we hope that it can be integrated with\\nexisting software being developed for the Chronicling America project.\",\n          \"This paper presents a new baseline for visual question answering task. Given\\nan image and a question in natural language, our model produces accurate\\nanswers according to the content of the image. Our model, while being\\narchitecturally simple and relatively small in terms of trainable parameters,\\nsets a new state of the art on both unbalanced and balanced VQA benchmark. On\\nVQA 1.0 open ended challenge, our model achieves 64.6% accuracy on the\\ntest-standard set without using additional data, an improvement of 0.4% over\\nstate of the art, and on newly released VQA 2.0, our model scores 59.7% on\\nvalidation set outperforming best previously reported results by 0.5%. The\\nresults presented in this paper are especially interesting because very similar\\nmodels have been tried before but significantly lower performance were\\nreported. In light of the new results we hope to see more meaningful research\\non visual question answering in the future.\",\n          \"We consider the problem of providing nonparametric confidence guarantees for\\nundirected graphs under weak assumptions. In particular, we do not assume\\nsparsity, incoherence or Normality. We allow the dimension $D$ to increase with\\nthe sample size $n$. First, we prove lower bounds that show that if we want\\naccurate inferences with low assumptions then there are limitations on the\\ndimension as a function of sample size. When the dimension increases slowly\\nwith sample size, we show that methods based on Normal approximations and on\\nthe bootstrap lead to valid inferences and we provide Berry-Esseen bounds on\\nthe accuracy of the Normal approximation. When the dimension is large relative\\nto sample size, accurate inferences for graphs under low assumptions are not\\npossible. Instead we propose to estimate something less demanding than the\\nentire partial correlation graph. In particular, we consider: cluster graphs,\\nrestricted partial correlation graphs and correlation graphs.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4945,\n        \"samples\": [\n          \"http://arxiv.org/pdf/1208.3530v1\",\n          \"http://arxiv.org/pdf/1704.03162v2\",\n          \"http://arxiv.org/pdf/1309.6933v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1224,\n        \"samples\": [\n          \"cs.AI, cs.CL, cs.GT\",\n          \"cs.LG, cs.AI, cs.NE\",\n          \"cs.DC, cs.AI, H.3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4945,\n        \"samples\": [\n          \"A\\nLeveraging Subjective Human Annotation for Clustering Historic\\nNewspaper Articles\\nHaimonti Dutta, The Center for Computational Learning Systems\\nWilliam Chan, Department of Computer Science\\nDeepak Shankargouda, Department of Computer Science\\nManoj Pooleery, The Center for Computational Learning Systems\\nAxinia Radeva, The Center for Computational Learning Systems\\nKyle Rego, Department of Computer Science\\nBoyi Xie, The Center for Computational Learning Systems\\nRebecca J. Passonneau, The Center for Computational Learning Systems\\nAustin Lee, The Center for Computational Learning Systems\\nBarbara Taranto, New York Public Library\\nThe New York Public Library is participating in the Chronicling America initiative to develop an online\\nsearchable database of historically signi\\ufb01cant newspaper articles. Micro\\ufb01lm copies of the newspapers are\\nscanned and high resolution Optical Character Recognition (OCR) software is run on them. The text from\\nthe OCR provides a wealth of data and opinion for researchers and historians. However, categorization of\\narticles provided by the OCR engine is rudimentary and a large number of the articles are labeled \\u201cedi-\\ntorial\\u201d without further grouping. Manually sorting articles into \\ufb01ne-grained categories is time consuming\\nif not impossible given the size of the corpus. This paper studies techniques for automatic categorization\\nof newspaper articles so as to enhance search and retrieval on the archive. We explore unsupervised (e.g.\\nKMeans) and semi-supervised (e.g. constrained clustering) learning algorithms to develop article categoriza-\\ntion schemes geared towards the needs of end-users. A pilot study was designed to understand whether there\\nwas unanimous agreement amongst patrons regarding how articles can be categorized. It was found that the\\ntask was very subjective and consequently automated algorithms that could deal with subjective labels were\\nused. While the small scale pilot study was extremely helpful in designing machine learning algorithms, a\\nmuch larger system needs to be developed to collect annotations from users of the archive. The \\u201cBODHI\\u201d\\nsystem currently being developed is a step in that direction, allowing users to correct wrongly scanned OCR\\nand providing keywords and tags for newspaper articles used frequently. On successful implementation of\\nthe beta version of this system, we hope that it can be integrated with existing software being developed\\nfor the Chronicling America project.\\nCategories and Subject Descriptors: C.2.2 [Computer Science-Machine Learning]: Unsupervised Learn-\\ning\\nGeneral Terms: Machine Learning Algorithms, Subjectivity, Unsupervised Learning\\nAdditional Key Words and Phrases: human judgement, annotation, unsupervised learning, subjective an-\\nnotation, kmeans, constrained clustering\\nAuthor\\u2019s addresses: H. Dutta and R. J. Passonneau and B. Xie and M. Pooleery and A. Radeva, The\\nCenter for Computational Learning Systems, Columbia University, New York, NY 10115; W. Chan and\\nD. Shankargouda and K. Rego, Department of Computer Science, Columbia University; A. Lee, SyncSort\\nInc. This work was done when A. Lee was a\\ufb03liated to The Center for Computational Learning Systems,\\nColumbia University; B. Taranto, New York Public Library, NYPL.\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is\\ngranted without fee provided that copies are not made or distributed for pro\\ufb01t or commercial advantage\\nand that copies show this notice on the \\ufb01rst page or initial screen of a display along with the full citation.\\nCopyrights for components of this work owned by others than ACM must be honored. Abstracting with\\ncredit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any\\ncomponent of this work in other works requires prior speci\\ufb01c permission and/or a fee. Permissions may be\\nrequested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA,\\nfax +1 (212) 869-0481, or permissions@acm.org.\\nc\\u20ddYYYY ACM 1539-9087/YYYY/01-ARTA $10.00\\nDOI 10.1145/0000000.0000000 http://doi.acm.org/10.1145/0000000.0000000\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\narXiv:1208.3530v1  [cs.IR]  17 Aug 2012\\n\\nA:2\\nACM Reference Format:\\nDutta, H., Passonneau, R. J., Chan, W., Xie, B., Pooleery, M., Radeva, A., Shankargouda, D., Rego, K.,\\nLee, A., Taranto, B. 2012. Leveraging Subjective Human Annotation for Clustering Historic Newspaper\\nArticles. ACM Trans. Know. Dis. from Data V, N, Article A (January YYYY), 28 pages.\\nDOI = 10.1145/0000000.0000000 http://doi.acm.org/10.1145/0000000.0000000\\n1. INTRODUCTION\\nChronicling America (http://chroniclingamerica.loc.gov/) is a website that provides\\naccess to over 3 million historic newspapers scanned by the National Digital Newspaper Pro-\\ngram (NDNP). It is an initiative of the National Endowment for Humanities (NEH) and the\\nLibrary of Congress (LC) and its goal is to develop an online, searchable database of his-\\ntorically signi\\ufb01cant newspapers between 1836 and 1922. State libraries, historical societies\\nand universities have been funded by the NEH to generate scanned newspaper pages repre-\\nsenting the state\\u2019s regional history, geographic coverage, and events which will be archived\\nby the LC. The New York Public Library (NYPL) is part of this initiative and has scanned\\n200,000 newspaper pages published between 1860 and 1920 from micro\\ufb01lm.\\nWhile the images scanned by the NYPL are sent to the Library of Congress for hosting\\non the Chronicling America website, duplicate copies of the archive are also available at\\nNYPL for use by its patrons. These digitized pages o\\ufb00er a wealth of data for researchers,\\nhistorians, genealogists and other patrons of the library. For example, the opening of the\\nBrooklyn Bridge in 1883, the construction of an immigration station at Ellis Island in 1890,\\nthe historic opening of the Metropolitan Opera House on Broadway and 39th Street are\\nsome interesting local news items of the time. Other topics widely covered by the American\\npress include presidential administrations (Cleveland (1885 \\u2013 1894), Gar\\ufb01eld (1880 - 1883),\\nMcKinley (1897 - 1901), Theodore Roosevelt (1901 - 1912)), natural calamities (Galveston\\n\\ufb02ood of 1900, San Francisco earthquake 1906), the sinking of the Titanic, events pertaining\\nto the \\ufb01rst world war and news from the world of medicine (patented medicines, spread of\\nepidemics, new discoveries). To e\\ufb00ectively use this archive, developing sophisticated search\\nand retrieval mechanisms is crucial.\\nIn order to make a newspaper available for searching on the Internet, the following processes\\nmust take place:\\n(1) The micro\\ufb01lm copy or original paper is scanned.\\n(2) Master and Web image \\ufb01les are generated.\\n(3) Metadata is assigned for each page to improve the search capability of the newspaper.\\n(4) Optical Character Recognition (OCR) software is run over high resolution images to\\ncreate searchable full text, and\\n(5) OCR text, images, and metadata are imported into a digital library software program.\\nThe newspaper archives can currently be searched using the OpenSearch protocol1. Unfor-\\ntunately, these search facilities are rudimentary and irrelevant documents are often more\\nhighly ranked than relevant ones. For instance consider a search for a natural calamity like\\nthe April 18th, 1906 San Francisco earthquake which killed approximately 2000 people and\\nmeasured 7.8 on the Richter scale; if the keywords \\u201cearthquake San Francisco\\u201d are entered\\nas the search criteria in the digitized newspaper archive along with a date range 01/01/1906\\nto 12/31/1906, the \\ufb01rst document returned is Page 7, April 19th of the 1906 Los Angeles\\nHerald with an article \\u201cFifty people killed at San Jose\\u201d (the word \\u201cSan\\u201d is tagged); the\\nsecond document returned is the June 3rd, 1906 issue of \\u201cThe San Francisco Sunday Call\\u201d\\nwith a full-page illustration of a drama by Frederick Irons Bamford and the third document\\nis page 13, June 3rd, 1906 issue of \\u201cThe Sunday Call\\u201d with a cartoon of \\u201cMajor ozone\\u2032s\\nfresh air crusade\\u201d. The retrieval technique missed \\ufb01nding Page 1, April 19th, 1906 of the\\n1http://www.opensearch.org/Home\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:3\\nnewspaper \\u201cThe Sun\\u201d published from New York, which had a headline article \\u201cEarthquake\\nlays Frisco in ruins\\u201d.\\nTable I. The Original Text in the article versus the scanned OCR.\\nARMY BILL PROVISIONS.\\nJLRMY BILL PROVISIONS\\nINCREASE AND REORGANIZATION\\nINCREASE AND REORCAMZATICN\\nRANKS MAY REACH ONE HUNDRED THOU-\\nHANKS MAT REACH ONE HUNDRED THOU\\nSAND IN EMERGENCIES-FILIPONOS AND\\nSAND IN EMERGENCIES Ai FILIPINOS AND\\nPORTO RICANS TO BE ENLISTED.\\nPORTO RICANS TO BE ENLISTED.\\n[BY TELEGRAPH TO THE TRIBUNE]\\nIBT TCLEGSAPB TO TCS TRIBUNE]\\nWashington, Nov. 30-The approved Army In-\\n\\u201cWashington, Nov. Ai-he approved Army in-\\ncrease and Reorganization bill agreed upon by\\ncrease and Reorganization bill agreed u(\\u00bfon by\\nSecretary Root and General Corbin on the one\\nSecretary Root and General Corbin on the one\\nhand and the Senate and House Military com-\\nhand and the Senate and House Military com-\\nmittees on the other is an elaborate measure of\\nrr.ittees on the other is an elaborate measure of\\nthirty-nine sections, providing for a detailed in\\nthirty-nine sections, providing for a detailed in\\nstead of a permanent sta\\ufb00; for an artillery\\nstead of a permanent sta\\ufb00; for an artillery\\ncorps, coast and \\ufb01eld, instead of regiments; for\\ncorps, coast and \\ufb01eld, instead of regiments; for\\nan Army of 60,000, including o\\ufb03cers to be in-\\nen Army of Go.\\u00a1\\u201d-\\u201dincluding of3eers to be in\\ncreased in ranks to 100,000 in emergencies, with\\ncreased in ranks to 100.000 in emergencies, with\\nCongressional approval;\\nCongressional approval;\\nOn investigating the reasons why irrelevant documents are ranked higher, it was found\\nthat the newspapers are scanned on a page-by-page basis and article level segmentation\\nis poor or non-existent. The OCR scanning process is far from perfect and the documents\\ngenerated from it contains a large amount of garbled text. Table I (Left) shows the text\\nin an article and (Right) the garbled text generated by the scanning process. In addition,\\ncategorization of article level data using the OCR software was not very successful \\u2013 most of\\nthe articles are labeled \\u201ceditorial\\u201d and there is no \\ufb01ne grained classi\\ufb01cation into categories\\nsuch as crime, politics, medicine, etc. For example, an attempt to categorize articles in the\\nedition of The Sun newspaper published on November 4th, 1894 resulted in 338 articles\\nclassi\\ufb01ed as editorial, 32 unclassi\\ufb01ed2, 10 sports, 23 advertising, 5 commercial, 3 birth-\\nrelated announcements, and 2 reviews.\\nEven though the New York Public Library has put in substantial e\\ufb00ort into improving the\\nquality of the images and text obtained from OCR by testing each word scanned against\\nenglish dictionaries, manually re-typing newspaper headlines and applying categories to\\narticles \\u2013 the digital outputs from the OCR software are not good enough to ensure adequate\\nquality of text retrieval or to meet user expectations. Consequently, we conjectured that\\na crowdsourcing project involving patrons of the library who use the archive frequently\\nfor research and learning could assist the task of categorizing articles of this huge online\\nrepository.\\nThis paper describes our experiences when setting up a pilot study on a randomly chosen\\nsample of newspaper articles from the archive. Annotators were asked to browse the articles\\nand come up with broad categories into which they thought the articles could be grouped;\\nthey were not given a pre-de\\ufb01ned list of categories to choose from. An attempt was made to\\nleverage the information provided by them (such as keywords and tags, labels) and study\\nwhether popular unsupervised3 machine learning and text mining algorithms bene\\ufb01ted from\\nsuch prior knowledge. In addition, annotators are evaluated based on the degree to which\\nthe additional knowledge they provide helps the learning task.\\nThis paper is organized as follows: Section 2 presents related work, Section 3 describes\\nthe characteristics of the data, Section 4 provides details of the pilot study and interprets\\n2These were later identi\\ufb01ed as banners of the newspaper.\\n3Since well-de\\ufb01ned labels are not easily available from the archive, unsupervised machine learning algorithms\\nwere preferred to the well known supervised counterparts.\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:4\\nthe results, Section 5 describes algorithms for incorporating domain knowledge into learning\\ntasks and empirical analysis performed on this data, Section 6 describes the implementation\\nof a system for correcting OCR text and collecting tags and Section 7 concludes the paper.\\n2. RELATED PRIOR WORK\\nIn recent years, the humanities have seen a transformation of scholarly information from\\nphysical media to digital form, resulting in the formation of large digital libraries (such as\\nARTstor4, Data-PASS5, Biodiversity Heritage Library6, English Broadside Ballad Archive7,\\nGreat War Primary Documents Archive8, National Archives of London9 ). Vast quantities\\nof datasets, manuscripts, reports and newspapers that might never have made their way into\\na traditional library are being made accessible to the general public using high-performance\\ncomputing, networking and storage.\\nSeveral digital humanities projects that have used machine learning and natural language\\nprocessing techniques to learn from historic newspaper archives are relevant to this work\\n\\u2013 the libraries of Richmond and Tufts have examined the Richmond Times Dispatch dur-\\ning the civil war years for more than two decades and their work focuses on automatic\\nidenti\\ufb01cation and analysis of full OCR text in newspapers to provide advanced searching,\\nbrowsing and visualization [Crane and Jones 2006]. The focus of this work was on named\\nentity extraction and ten categories prominent in these newspapers were studied including\\nship names, railroads, streets and organizations. In an earlier project at the universities, the\\nPerseus project ([Smith 2002a], [Smith 2002b], [Smith and Crane 2001]), a general system to\\nextract dates and names from text was developed. At the Hull Digital Library10, information\\ncapture and semantic indexing of the newspaper archive is done using document analysis\\nand classi\\ufb01cation [Esposito et al. 1997]. In Collection OCR, Sankar et. al. [Sankar K. et al.\\n2010] use an approximate fast nearest neighbor algorithm based on hierarchical K-Means\\n(HKM) to clean OCR text. In general, the focus of almost all of the prior work in digital\\nhumanities has been on language modeling and has not focused on categorization of articles\\ntaking into consideration subjectivity of human annotation. Finally, it must be noted that\\na preliminary version of this work using unsupervised learning algorithms was published\\nin [Dutta et al. 2011] and the problem of topic evolution in the historic newspaper archive\\nover time was explored by Lee et. al. [Lee et al. 2010].\\nRecruiting web users to tag and annotate text and images in large archives, especially when\\nthere are too many documents and objects for a single authority to label has become com-\\nmon practice. Several projects use the collective e\\ufb00ort of a large number of people to label\\ntext, images and annotate maps; with the advent of crowdsourcing services (such as Ama-\\nzon\\u2019s Mechanical Turk11, reCAPTCHA ([von Ahn et al. 2008], [Faymonville et al. 2009]),\\nthe LISTEN ([Turnbull et al. 2007]), ESP ([von Ahn and Dabbish 2004]) and Games with a\\nPurpose ([Ahn 2008]) paying people small sums of money to do \\u201cHuman Intelligence Tasks\\u201d\\n(HITs) is also becoming the norm. Such tasks include anything from labeling images, to\\nlistening to short pieces of audio, researching topics on the internet and scrubbing database\\nrecords. A number of recent papers have evaluated the e\\ufb00ectiveness of using Mechanical\\nTurk to create annotated data for text and natural language processing applications (see\\nfor e.g. [Snow et al. 2008], papers accepted at the \\u201cCreating Speech and Language Data\\n4www.artstor.org\\n5http://www.icpsr.umich.edu/DATAPASS/\\n6biodiversitylibrary.org\\n7ebba.english.ucsb.edu\\n8www.gwpda.org\\n9nationalarchives.gov.uk\\n10http://www2.hull.ac.uk/lli/libraries.aspx\\n11https://www.mturk.com/mturk/welcome\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:5\\nWith Amazon\\u2019s Mechanical Turk\\u201d Workshop12 co-located with HLT-NAACL 2010). While\\nit is relatively inexpensive to obtain a large number of labels from annotators, the question\\nof how to incorporate them into machine learning algorithms with theoretical guarantees\\non performance remains an open research problem.\\nIn the context of supervised learning tasks, Smyth et al. were the \\ufb01rst in the machine\\nlearning community to propose a solution to the problem of noisy labels in the context of\\nlabeling volcanoes in satellite images of Venus ([Smyth et al. 1994], [Burl et al. 1994], [Smyth\\net al. 1996]). They \\ufb01rst estimate the ground truth and then use probabilistic reasoning to\\nlearn the classi\\ufb01er. Raykar and his colleagues [Raykar et al. 2009] describe a probabilistic\\napproach when multiple annotators provide possibly noisy labels but there is no absolute\\ngold standard. Their algorithm iteratively establishes a particular gold standard, measures\\nthe performance of annotators given that gold standard and then re\\ufb01nes it based on perfor-\\nmance metrics. Key assumptions made by them include: a) performance of each annotator\\ndoes not depend on the feature vector and b) conditioned on the truth the experts are\\nindependent.\\n[Sheng et al. 2008] analyzed when it is worthwhile to acquire new labels for some training\\nexamples. They show that repeated labeling can improve label quality, but not always; when\\nlabels are noisy, repeated labeling can be preferable to single labeling even in the traditional\\nsetting where labels are not particularly cheap. An empirical study to examine the e\\ufb00ect of\\nnoisy annotations on the performance of sentiment classi\\ufb01cation models was performed by\\n[Hsueh et al. 2009]. More theoretical work on when it is useful to deal with multiple experts\\ncan be found in [Lugosi 1992], [Dekel and Shamir 2009b], [Dekel and Shamir 2009a].\\nAnother class of algorithms that needs discussion are semi-supervised clustering algo-\\nrithms. These algorithms fall in between totally unsupervised learning and totally super-\\nvised learning. The primary goal of such algorithms is to \\u201csteer\\u201d the clustering process\\nwith user feedback; also the clusters obtained by guidance from humans enable the users\\nto play with the data and understand it intuitively. Semi-supervised clustering algorithms\\ncan be broadly classi\\ufb01ed into two main genres: semi-supervised clustering of (a) labels and\\n(b) constraints ([Basu 2005]).\\nUsing labeled data, iterative feedback from users has been incorporated by [Cohn et al.\\n2003] and methods for using conditional distributions in auxiliary space are reported in\\n[Sinkkonen and Kaski 2002]. Seeding mechanisms for semi-supervised clustering have been\\nstudied by [Basu et al. 2002] and [Wagsta\\ufb00et al. 2001]. For iterative clustering algorithms\\n(such as K-Means), a common technique is to seed at random by arbitrarily creating K\\npartitions and choosing the mean of each partition as seeds. [Forgy 1965] proposed a variant\\nthat chooses K instances at random as seeds, then assigns the remaining instances to the\\ncluster represented by the nearest seed. [MacQueen 1967] recalculates the centroids after\\nthe assignment of instances to the cluster represented by the nearest seed. [Kaufman and\\nRousseeuw 1990] propose an elaborate mechanism of seed selection: the \\ufb01rst seed is the\\ninstance that is most central in the data; the rest of the representatives are selected by\\nchoosing instances that promise to be closer to more of the remaining instances. Other\\ninteresting seeding mechanisms include the Buckshot method of doing hierarchical clustering\\non a sample of data to get initial set of cluster centers ([Cutting et al. 1992]), selecting\\nthe k densest intervals along each co-ordinate to get the k cluster centers ([Bradley et al.\\n1997]) and re\\ufb01ning the initial seeds by taking into account the modes of the underlying\\ndistribution ([Bradley and Fayyad 1998]). In K-Means++ ([Arthur and Vassilvitskii 2007]),\\nthe random starting points are chosen with speci\\ufb01c probabilities. By augmenting K-Means\\nusing this simple, randomized seeding technique, K-Means++ is \\u03b8 (log K) competitive with\\nthe optimal clustering.\\n12http://sites.google.com/site/amtworkshop2010/home\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:6\\nIn semi-supervised clustering with constraints, the focus is on either: (a) similarity adapt-\\ning or (b) search-based methods. In similarity-adapting methods, an existing clustering al-\\ngorithm using some similarity measure is employed, but the similarity measure is adapted\\nto suit the problem (such as the Jensen-Shannon divergence trained with gradient descent\\n[Cohn et al. 2003], the Euclidean distance modi\\ufb01ed by a shortest-path algorithm [Klein\\net al. 2002], Mahalanobis distances adjusted by convex optimization [Bilenko and Mooney\\n2003], [Xing et al. 2002]. In search-based methods, the clustering algorithm itself is mod-\\ni\\ufb01ed so that user-provided constraints can be used to bias the search for an appropriate\\nclustering. This can be done in several ways, such as by performing a transitive closure of\\nthe constraints and using them to initialize clusters [Basu et al. 2002], by including in the\\ncost function a penalty for lack of compliance with the speci\\ufb01ed constraints [Demiriz et al.\\n1999], or by requiring constraints to be satis\\ufb01ed during cluster assignment in the clustering\\nprocess [Wagsta\\ufb00et al. 2001].\\nPairwise constrained semi-supervised clustering has also been studied by [Bansal et al.\\n2004], [Blum et al. 2004], [Kulis et al. 2009], [Lange et al. 2005], [Ge et al. 2007], [Davidson\\net al. 2006a]) and [Charikar et al. 2003] and a related approach based on Gaussian random\\n\\ufb01eld model is presented by [Zhuxj et al. 2003]. A probabilistic model for semi-supervised\\nclustering based on Hidden Markov Random Fields was studied by [Basu et al. 2004] where\\nthe goal is to perform partitional semi-supervised clustering of data by minimizing an ob-\\njective function derived from the posterior energy of the HMRF model.\\n[Cohn et al. 2003] note that \\u201csemi-supervised clustering assumes that human user has\\nin their mind criteria that enable them to evaluate the quality of clustering\\u201d. It does not\\nassume that the user is conscious of what they think de\\ufb01nes a good clustering but that, as\\nwith art, they will \\u201cknow it when they see it\\u201d. While this is very interesting, an important\\nobservation is that the subjectivity of labels assigned by humans is not discussed in the\\ncontext of evaluation of semi-supervised clustering; furthermore, several open questions\\nexist including incomplete seeding techniques (what if some cluster labels are not observed\\nin the labeled set available) and how to deal with noise in the data.\\n3. THE DATA\\nFigure 1 shows a scanned newspaper page (The Sun, November 2, 1894) from the NYPL\\narchive and an article from this paper. The historical newspaper archive contains two types\\nof XML \\ufb01les: (1) Page-Level XMLs: For each page of a newspaper, there is an XML \\ufb01le\\nthat contains metadata about the page and the text in it. Each word scanned is stored as a\\nstring in the page level XML (See Figure 2) along with possible alternative suggestions for\\nthe word13. The text is extracted from the page level XML using Xpath queries and stored\\nin a PostGreSQL database. (2) Issue-Level XMLs: The issue-level XMLs (illustrated in\\nTable 3) provide the following information about articles: (a) Headlines cleaned by humans\\nwhich are of much higher quality than the text produced by the OCR software. (b) Article\\nsegmentation information: Each newspaper article is represented as a collection of one or\\nmore text blocks whose pixel coordinates are available. This helps to determine where one\\narticle ends and the next one begins and is particularly useful when an article spans more\\nthan one page. (c) High-level categorization of the articles produced by the OCR software.\\nWe have access to only a subset of the NYPL archive14 \\u2013 issues of The Sun newspaper from\\nNovember 1, 1894 to December 31, 1894. (d) In addition, issue level XMLs also store the\\ndate of publication, volume number and issue number and provide pointers to the location\\nand names of the page-level XML \\ufb01les.\\nTable 4 shows all the categories found by the OCR software for \\u201cThe Sun\\u201d newspaper\\npublished between November 2nd, 1894 \\u2013 December 31st, 1894. Articles in the \\u201cedito-\\n13We found that its primary selections are usually better than their alternatives.\\n14These have been substantially cleaned by humans\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:7\\nFig. 1.\\n(Left) A newspaper page from the NYPL archive. The red-border shows an article from the news-\\npaper, zoomed in on the right hand \\ufb01gure.\\nFig. 2.\\nPage Level XML \\ufb01le showing the article with headline \\u201cGRANT ON THE EAST SIDE\\u201d.\\nrial/opinion\\u201d and \\u201csports\\u201d categories contain statistically signi\\ufb01cant amounts of text while\\nreviews, illustrations, birth/death/wedding announcements are not included in our study.\\nPre-processing: We preprocess the documents to reduce dimensionality and have clean\\ndata to learn from. For each article, a bag-of-words representation and tf-idf weights are\\nobtained. Stop words such as \\u201cthe\\u201d, \\u201cand\\u201d, etc. are removed from the set of words. Letters\\nof length three or less and words that contain digits or repeated characters (e.g. \\u201cpaaa\\u201d\\nand \\u201cornnn\\u201d) are also removed. After applying the above noise reduction techniques, the\\ndimensionality of the feature space is 3210.\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:8\\nFig. 3.\\nA segment of the issue-level XML \\ufb01le illustrating the OCR Classi\\ufb01cation (as \\u201carticle/editorial\\u201d) for\\nthe article and its headline.\\nCategory\\nArticle Counts\\nEditorial/Opinion\\n11,441\\nSports\\n764\\nAdvertising\\n683\\nCommercial/Legal/Public notices\\n361\\nBirth/Death/Wedding\\n158\\nReviews\\n45\\nIllustrations\\n2\\nUnclassi\\ufb01ed\\n785\\nTotal\\n14,239\\nFig. 4.\\nTop-level categories of articles from OCR software for The Sun newspaper between November 2nd,\\n1894 and December 31st, 1894.\\n4. CASE STUDY INVOLVING HUMAN ANNOTATORS\\nWe conducted a pilot study to test whether the category labeled \\u201carticle/editorial\\u201d by\\nthe OCR software could be further broken down to more meaningful sub-categories. Six\\nannotators were recruited to determine the number of natural categories found in a random\\nsample of twenty-\\ufb01ve articles. The articles (all labeled article/editorial by the OCR software)\\nwere selected from the November 2nd, 1894 issue of The Sun newspaper. All the annotators\\nwere given the same set of articles to work with. They were asked to skim the articles \\ufb01rst\\nand group them into obvious and intuitive categories and focusing on the \\u201cbigger picture\\u201d.\\nThe de\\ufb01ned categories had be described in 5 - 10 words and preferably had to include words\\nfrom the articles. Finally, they were interviewed with the following set of questions:\\n(1) What was the strategy you used for coming up with the categories?\\n(2) Were there any documents that you found di\\ufb03cult to assign to categories?\\n(3) Did you \\ufb01nd any part of the study particularly di\\ufb03cult or ambiguous? If so, describe\\nthe problem you faced.\\n(4) How long did it take you to complete the study?\\n(5) If you had the opportunity to change anything with this study, what would it be?\\nWhile there are many other interesting research questions that can be investigated with\\nhuman annotated data, the focus was on determining a meaningful number of sub-categories\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:9\\nTable II. Sub-Categories found by humans in\\nthe random sample of 25 articles labeled \\u201car-\\nticle/editorial\\u201d by the OCR software.\\nID\\nNumber of sub-categories\\nAnnotator 1\\n8\\nAnnotator 2\\n14\\nAnnotator 3\\n13\\nAnnotator 4\\n9\\nAnnotator 5\\n10\\nAnnotator 6\\n9\\nfor the \\u201carticle/editorial\\u201d category; thus reaction times, self-consistency among annotators\\nwere not emphasized.\\n4.1. Interpreting Results from the Pilot Study\\nTable II shows the number of categories found by the annotators. The November 2nd, 1894\\nnewspaper was published immediately after general elections; thus a lot of articles in this\\nissue had to do with politics and elections. This is also re\\ufb02ected in the random sample\\nused for the categorization task \\u2013 annotators unanimously agreed that seven of the twenty\\n\\ufb01ve articles used for the study belong to the category \\u201cpolitics/elections/governmental\\nappointments\\u201d. Three of the annotators found hierarchies among this category such as\\n\\u201cpolitics/ballot, politics/election, politics/nomination, politics/war, politics/social, poli-\\ntics/entertainment, politics/gossip\\u201d. This accounted for the increased number of total cat-\\negories they listed. Since the instructions explicitly mentioned focusing on the \\u201cbigger pic-\\nture\\u201d and not drilling down to very \\ufb01ne-grained categories, these were merged together to\\nform the category \\u201cpolitics\\u201d. Annotators also agreed unanimously on one article belonging\\nto the category \\u201cmedicine, public-health, and safety\\u201d. This article presented a report on a\\nnew diphtheria remedy and announced the arrival of fresh serum from Germany which was\\ntried on two cases in Philadelphia. They merged together articles that contained arts, biogra-\\nphies, book reviews and the like into one category called \\u201carts/human interest\\u201d. Creating\\na homogeneous category for these articles was not easy due to the wide variety of articles.\\nArticles pertaining to \\u201cdeath\\u201d and \\u201cmarriage announcements\\u201d were binned into separate\\ncategories. There was no agreement among annotators on eleven articles \\u2013 for example,\\nan article with a headline \\u201cPresident Cleveland goes hunting for squirrels\\u201d was labeled as\\nbelonging to the following categories: human interest/politics/sports/entertainment/social.\\nAll of these eleven articles had a much higher level of ambiguity and there was no agree-\\nment among annotators. Since we did not have categories pre-de\\ufb01ned for the annotation\\ntask and chose rather to let annotators come up with appropriate categories by themselves,\\ncomputing agreement on these articles was not straightforward.\\nIn essence, six15 sub-categories for the \\u201carticle/editorial\\u201d OCR category were found by\\nhuman annotators and are illustrated in Table III. It must be noted that in this application,\\nit is hard to obtain \\u201cground truth\\u201d or a \\u201cgold standard\\u201d which can be used for further\\nlabeling. Consequently, we are forced to rely on the subjective opinion of annotators who\\nsometimes disagree on labels. The six categories described above are also referred to as\\ninferred ground truth labels in later sections of this paper. There is considerable interest in\\nthe research community on whether this subjective labeling at low cost is indeed useful for\\nmachine learning algorithms [Raykar et al. 2009; Hsueh et al. 2009].\\nThe interview section of the annotation task indicated that small or singleton categories\\nlead to less agreement among humans; these outliers do not \\ufb01t into a larger category easily\\n15This is the value of K chosen for our experiments in later sections.\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:10\\nTable III. Sub-Categories formed by humans in the random sample of 25 arti-\\ncles.\\nID\\nCategory\\nArticle counts\\n1\\npolitics, elections, governmental appointments\\n7\\n2\\nmedicine, public health and safety\\n1\\n3\\ndeath\\n3\\n4\\narts, human interest, entertainment\\n2\\n5\\nmarriage\\n1\\n6\\nOther\\n11\\nand this raised confusion and di\\ufb03culty in categorization. Thus, learning from more examples\\nof similar kind was the norm.\\nMany of the annotators based the initial decision of the number of categories by reading\\nthe headlines of the articles and making notes; a feedback loop was almost always involved\\nwhere annotators re\\ufb01ned the initial estimates based on more careful and thorough reading of\\nthe articles. Finally, since it was not clearly indicated whether an article is allowed to belong\\nto multiple categories, this question was raised by several annotators. The time recorded\\nby annotators indicates that it took anywhere between 45 mins - 2 hrs to categorize all the\\n25 articles.\\n5. INCORPORATING PRIOR KNOWLEDGE INTO CLUSTERING ALGORITHMS\\nThe information provided by annotators, albeit subjective, provides important insights\\nabout how documents in the archive can be grouped together. Thus, studying whether\\nthis domain knowledge can be incorporated into automated document clustering algorithms\\nwould be bene\\ufb01cial.\\nClustering (or unsupervised learning) is ubiquitously used in machine learning problems\\nwhere labels are not easily available or are di\\ufb03cult to generate. Given a data set X, a par-\\ntitional clustering algorithm groups the data into K block sets and thus provides structure\\nto previously unstructured data. This is particularly useful in the context of the NYPL his-\\ntoric newspaper archive since the documents in that repository have no prior labels other\\nthan the broad categorization provided by the OCR software which can be inaccurate. A\\nclustering algorithm can thus be used to generate a taxonomy amongst similar articles.\\nA wide variety of clustering algorithms exist including the K-means algorithm, hierar-\\nchical clustering, the expectation maximization algorithm and their variants to name a few\\npopular algorithms. In this study we focus on the K-means algorithm which requires setting\\na large number of parameters such as the number of clusters, an appropriate distance func-\\ntion and a mechanism to initialize centroids. To estimate the performance of the algorithm,\\nit is common practice to compare the labels obtained from it with the \\u201cground truth\\u201d.\\nHowever, \\u201cground truth\\u201d may not be available and/or can be subjective.\\nSemi-supervised clustering algorithms have become very popular in data mining and\\nknowledge discovery [Zhu and Goldberg 2009]. These algorithms are typically used in sce-\\nnarios where only a small amount of data with prior knowledge (either as labels, constraints,\\netc.) is available in addition to a large proportion of unlabeled data. The design of a semi-\\nsupervised clustering algorithm depends on the mechanism by which the prior knowledge is\\nincorporated \\u2013 for example, (1) it may be available as pairwise constraints implying there\\nis pre-existing knowledge about whether two instances should belong to the same cluster\\n(Must-link) or di\\ufb00erent clusters (Cannot-link); (2) labels associated with each instance or\\n(3) inferring clustering constraints based on neighborhoods derived from labeled examples.\\nThe literature in semi-supervised learning however does not consider exhaustively scenarios\\nwhere the labels provided are subjective. This is the focus of our research.\\nIn this paper, we \\ufb01rst discuss the K-means algorithm with seeding as an example of\\nhow di\\ufb00erences in parameterization of the clustering algorithms can a\\ufb00ect the \\ufb01nal results\\nof the document clustering task. Next, we describe a di\\ufb00erent setting for the same doc-\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:11\\nument clustering task where domain knowledge comes as pairwise constraints. In either\\ncase the task of evaluation of the algorithms is hard, due to the subjective labels provided\\nby our annotators. In the following subsections, we describe one after the other the stan-\\ndard K-means algorithm, its semi-supervised counterpart obtained by careful seeding and\\nconstrained K-means clustering with Must-link and Cannot-link constraints.\\n5.1. The K-Means and Seeded K-means Algorithms\\nOne of the oldest and most commonly used clustering algorithms is the K-means algo-\\nrithm [Lloyd 1957], [MacQueen 1967]. Assume we are given an integer K and a set of N\\ndata points X \\u2282Rd; the goal is to partition X into K clusters, K < N. This can be\\nachieved by choosing K centroids C1, C2, \\u00b7 \\u00b7 \\u00b7 , CK so as to minimize the potential function\\n\\u03c6 = P\\nx\\u2208X minc\\u2208CDist[x \\u2212c], where Dist represents a distance function (such as squared\\neuclidean, L1 norm). The basic steps of the algorithm are as follows: Arbitrarily choose\\ninitial K centroids C1, C2, \\u00b7 \\u00b7 \\u00b7 , CK from X; for each i \\u2208{1, 2, \\u00b7 \\u00b7 \\u00b7 K} set the cluster Ci to be\\nthe set of all points in X that are closer to centroid Ci than they are to centroid Cj, \\u2200j \\u0338= i;\\nfor each i \\u2208{1, 2, \\u00b7 \\u00b7 \\u00b7 K} set the cluster centroid Ci =\\n1\\n|Ci|\\nP\\nx\\u2208Ci x; the last two steps are\\nrepeated until the process stabilizes and there are no new cluster assignments.\\n5.1.1. Choice of Parameters:. There is much debate on how to choose a suitable number of\\nclusters (K) appropriate for the data set. For our experiments we relied on human annotators\\nto come to a consensus regarding the choice of an appropriate K as described in Section 4.\\nThe other parameter that warrants some discussion is the choice of initial seeds; we have\\nused two di\\ufb00erent seeding mechanisms in our experiments: (a) randomly chosen seeds which\\ndo not use information about clusters that humans produced (b) a semi-supervised K-Means\\nalgorithm called Seeded K-Means [Basu et al. 2002]. This algorithm assumes that there\\nexists S \\u2286X, called the \\u201cseed set\\u201d on which supervision is provided by annotators; thus,\\nfor each xi \\u2208S the annotator indicates which cluster it seeds; there is at least one seed\\npoint xi per cluster. Once appropriate parameters have been set, the labels from K-Means\\nare compared with those inferred as \\u201cground-truth\\u201d in our pilot study. Note that all articles\\nwhere annotators did not agree on labels were designated to a category called \\u201cOther\\u201d.\\n5.1.2. Testing the validity of clusters:. In order to measure the quality of the clusters produced\\nby the K-means algorithm, we \\ufb01rst compare them to human annotated data marking each\\ninstance as one of the six categories illustrated in Table III. This procedure allows us to\\nquantitatively evaluate the system. The external cluster-validity measure used in this work\\nwas \\ufb01rst suggested by Dom [Dom 2001] and is equivalent to mutual information when\\ncluster labels and class labels are exactly the same. Let each data set D have n instances\\nO1, O2, \\u00b7 \\u00b7 \\u00b7 , On and we want to partition it into K clusters. Let K = {1, 2, \\u00b7 \\u00b7 \\u00b7 6} be the\\nset of cluster labels and C = {1, 2, \\u00b7 \\u00b7 \\u00b7 , 6} be the expert annotated class labels assigned to\\nthe objects in D. Consider a two-dimensional contingency table, H = h(c, k) where h(c, k)\\nrepresents the number of objects labeled class c are assigned to cluster k by the algorithm.\\nThen, if there is a perfect clustering H is a square matrix with only one non-zero element\\nper row / column. The marginals are de\\ufb01ned as h(c) = P\\nk h(c, k) and h(k) = P\\nc h(c, k).\\nSince in our experiments the number of clusters are known apriori, the cluster-validity\\nmeasure is essentially the empirical mutual information \\u02c6I(C, K) = \\u02c6H(C) \\u2212\\u02c6H(C|K), where\\n\\u02c6H(C) = \\u2212P|C|\\nc=1\\nh(c)\\nn log h(c)\\nn\\nand \\u02c6H(C|K) = \\u2212P|C|\\nc=1\\nP|K|\\nk=1\\nh(c,k)\\nn\\nlog h(c,k)\\nh(k) .\\nIn the second experiment, we questioned whether the choice of six categories was ap-\\npropriate and instead compared the performance of seeded K-means with the standard\\nK-means algorithm, setting the number of clusters as suggested by each annotator. As\\nbefore, mutual information was recorded over multiple trials and the averaged results are\\npresented in the following section.\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:12\\nTable IV. Mutual Information over 10 trials using Random vs Semi-\\nsupervised Seeding techniques. The inferred \\u201cground truth\\u201d sets\\nK = 6.\\nSeeding Algorithm\\nMean\\nStd over 10 trials\\nRandom Sampling\\n0.19\\n\\u00b10.10\\nSemi-supervised with Seeding\\n0.26\\n\\u00b10.07\\nTable V. Average Mutual Information and standard deviation over 5 trials using Standard and Seeded K-means algorithms. The\\nnumber of clusters is as proposed by each annotator. Ann 1 \\u00b7 \\u00b7 \\u00b7 6 refers to Annotator 1 through 6 who participated in the pilot study.\\nAnn1\\nAnn2\\nAnn3\\nAnn4\\nAnn5\\nAnn6\\nNo of clusters\\n8\\n14\\n13\\n9\\n10\\n9\\nSeeded K-Means Algorithm\\n0.235\\u00b10.056\\n0.131\\u00b10.048\\n0.097\\u00b10.058\\n0.214\\u00b10.107\\n0.130\\u00b10.064\\n0.210\\u00b10.089\\nK-means Algorithm\\n0.183\\u00b10.088\\n0.130\\u00b10.057\\n0.134\\u00b10.063\\n0.114\\u00b10.049\\n0.075\\u00b10.042\\n0.097\\u00b10.062\\n5.1.3. Empirical Evaluation:. The pilot study includes 25 articles. A bag-of-words represen-\\ntation and tf-idf weights are obtained for these articles. Each article has 3210 features\\nand one of possible six labels as indicated in section 4.1. The K-Means algorithm with\\nK = 6 is run over 10 trials using both the random seeding and semi-supervised seeding. For\\nsemi-supervised seeding, one representative article from each category provided by human\\nannotators is randomly selected for creating the seed; however care is taken to ensure that\\nall six categories are represented by at least one seed. In each trial, the labels obtained\\nafter clustering are tested against the inferred \\u201cground-truth\\u201d generated by annotators (as\\ndescribed in section 4.1) and mutual information is recorded. The average and standard\\ndeviation of mutual information obtained over all trials is presented in Table IV. Clearly,\\nusing Seeded K-Means with semi-supervision from annotators is more robust than the ran-\\ndom seeding mechanism since the mutual information is higher and has a lower standard\\ndeviation over all the trials.\\nIn the second experiment, the standard and seeded K-means algorithms were run with\\ndi\\ufb00erent values of K as re\\ufb02ected in the annotator choices. The results are shown in Ta-\\nble V. For all the annotators, except annotator 3, Seeded K-means performs better than\\nthe standard K-means algorithm. Closer investigation revealed that annotator 3 had indeed\\nprovided multiple labels for a given article; for example, s/he surmised that a particular\\narticle in the pilot study could belong to the category \\u201carts/human interest/politics\\u201d; since\\nthe focus of our work was not on studying the impact of multiple labels, we decided to\\nresolve ties by randomly selecting one label from all the suggestions; this process may have\\nintroduced bias and consequently a\\ufb00ected the performance of seeded K-means. A better\\nway to deal with multiple labelings would be to associate a probability of belonging to a\\nparticular class and then use this information to guide the seeding algorithm.\\nIn another experiment, we used the results from the pilot study to annotate unlabeled\\narticles. We applied the Seeded K-Means algorithm with seeds suggested by annotators, on\\nthe remaining articles of the November 2nd, 1894 issue of The Sun newspaper that were\\nnot included in the pilot study. At least one representative article from each category was\\nrandomly selected from clusters found by humans for creating the seed and care is taken to\\nensure that all six categories are represented. We ran the Seeded K-Means algorithm ten\\ntimes on the unlabeled articles. For each run, the number of clusters is \\ufb01xed at six, the cosine\\ndistance metric is used to compare similarity between instances, and the same technique\\n(randomly choose one of the representative documents of a category as the centroid) is used\\nfor generating seeds. The labels obtained from each run can be considered as produced by\\nan automated annotator. Since each automated annotator only provides labels between\\n1 and 6 we are able to use Krippendor\\ufb00\\u2019s alpha16 to measure inter-annotator agreement\\nbetween them. It is seen that there is a very low agreement (\\u03b1=0.316) when 200 resamplings\\n16We used the implementation available from http://ron.artstein.org/resources/\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:13\\nTable VI. Confusion Matrix generated by two runs of Seeded K-Means on blind test data formed by articles\\nof the newspaper not considered for the pilot study.\\nElections\\nMedicine\\nOther\\nDeath\\nHuman Interest\\nMarriage\\nTotal\\nElections\\n0\\n1\\n2\\n0\\n23\\n0\\n26\\nMedicine\\n6\\n7\\n0\\n3\\n1\\n4\\n21\\nOther\\n1\\n4\\n4\\n2\\n2\\n9\\n22\\nDeath\\n7\\n0\\n0\\n13\\n0\\n1\\n21\\nHuman Interest\\n2\\n16\\n0\\n5\\n1\\n1\\n25\\nMarriage\\n3\\n0\\n14\\n0\\n0\\n3\\n20\\nare used for calculating two-tailed 1% con\\ufb01dence intervals. To illustrate this point further, we\\nclosely examined the labels provided by two representative automated annotators as shown\\nin the confusion matrix illustrated in Table VI. For these two automated annotators, there is\\ncomplete agreement on sub-categories for 20.7% of the articles used for blind testing; 61.9%\\nof articles labeled \\u201cdeath\\u201d and 33% of articles labeled \\u201cMedicine\\u201d are correctly labeled.\\nWhile these results are encouraging, there seems to be confusion in distinguishing between\\nthe \\u201celection\\u201d and \\u201chuman interest\\u201d categories. It is worthwhile to note that humans also\\nfound it di\\ufb03cult to assign articles to the \\u201chuman interest\\u201d category and thus this task\\nappears to be signi\\ufb01cantly harder. An interesting direction for future work is to use other\\nmechanisms of \\ufb01nding representative seeds to be used with the Seeded K-Means algorithm.\\nOne such approach is to identify a centroid of the human clusters by calculating the cosine\\ndistance of each pair of documents in each human cluster, estimate the mean and then \\ufb01nd\\nthe document closest to the mean as the seed.\\n5.2. Constrained Clustering\\nThe simplest form of constrained clustering uses instance-level constraints, introduced by\\nWagsta\\ufb00et. al. [Wagsta\\ufb00et al. 2001]. There are primarily two types of constraints \\u2013 must-\\nlink and cannot-link. A must-link constraint, de\\ufb01ned as c=(a, b), requires a pair of points a\\nand b to appear in the same cluster. It is an equivalence relation, hence it is symmetrical,\\nre\\ufb02exive, and transitive. This implies, if c=(a, b) and c=(b, c), then c=(a, c). A cannot-link\\nconstraint, written as c\\u0338=(a, b), on the other hand, restricts both points from being part of\\nthe same cluster.\\nAn example of the above from our study is the following: assume an annotator has classi-\\n\\ufb01ed articles 1, 3, 6, 7 in one category, and articles 2, 4, 5 in another category. The constrained\\nclustering algorithm would create a series of must-links which might include c=(1, 3), which\\nimplies articles 1 and 3 are required to be part of the same group. It would also generate\\na set of cannot-links that may include c\\u0338=(3, 4), meaning articles 3 and 4 cannot be part of\\nthe same group.\\nConstrained clustering can be categorized as (1) constraint-based and (2) distance-based.\\nIn constraint-based clustering, the algorithm utilizes constraints only after it has already\\nclassi\\ufb01ed all the points into initial clusters such as by using the K-means algorithm. It then\\nveri\\ufb01es if there are any constraint violations and reallocates points to resolve violations.\\nThere can be a strict or soft enforcement of constraints. In strict-enforcement, no instance\\ncan violate any constraint. It either outputs a solution, or fails. A soft-enforcement allows\\nviolations, but adds penalties17 when required. The clustering algorithm seeks the best\\nfeasible assignments and always produces a solution. Distance-based constrained clustering,\\non the other hand, treats constraints as weights on the distance functions. A must-link\\n\\u201cshrinks\\u201d the distance between two instances, while a cannot-link \\u201cwidens\\u201d the distance\\nbetween them.\\n17Penalties can be distance metrics or conditional probabilities.\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:14\\nThe empirical results presented in this paper use a constraint-based algorithm (Pairwise\\nConstrained Clustering K-Means (PCKMeans) [Basu et al. 2004]) with probabilistic penal-\\nties. The algorithm is presented in Figure 1. As an initialization step, it receives a list of\\nconstraints and generates the transitive closure of the must-links. It is important to note\\nthat this step makes it susceptible to noise. Following this, the standard K-means algorithm\\nis run. The di\\ufb00erence between the standard K-means and PCKmeans occurs in terms of\\nwhen and how they exploit the constraints.\\nALGORITHM 1: PCKMeans Algorithm\\ninput : Set of data points \\u03c7 = {xi}n\\nx=1, set of must-link constraints M = (xi, xj), set\\nof cannot-link constraints C = (xi, xj), number of clusters k, weight of\\nconstraints w.\\noutput: Disjoint k partitioning {\\u03c7h}k\\nh=1 of \\u03c7 such that objective function \\u03c4pckm is\\n(locally) minimized.\\nmethod\\n1. Initialize clusters.\\nCreate the \\u03bb neighborhoods {Np}\\u03bb\\np=1 from M and C.\\nSort the indices p in decreasing size of Np.\\nif \\u03bb \\u2265k then\\nInitialize {\\u00b5(0)\\nh }k\\nh=1 with centroids of {Np}k\\np=1.\\nelse\\nInitialize {\\u00b5(0)\\nh }k\\nh=1 with centroids of {Np}\\u03bb\\np=1.\\nif \\u2203point x cannot-linked to all neighborhoods {Np}\\u03bb\\np=1. then\\ninitialize \\u00b5(0)\\n\\u03bb+1 with x.\\nInitialize remaining clusters at random.\\n2. Repeat until convergence.\\nassign cluster: Assign each data point x to the cluster h\\u2217(i.e. set \\u03c7(t+1)\\nh\\u2217\\n), for\\nh\\u2217= argminh( 1\\n2||x \\u2212\\u00b5(t)\\nh ||2 + w P\\n(x,xj)\\u2208M 1[h \\u0338= lj] + w P\\n(x,xj)\\u2208C 1[h = lj]).\\nestimate means: {\\u00b5(t+1)\\nh\\n}k\\nh=1 \\u2190{\\n1\\n|\\u03c7(t+1)\\nh\\n|\\nP\\nx\\u2208\\u03c7(t+1)\\nh\\nx}k\\nh=1.\\nt \\u2190(t + 1).\\n5.2.1. E\\ufb00ectiveness of Constraints:. How can we tell how useful constraints really are to\\nthe clustering process? This is important to consider because it may be possible that\\nconstraints introduced hurt instead of improving performance [Davidson et al. 2006b].\\nThe e\\ufb00ectiveness of a set of constraints on a clustering problem can be measured by\\ninformativeness and coherence.\\nInformativeness is a measure of how much additional information about the domain the\\nconstraints provide to the clustering algorithm that it was not able to determine on its own.\\nIt is de\\ufb01ned as:\\nIA(C) =\\n1\\n|C|[P\\nc\\u2208C unsat(c, PA)]\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:15\\nFig. 5.\\nFirst illustration of projected overlap between a must-link and a cannot-link. The must-link is the\\ngreen line, the cannot-link is the red line, and the orange line is the projection of the cannot-link onto the\\nmust-link. In this example, there is no overlap between the two links.\\nFig. 6.\\nSecond illustration of projected overlap between a must-link and a cannot-link. The must-link is\\nthe green line, the cannot-link is the red line, and the orange line is the projection of the cannot-link onto\\nthe must-link. In this example, there is some overlap between the two links.\\nwhere C is the set of constraints, A is an unconstrained clustering algorithm, PA is the\\nunconstrained clustering results of running A, and unsat(c, PA) is 1 if PA does not satisfy\\nc and 0 otherwise.\\nCoherence tries to determine how \\u201ccontradictory\\u201d the information is. It calibrates the\\nlevel of agreement between constraints in set C, using a distance metric D. It is measured\\nby the projected overlap between two constraints i.e. how much overlap there is when one\\nconstraint is projected along the direction of the other.\\nLet \\u20d7a be a cannot-link and \\u20d7b a must-link, then projection is estimated as follows:\\n\\u20d7p = proj \\u20d7b \\u20d7a = (|\\u20d7a| cos\\u03b8)\\n\\u20d7b\\n|\\u20d7b|\\nwhere \\u03b8 is the angle between the two vectors. To calculate how much of the projection of \\u20d7a\\noverlaps with \\u20d7b, we compute the distance corresponding to the three cases:\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:16\\nFig. 7.\\nThird illustration of projected overlap between a must-link and a cannot-link. The must-link is the\\ngreen line, the cannot-link is the red line, and the orange line is the projection of the cannot-link onto the\\nmust-link. In this example, there is complete overlap between the two links.\\noverlapb(a) =\\n(\\n0\\nif db2,b1 \\u2264db2,p2, db2,b1 \\u2264db2,p1\\ndb1,p2\\nif db2,p2 < db2,b1, db2,p1 \\u2265db2,b1\\ndp1,p2\\nif db2,p2 < db2,b1, db2,p1 < db2,b1\\nwhere b1, b2, p1, p2 are the beginning and end co-ordinates of vector \\u20d7b and the projection\\nof \\u20d7a given by p. Figures 5, 6, and 7 provide examples of the projection scheme. There are\\ntwo ways by which constraints can have zero projected overlap: (1) they are orthogonal to\\neach other, so that neither link interferes with the other or (2) they are both the same type\\nof links (both are must-links or both are cannot-links), so any overlap that exists does not\\nmatter. Coherence (COH) of a constraint set C using a distance metric D is then be de\\ufb01ned\\nas the fraction of ML-CL18 constraint pairs in the constraint set C, that have zero projected\\noverlap i.e.\\nCOHD(C) =\\nP\\nm\\u2208CML , c\\u2208CCL \\u03b4(overlapc m = 0 and overlapm c = 0)\\n|CML||CCL|\\n(1)\\nwhere CML and CCL represent the set of must-link and cannot-link constraints respectively\\nin C; |CML| and |CCL| represents the cardinality of each set. A clustering algorithm, when\\ngiven a constraint set with low coherence, gets confused on how to properly label points.\\nDavidson et. al. [Davidson et al. 2006b] indicates that constraint sets with high informa-\\ntiveness and high coherence improve performance, while sets with low informativeness and\\nlow coherence hurt performance. Low informativeness means the constraint set does not\\nprovide much helpful information. Low coherence means the information provided by the\\nconstraints set is contradictory and confusing. This is further illustrated in Figures 8, 9, 10\\nand 11.\\n5.2.2. Modeling Annotators and Assessing their Quality. Online digital archives such as the\\nNYPL historic newspaper archive are unlabeled and getting a dataset labeled by annotators\\ncan be time consuming, if not impossible. With the advent of crowdsourcing, getting cheap\\nlabels is relatively easy. How good are these labels? People with di\\ufb00erent levels of expertise\\n\\u2013 novices, scholars, biased and malicious annotators may provide inexpensive labels but\\n18ML: Must-Link, CL: Cannot-Link\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:17\\nFig. 8.\\nAn example of a constraint set with high informativeness. The points represent articles. The black\\ndots represent one cluster, and the blue dots represent a second cluster. This is typical of how a simple\\nKMeans algorithm would classify points based on distance. The red edges are cannot-link constraints, and\\nthe green edges are must-link constraints. This constraint set indicates that many of the points close together\\ndo not belong in the same set, while the two points that the must-link connects belong in the same set. A\\nsimple KMeans algorithm would not partition the dataset in this way, meaning the constraints provide very\\nvaluable information.\\nFig. 9.\\nAn example of a constraint set with low informativeness. The points represent articles. The black\\ndots represent one cluster, and the blue dots represent a second cluster. This is typical of how a simple\\nKMeans algorithm would classify points based on distance. The red edges are cannot-link constraints, and\\nthe green edges are must-link constraints. This constraint set does not provide any additional information\\nthat con\\ufb02icts with how the simple KMeans algorithm would class\\ufb01y these points. All the links between a\\npoint in the \\ufb01rst cluster and a point in the second cluster are cannot-links, while all the links connecting\\ntwo points within the same cluster are must-links.\\ncha\\ufb03ng meaningful information from it could be challenging. Accessing the \\u201cquality\\u201d of\\nlabels is therefore of interest.\\nIn the PCKMeans algorithm, the constraints can be used to encapsulate the prior knowl-\\nedge an annotator has. The informativeness of constraints then provides a qualitative mea-\\nsure of how good the suggested constraints are \\u2013 in other words, informativeness measures\\nhow much additional information about the domain these constraints have provided by\\ncomparing the same clustering algorithm with and without19 constraints. An annotator\\n19It should be noted that for the comparison to be meaningful, the same set of initial parameters for\\nthe unconstrained clustering algorithm should be used; for example if using the KMeans algorithm, the\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:18\\nFig. 10.\\nAn example of a constraint set with high coherence. The points represent articles. The red edges\\nare cannot-link constraints, and the green edges are must-link constraints. The points in the region near the\\ntwo points connected by the constraint should have the same type of link. In this \\ufb01gure, all the must-links\\nconnect to points very near each other and do not show any contradictions.\\nFig. 11.\\nAn example of a constraint set with low coherence. The points represent articles. The red edges\\nare cannot-link constraints, and the green edges are must-link constraints. The links connecting the points\\nin the upper right-hand corner with other points in the \\ufb01gure are a mixture of must-links and cannot-links\\nthat seem to contradict each other. This constraint set can confuse the PCKMeans algorithm.\\nwith more \\u201cinformative\\u201d constraints is clearly preferred over one whose constraints do not\\nprovide additional information about the domain.\\nFor our experiments, we modeled each annotator by the PCKMeans clustering algorithm;\\ndi\\ufb00erent sets of constraints were generated from the labels provided by the annotator; the\\nperformance of the KMeans algorithm with and without constraints was measured using\\ninformativeness.\\n5.2.3. Empirical Evaluation:. For our experiments, we used the WekaUT extension for Weka\\nfrom http://www.cs.utexas.edu/users/ml/risc/code/.\\nExperiment 1: Replicating annotator performance by PCKMeans\\nIn the \\ufb01rst experiment, we wanted to determine if PCKMeans could accurately replicate\\nthe performance of annotators and classify the twenty-\\ufb01ve articles the way each annotator\\nunconstrained algorithm should be trained each time with the same initial seeds, distance metric and the\\nnumber of clusters.\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:19\\nFig. 12.\\nAverage Mutual Information vs. Constraints. The mutual information compares each annotator\\u2019s\\nPCKMeans results with his/her own labels across increasing number of constraints. As expected, as the\\nnumber of constraints increased, the PCKMeans results for each annotator became more in line with his/her\\nlabelings.\\ndid. We ran \\ufb01ve trials of the clustering algorithm on the data for each annotator for\\ndi\\ufb00erent number of constraints, varying them from 10 to 300 in increments of 10. The\\nconstraints were generated as follows from the labels provided by the annotators: Randomly\\nsample two instances; if they were assigned the same class label by the annotator there is a\\nMUST link between them; if they are assigned di\\ufb00erent class labels, they have a CANNOT\\nlink between them. Note that C25\\n2\\n= 300 was the maximum number of constraints that\\ncould be generated from the pilot study data. The mutual information (as described in\\nSection 5.1.2) between each annotator\\u2019s PCKMeans clustering results vs his/her own labels\\nis estimated. The average mutual information over all the \\ufb01ve trials is shown in Figure 12.\\nIt appears that that PCKMeans can accurately replicate the annotators only when supplied\\nwith a su\\ufb03ciently large number of constraints. The mutual information results reached one\\nonly after 200 constraints were supplied to the algorithm. In the second experiment, the\\nmutual information between each annotator\\u2019s PCKMeans results and those inferred from\\nthe pilot study were computed. The results are illustrated in Figure 13. It was not expected\\nthat each annotator\\u2019s labels would converge20 to 1.0 and indeed some annotators such as\\nAnnotator 2, 3 and 5 have low average mutual information. However, the informativeness\\nimproved as expected. Another interesting thing was that the annotators who supplied the\\nlargest numbers of categories were the ones who had the lowest mutual information score\\nsince they were providing far more labels than what was inferred from ground truth.\\nExperiment 2: Comparing Annotators\\nIn the next experiment, we had six PCKMeans clusterers, one trained for each annotator;\\nwe compared the output of each PCKMeans clusterer with the results of running standard\\nKMeans (Figure 14) for inferred ground truth labels. This enabled us to measure the\\n20Convergence to one meant that the annotator\\u2019s label exactly matched the inferred ground truth label.\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:20\\nFig. 13.\\nAverage Mutual Information vs. Constraints. Here, the mutual information compares each annota-\\ntor\\u2019s PCKMeans results with the inferred ground truth labels. Here, we didn\\u2019t expect the values to converge\\nto one as the number of constraints increased, but we did expect the informativeness to improve. The graph\\nindicate that the level of informativeness depended on the di\\ufb00erence between the number of clusters each\\nannotator assigned and the number in the inferred ground truth.\\nFig. 14.\\nAverage Informativeness vs. Constraints. Here, the informativeness is measured by taking each\\nannotator\\u2019s PCKMeans results over the results of the standard KMeans for the inferred ground truth data.\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:21\\nFig. 15.\\nAverage Informativeness vs. Constraints. Here, the informativeness is measured by taking each\\nannotator\\u2019s PCKMeans results over the results of PCKMeans for the inferred ground truth data.\\ninformativeness of constraints for each PCKMeans clusterer and thereby provide a way\\nto compare annotators as described in Section 5.2.2. As before, the number of constraints\\nwas varied from 10 to 300 in increments of 10. Each colored line in Figure 14 refers to the\\naverage performance of an annotator over \\ufb01ve di\\ufb00erent trials. It is interesting to see that\\nall of the annotators seem to have a high informativeness when the number of constraints is\\nbelow 60. When the number increases beyond 60, the informativeness becomes more or less\\nconstant \\u2013 in other words, increase in number of constraints arbitrarily does not provide\\nmore prior knowledge with respect to the unconstrained problem. It is also interesting to\\nnote that annotators 2 and 3 maintained a high informativeness compared to all the other\\nannotators \\u2013 a closer look at their annotations revealed that they had provided the largest\\nnumber of categories in the pilot study; not only were they thinking hierarchically when\\nproviding the categories, they also seemed much more detail-oriented in their approach to\\nproviding annotations.\\nExperiment 3: Studying the impact of providing (and inferring) di\\ufb00erent num-\\nber of categories\\nAnother interesting experiment that we conducted was to test the output of each of the\\nPCKMeans clusterer with the results of running PCKMeans (Figure 15) for inferred ground\\ntruth labels. This allowed us to study the impact of an annotator suggesting di\\ufb00erent\\nnumber of classes than those inferred from the pilot study. For example, if a PCKMeans\\nclusterer with 10 constraints was generated from 13 class labels suggested by an annotator\\nwas it necessarily better (in terms of informativeness) than a PCKMeans algorithm with\\n10 constraints generated from 6 class labels as inferred from the pilot study? In this case,\\nannotator 3 still consistently had a higher informativeness than others, but surprisingly\\nannotator 2 who provided the maximum number of classes in the pilot study did not have\\nan overall high informativeness. This could be attributed to the fact that the constraints\\nare generated randomly \\u2013 it would be much more useful to study the case where the\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:22\\npairwise constraints are provided manually by the annotators.\\nExperiment 4: Studying Coherence\\nFigures 16 and\\n17 compare the informativeness against the coherence of the clusters\\ngenerated in the two cases described above. Each dot in the \\ufb01gure represents the informa-\\ntiveness vs. coherence value for a given number of constraints21.\\nFig. 16.\\nAverage Informativeness vs. Coherence. Here, the informativeness is measured by taking each\\nannotator\\u2019s PCKMeans results over the results of the standard KMeans for the inferred ground truth data.\\nIf we divide the graphs into four regions, each region indicates a di\\ufb00erent set of charac-\\nteristics of the constraint set. The upper right region, the area with high informativeness\\nand high coherence, indicates that the annotator\\u2019s labelings are extremely helpful to the\\nclustering algorithm. This is the ideal region. The algorithm will very likely produce great\\nresults. The lower left region of the graph indicates low informativeness and low coherence,\\nmeaning the classi\\ufb01cations the annotator provides not only is not very helpful, but contra-\\ndictory and confusing to the clusterer. This is the worst region, and any results produced by\\nPCKMeans algorithm will mostly be poor. The other two regions are of mixed usefulness.\\nThe upper left region means the annotator provides a lot of but confusing information,\\nwhile the lower right region means the annotator provides clear but very little information.\\nAs Figure 17 shows, Annotator 3\\u2019s datapoints fall closest to the ideal region, while Annotator\\n6\\u2019s points fall in the bad region. This indicates that Annotator 3 provides a better and clearer\\ninformation to the clusterer.\\n21The reader is reminded that the constraints are varied from 10 to 300 in increments of 10 each.\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:23\\nFig. 17.\\nAverage Informativeness vs. Coherence. Here, the informativeness is measured by taking each\\nannotator\\u2019s PCKMeans results over the results of PCKMeans for the inferred ground truth data.\\nIn addition, it is helpful to look at how spread out the datapoints are. A correct clustering\\nin one region means the characteristics of the annotator\\u2019s labels are consistent across a\\nvarying number of constraints. A wide and loose scattering means the labelings are not\\nconsistent. Comparing Figures 16 and 17, one sees that the datapoints for all the annotators\\nare clustered much more tightly when compared to the PCKMeans algorithm than when\\ncompared to simple KMeans. This indicates that the PCKMeans ground truth is more\\nconsistent with what the annotators have in mind than the simple KMeans.\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:24\\nFig. 18.\\nArchitecture and Implementation Details of the BODHI Crowd Sourcing System\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:25\\n6. THE BODHI SYSTEM: LARGE SCALE OCR CORRECTION AND TAG COLLECTION AT\\nTHE NYPL\\nTo enable collaborative tagging, we are designing a system (named \\u201cBODHI\\u201d) that can\\nbe integrated with the current architecture used in National Digital Newspaper Program\\n(NDNP) and allow patrons to correct garbled OCR text, enter keywords for tagging articles,\\nprovide useful information about segmentation of the article (for example \\u2013 is the article\\ncontinued onto another page or not), links to other related articles, etc. These user-provided\\nmeta-data will augment the scanned text and image data obtained from the OCR scanning\\nprocess. While the NDNP Content Management System has many modules to manage the\\nnewspaper digitization work\\ufb02ow from scanning micro\\ufb01lm to public delivery, the BODHI\\nsystem focuses only on improving search and retrieval.\\nFigure 18 presents the architecture and digital technology that will be used in devel-\\nopment of the \\u201cBODHI\\u201d system. The \\ufb01le server stores the digitized newspaper images as\\njpeg, ti\\ufb00, pdf or xml \\ufb01les (these are obtained from the NYPL after the OCR scanning pro-\\ncess) \\u2013 this is subjected to Extract, Transform and Load (ETL) operations using software\\ndeveloped in Java and XML and stored into a PostgreSQL database. In addition to the\\nstorage of OCR data from the newspaper articles, the database is also capable of storing\\nuser registration information, version and change tracking as required (assuming the patrons\\nof the archive may edit an article multiple times and add di\\ufb00erent annotations) and is in-\\ndexed using Apache Lucene (http://lucene.apache.org/java/docs/index.html), which\\nhas an open-source Java-based indexing and search implementation as well as spellcheck-\\ning, hit highlighting and advanced analysis/tokenization capabilities. The components of\\nthe BODHI application include a user manager, article display, OCR corrector, tag (or an-\\nnotation) collector and search and retrieval manager. The prototype has been developed\\nusing Ruby-on-Rails and will be deployed using the Apache Phusion Passenger framework\\n(http://www.modrails.com/). Once deployed on the library infrastructure, the web inter-\\nface can be accessed by social media, mobile apps and the internet.\\n7. CONCLUSION AND FUTURE WORK\\nThe New York Public Library has an archive of over 200,000 historical newspapers published\\nbetween 1890 and 1920 which have been subjected to OCR and are currently stored in an\\nonline database making them accessible to patrons. Unfortunately search facilities on this\\ndatabase are rudimentary; newspapers are scanned on a page-by-page basis and article level\\nsegmentation is almost non-existent; the OCR scanning process introduces a lot of garbled\\ntext. In a bid to make these archives more accessible to the general public, text mining\\nalgorithms are being considered for categorization of articles. The OCR software provides\\na rough categorization, but a large chunk of the articles are labeled \\u201carticle/editorial\\u201d\\nwithout division into further meaningful categories. Thus, articles dealing with medicine\\nand crime are deemed to belong to the same category; this makes search and retrieval of\\narticles di\\ufb03cult. We designed a pilot study to observe if humans were able to \\ufb01nd coherent\\ncategories in a small subset of articles. Our results indicate that the presence of small and\\nnoisy clusters in the data made it di\\ufb03cult to \\ufb01nd an agreement in the number of clusters.\\nWe also evaluated the quality of the annotation provided by humans by measuring how\\nmuch additional information they could provide to help the clustering algorithm. More\\nspeci\\ufb01cally, the informativeness of constraints in a constrained clustering algorithm was\\nused as a metric to evaluate how well they labeled articles. Our results from the pilot study\\nare very encouraging and we are developing a large scale system (christened \\u201cBODHI\\u201d)\\nin collaboration with the New York Public Library to collect tags and incorporate the\\n\\u201cwisdom of crowds\\u201d into machine learning algorithms. Future work involves development of\\nmore sophisticated non-parametric and bayesian text mining algorithms and experiments\\non a large scale using the deployed \\u201cBODHI\\u201d system.\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:26\\nACKNOWLEDGMENTS\\nThis work is supported by funding from the National Endowment for Humanities, Grant No: NEH HD-\\n51153-10. The authors would like to thank Dr. David Waltz for stimulating discussions during the initial\\nphases of the project, Dr. Dragomir Radev for his generous and insightful comments on drafts of the paper,\\nSam Lee and Hatim Diab for help with infrastructure and system development.\\nREFERENCES\\nAhn, L. V. 2008. Designing Games with a Purpose. In Communications of the ACM. Vol. 51.\\nArthur, D. and Vassilvitskii, S. 2007. k-means++: The advantages of careful seeding. In Proceedings of\\nthe 18th Annual ACM-SIAM Symposium on Discrete Algorithms. 1027 \\u2013 1035.\\nBansal, N., Blum, A., and Chawla, S. 2004. Correlation Clustering. In Machine Learning. Vol. 56.\\nBasu, S. 2005. Semi-supervised clustering: Probabilistic models, algorithms and experiments. Ph.D. thesis.\\nBasu, S., Banerjee, A., and Mooney, R. J. 2002. Semi-supervised clustering by seeding. In ICML \\u201902:\\nProceedings of the Nineteenth International Conference on Machine Learning. San Francisco, CA, USA,\\n27\\u201334.\\nBasu, S., Banerjee, A., and Mooney, R. J. 2004. Active semi-supervision for pairwise constrained clus-\\ntering. In Proceedings of the SIAM International Conference on Data Mining SDM-2004. 333 \\u2013 344.\\nBilenko, M. and Mooney, R. J. 2003. Adaptive duplicate detection using learnable string similarity\\nmeasures. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery\\nand data mining. KDD \\u201903. 39\\u201348.\\nBlum, A., Lafferty, J., Rwebangira, M. R., and Reddy, R. 2004. Semi-supervised learning using ran-\\ndomized mincuts. In Proceedings of the twenty-\\ufb01rst international conference on Machine learning.\\nICML \\u201904.\\nBradley, P. S. and Fayyad, U. M. 1998. Re\\ufb01ning initial points for K-means clustering. In Proceedings of\\nthe 15th International Conference on Machine Learning (ICML). 91 \\u2013 99.\\nBradley, P. S., Mangasarian, O. L., and Street, W. N. 1997. Clustering via concave minimization. In\\nAdvances in Neural Information Processing Systems -9. MIT Press, 368\\u2013374.\\nBurl, M. C., Fayyad, U. M., Perona, P., and Smyth, P. 1994. Automated analysis of radar imagery of\\nvenus: Handling lack of ground truth. In ICIP (3). 236\\u2013240.\\nCharikar, M., Guruswami, V., and Wirth, A. 2003. Clustering with qualitative information. In Proceed-\\nings of the 44th Annual IEEE Symposium on Foundations of Computer Science. FOCS \\u201903. 524\\u2013533.\\nCohn, D., Caruana, R., and McCallum, A. 2003. Semi-supervised clustering with user feedback. Tech.\\nRep. TR2003-1892, Cornell University.\\nCrane, G. and Jones, A. 2006. The challenge of virginia banks: an evaluation of named entity analysis\\nin a 19th-century newspaper collection. In Proceedings of the 6th ACM/IEEE-CS joint conference on\\nDigital libraries. JCDL \\u201906. 31\\u201340.\\nCutting, D. R., Karger, D. R., Pedersen, J. O., and Tukey, J. W. 1992. Scatter/gather: a cluster-based\\napproach to browsing large document collections. In Proceedings of the 15th annual international ACM\\nSIGIR conference on Research and development in information retrieval. SIGIR \\u201992. 318\\u2013329.\\nDavidson, I., Wagstaff, K., and Basu, S. 2006a. Measuring constraint-set utility for partitional clustering\\nalgorithms. In PKDD. 115\\u2013126.\\nDavidson, I., Wagstaff, K. L., and Basu, S. 2006b. Measuring constraint-set utility for partitional clus-\\ntering algorithms. In Proceedings of the Tenth European Conference on Principles and Practice of\\nKnowledge Discovery in Databases. 115 \\u2013 126.\\nDekel, O. and Shamir, O. 2009a. Good learners for evil teachers. In Proceedings of the 26th Annual\\nInternational Conference on Machine Learning. ICML \\u201909. 233\\u2013240.\\nDekel, O. and Shamir, O. 2009b. Vox populi: Collecting high-quality labels from a crowd. In In Proceedings\\nof the 22nd Annual Conference on Learning Theory.\\nDemiriz, A., Bennett, K., and Embrechts, M. J. 1999. Semi-supervised clustering using genetic algo-\\nrithms. In In Arti\\ufb01cial Neural Networks in Engineering (ANNIE-99. ASME Press, 809\\u2013814.\\nDom, B. E. October - 2001. An information-theoretic external cluster-validity measure. IBM Research\\nTechnical Report RJ - 10219.\\nDutta, H., Passonneau, R. J., Lee, A., Radeva, A., Xie, B., and Waltz, D. L. 2011. Learning parameters\\nof the k-means algorithm from subjective human annotation. In Twenty-Fourth International FLAIRS\\nConference, AAAI Publications.\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:27\\nEsposito, F., Malerba, D., Semeraro, G., Antifora, C. D., and Gennaro, G. d. 1997. Information\\ncapture and semantic indexing of digital libraries through machine learning techniques. In Proceedings\\nof the 4th International Conference on Document Analysis and Recognition. ICDAR \\u201997. 722\\u2013727.\\nFaymonville, P., Wang, K., Miller, J., and Belongie, S. 2009. Captcha-based image labeling on the\\nsoylent grid. In Proceedings of the ACM SIGKDD Workshop on Human Computation. HCOMP \\u201909.\\n46\\u201349.\\nForgy, E. 1965. Cluster analysis of multivariate data: E\\ufb03ciency versus interpretability of classi\\ufb01cations.\\nBiometrics 21.\\nGe, R., Ester, M., Jin, W., and Davidson, I. 2007. Constraint-driven clustering. In KDD. 320\\u2013329.\\nHsueh, P.-Y., Melville, P., and Sindhwani, V. 2009. Data quality from crowdsourcing: a study of an-\\nnotation selection criteria. In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for\\nNatural Language Processing. HLT \\u201909. Morristown, NJ, USA, 27\\u201335.\\nKaufman, L. and Rousseeuw, P. J. 1990. Finding Groups in Data: An Introduction to Cluster Analysis.\\nWiley, Canada.\\nKlein, D., Kamvar, S. D., and Manning, C. D. 2002. From instance-level constraints to space-level\\nconstraints: Making the most of prior knowledge in data clustering. In Proceedings of the Nineteenth\\nInternational Conference on Machine Learning. ICML \\u201902. San Francisco, CA, USA, 307\\u2013314.\\nKulis, B., Basu, S., Dhillon, I., and Mooney, R. 2009. Semi-supervised graph clustering: a kernel ap-\\nproach. In Machine Learning. Vol. 71. 1\\u201322.\\nLange, T., Law, M. H. C., Jain, A. K., and Buhmann, J. M. 2005. Learning with constrained and\\nunlabelled data. In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision\\nand Pattern Recognition (CVPR\\u201905) - Volume 1 - Volume 01. CVPR \\u201905. 731\\u2013738.\\nLee, A., Dutta, H., Passonneau, R. J., Lee, A., Waltz, D. L., and Taranto, B. 2010. Topic identi\\ufb01cation\\nfrom historic newspaper articles of the new york public library: A case study. In The Fifth Annual\\nMachine Learning Symposium, New York Academy of Sciences.\\nLloyd, S. 1957. Least squares quantization in pcm. In Bell Telephone Laboratories Paper.\\nLugosi, G. 1992. Learning with an unreliable teacher. Pattern Recogn. 25, 79\\u201387.\\nMacQueen, J. 1967. Some methods for classi\\ufb01cation and analysis of multivariate observations. In Proc. of\\nthe 5th Berkeley Symposium. 281 \\u2013 297.\\nRaykar, V. C., Yu, S., Zhao, L. H., Jerebko, A., Florin, C., Valadez, G. H., Bogoni, L., and Moy, L.\\n2009. Supervised learning from multiple experts: Whom to trust when everyone lies a bit. In Proceedings\\nof the International Conference on Machine Learning (ICML).\\nSankar K., P., Jawahar, C. V., and Manmatha, R. 2010. Nearest neighbor based collection ocr. In\\nProceedings of the 9th IAPR International Workshop on Document Analysis Systems. DAS \\u201910. 207\\u2013\\n214.\\nSheng, V. S., Provost, F., and Ipeirotis, P. G. 2008. Get another label? improving data quality and data\\nmining using multiple, noisy labelers. In Proceeding of the 14th ACM SIGKDD international conference\\non Knowledge discovery and data mining. KDD \\u201908. 614\\u2013622.\\nSinkkonen, J. and Kaski, S. 2002. Clustering based on conditional distributions in an auxiliary space.\\nNeural Comput. 14, 217\\u2013239.\\nSmith, D. A. 2002a. Detecting and browsing events in unstructured text. In Proceedings of the 25th annual\\ninternational ACM SIGIR conference on Research and development in information retrieval. SIGIR\\n\\u201902. 73\\u201380.\\nSmith, D. A. 2002b. Detecting events with date and place information in unstructured text. In Proceedings\\nof the 2nd ACM/IEEE-CS joint conference on Digital libraries. JCDL \\u201902. 191\\u2013196.\\nSmith, D. A. and Crane, G. 2001. Disambiguating geographic names in a historical digital library. In\\nProceedings of the 5th European Conference on Research and Advanced Technology for Digital Libraries.\\nECDL \\u201901. 127\\u2013136.\\nSmyth, P., Fayyad, U. M., Burl, M. C., and Perona, P. 1996. Modeling subjective uncertainty in image\\nannotation. In Advances in Knowledge Discovery and Data Mining. 517\\u2013539.\\nSmyth, P., Fayyad, U. M., Burl, M. C., Perona, P., and Baldi, P. 1994. Inferring ground truth from\\nsubjective labelling of venus images. In NIPS. 1085\\u20131092.\\nSnow, R., O\\u2019Connor, B., Jurafsky, D., and Ng, A. Y. 2008. Cheap and fast\\u2014but is it good?: evaluating\\nnon-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical\\nMethods in Natural Language Processing. EMNLP \\u201908. 254\\u2013263.\\nTurnbull, D., Liu, R., Barrington, L., and Lanckriet, G. 2007. A game-based approach for collecting\\nsemantic annotations of music. In Proceedings of the 8th International Conference on Music Informa-\\ntion Retrieval (ISMIR 2007).\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nA:28\\nvon Ahn, L. and Dabbish, L. 2004. Labeling images with a computer game. In Proceedings of the SIGCHI\\nconference on Human factors in computing systems. CHI \\u201904. 319\\u2013326.\\nvon Ahn, L., Maurer, B., Mcmillen, C., Abraham, D., and Blum, M. 2008. reCAPTCHA: Human-Based\\nCharacter Recognition via Web Security Measures. Science 321, 5895.\\nWagstaff, K., Cardie, C., Rogers, S., and Schroedl, S. 2001. Constrained k-means clustering with\\nbackground knowledge. In Proceedings of the International Conference on Machine Learning (ICML).\\nMorgan Kaufmann, 577\\u2013584.\\nXing, E. P., Ng, A. Y., Jordan, M. I., and Russell, S. 2002. Distance metric learning, with application\\nto clustering with side-information. In Advances in Neural Information Processing Systems 15. MIT\\nPress, 505\\u2013512.\\nZhu, X. and Goldberg, A. B. 2009. Introduction to Semi-Supervised Learning. Synthesis Lectures on\\nArti\\ufb01cial Intelligence and Machine Learning. Morgan & Claypool Publishers.\\nZhuxj, X. Z., Ghahramani, Z., and Lafferty, J. 2003. Semi-supervised learning using gaussian \\ufb01elds and\\nharmonic functions. In In ICML. 912\\u2013919.\\nReceived April 2012; revised August 2012; accepted October 2012\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nOnline Appendix to:\\nLeveraging Subjective Human Annotation for Clustering Historic\\nNewspaper Articles\\nHaimonti Dutta, The Center for Computational Learning Systems\\nWilliam Chan, Department of Computer Science\\nDeepak Shankargouda, Department of Computer Science\\nManoj Pooleery, The Center for Computational Learning Systems\\nAxinia Radeva, The Center for Computational Learning Systems\\nKyle Rego, Department of Computer Science\\nBoyi Xie, The Center for Computational Learning Systems\\nRebecca J. Passonneau, The Center for Computational Learning Systems\\nAustin Lee, The Center for Computational Learning Systems\\nBarbara Taranto, New York Public Library\\nA. USER INTERFACE DESIGN OF THE BODHI SYSTEM\\nA prototype for the user interface in the BODHI system, which allows correction of OCR\\ntext and collection of tags from patrons was developed. The article manipulation module is\\ncapable of retrieving an article from the database based on a search criteria, displaying the\\nscanned OCR text alongside a high resolution image, highlighting sections of it when clicked,\\nallowing a user to edit the OCR text after checking the content in the high resolution image\\nand storing the corrected text back into the database. Addition of tags and comments\\non an article-by-article basis is permissible. The module also has an user authentication\\nmechanism for patrons registered to correct OCR, add notes or tags.\\nFigure 19 in the Appendix presents screen-shots from the prototype. The application uti-\\nlizes Rails 3.1 (on Ruby 1.9.2), a Model-View-Controller framework to map rows from the\\nPostgreSQL database into objects that may be utilized by the client. The viewer extends\\nthe functionality of a JQuery plugin, called ImgAreaSelect\\n(http://odyniec.net/projects/imgareaselect/examples.html), an open source inde-\\npendent project by Michael Wojciechowski. This plugin provides useful functions for ma-\\nnipulating an image, such as selection of a speci\\ufb01c area, customization of the behavior of\\nthe selection box (the dotted box shown in Figure 19) and functions that expose key events\\n(such as moving the selection box). In our application, the ImgAreaSelect plugin is utilized\\nby loading two images \\u2013 one in low resolution and the other in high resolution. The low\\nresolution image allows a user to select the part of the picture s/he wants enlarged. The\\nhigh resolution image is displayed by the \\u201cViewer\\u201d. Javascript code takes the selection of\\nthe low resolution image and scales to the a high resolution image and \\ufb01nally displays it in\\na box that overlaps the original image.\\nThe backend for the article manipulation module stores precise x and y coordinates for\\neach word \\u2013 the Rails application is capable of retrieving this data and using the ImgAreaS-\\nelect plugin to pinpoint a word that is clicked on both the low resolution preview picture\\nand the magni\\ufb01ed high resolution picture. The highlighting of a word is done by a simple\\nJQuery call that manipulates the Cascading Style Sheet (CSS) attributes of the Document\\nObject Model (DOM) element in the HTML.\\nc\\u20ddYYYY ACM 1539-9087/YYYY/01-ARTA $10.00\\nDOI 10.1145/0000000.0000000 http://doi.acm.org/10.1145/0000000.0000000\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\\nApp\\u20132\\nFig. 19.\\nThe OCR correction module of the BODHI system, illustrating features to highlight text in articles,\\ndisplay high-resolution images, add tags and comments from patrons from the database.\\nA basic login system is implemented using Devise 1.522, an open source plugin \\u201cgem\\u201d\\navailable to Rails. Each user will be required to create an account by specifying an email\\nand password and use the information to actually correct a newspaper article and create\\ntags.\\n22https://github.com/plataformatec/devise\\nACM Transactions on Knowledge Discovery from Data , Vol. V, No. N, Article A, Publication date: January YYYY.\\n\",\n          \"Show, Ask, Attend, and Answer:\\nA Strong Baseline For Visual Question Answering\\nVahid Kazemi\\nAli Elqursh\\nGoogle Research\\n1600 Amphitheater Parkway\\n{vahid, elqursh}@google.com\\nAbstract\\nThis paper presents a new baseline for visual question\\nanswering task. Given an image and a question in natural\\nlanguage, our model produces accurate answers according\\nto the content of the image. Our model, while being archi-\\ntecturally simple and relatively small in terms of trainable\\nparameters, sets a new state of the art on both unbalanced\\nand balanced VQA benchmark. On VQA 1.0 [2] open ended\\nchallenge, our model achieves 64.6% accuracy on the test-\\nstandard set without using additional data, an improvement\\nof 0.4% over state of the art, and on newly released VQA 2.0\\n[8], our model scores 59.7% on validation set outperform-\\ning best previously reported results by 0.5%. The results\\npresented in this paper are especially interesting because\\nvery similar models have been tried before [32] but signi\\ufb01-\\ncantly lower performance were reported. In light of the new\\nresults we hope to see more meaningful research on visual\\nquestion answering in the future.\\n1. Introduction\\nDeep neural networks in the last few years have made\\ndramatic impact in computer vision and natural language\\nprocessing \\ufb01elds. We are now able to build models that rec-\\nognize objects in the images with high accuracy [15, 26, 9].\\nBut we are still far from human level understanding of im-\\nages. When we as humans look at images we don\\u2019t just see\\nobjects but we also understand how objects interact and we\\ncan tell their state and properties. Visual question answer-\\ning (VQA) [2] is particularly interesting because it allows\\nus to understand what our models truly see. We present the\\nmodel with an image and a question in the form of natural\\nlanguage and the model generates an answer again in the\\nform of natural language.\\nA related and more throughly researched task to VQA is\\nimage caption generation [31, 28], where the task is to gen-\\nerate a representative description of an image in natural lan-\\nFigure 1. Top 5 predictions from our model and their probabilities\\nfor an example image/question pair. On the right we visualize the\\ncorresponding attention distribution produced by the model.\\nguage. A clear advantage of VQA over caption generation\\nis that evaluating a VQA model is much easier. There is not\\na unique caption that can describe an image. Moreover, it\\nis rather easy to come up with a single caption that more or\\nless holds for a large collection of images. There is no way\\nto tell what the model actually understands from the image\\nbased on a generic caption. Some previous work have been\\npublished that tried to mitigate this problem by providing\\ndense [12] or unambiguous captions [19], but this problem\\nis inherently less severe with VQA task. It is always possi-\\nble to ask very narrow questions forcing the model to give\\na speci\\ufb01c answer. For these reasons we believe VQA is a\\ngood proxy task for creating rich representations for mod-\\neling language and vision.\\nSome novel and interesting approaches [6, 22] have been\\npublished in the last few years on visual question answer-\\ning that showed promising results. However, in this work,\\nwe show that a relatively simple architecture (compared to\\nthe recent works) when trained carefully bests state the art.\\n1\\narXiv:1704.03162v2  [cs.CV]  12 Apr 2017\\n\\nFigure 2 provides a high level overview of our model. To\\nsummarize, our proposed model uses long short-term mem-\\nory units (LSTM) [11] to encode the question, and a deep\\nresidual network [9] to compute the image features. A soft\\nattention mechanism similar to [31] is utilized to compute\\nmultiple glimpses of image features based on the state of the\\nLSTM. A classi\\ufb01er than takes the image feature glimpses\\nand the \\ufb01nal state of the LSTM as input to produce proba-\\nbilities over a \\ufb01xed set of most frequent answers. On VQA\\n1.0 [2] open ended challenge, our model achieves 64.6%\\naccuracy on the test-standard set without using additional\\ndata, an improvement of 0.4% over state of the art, and on\\nnewly released VQA 2.0 [8], our model scores 59.7% on\\nvalidation set outperforming best reported results by 0.5%.\\nThis paper proves once again that when it comes to train-\\ning neural networks the devil is in the details [4].\\n2. Related work\\nIn this section we provide an overview of related work.\\nConvolutional neural networks (CNNs) [16] have revo-\\nlutionalized the \\ufb01eld of computer vision in the recent years.\\nLandmark paper by Krizhevsky et al. [15] for the \\ufb01rst time\\nshowed great success on applying a deep CNN on large\\nscale ImageNet [5] dataset achieving a dramatic improve-\\nment over state of the art methods that used hand designed\\nfeatures. In the recent years researchers have been hard at\\nwork training deeper [26], very deep [27], and even deeper\\n[9] neural networks. While success of neural networks are\\ncommonly attributed to larger datasets and more compute\\npower, there are a lot of details that we know and consider\\nnow that were not known just a few years ago. These in-\\nclude choice of activation function [21], initialization [7],\\noptimizer [14], and regularization [10]. As we show in this\\npaper at times getting the details right is more important\\nthan the actual architecture.\\nWhen it comes to design of deep neural networks, very\\nfew ideas have been consistently found advantageous across\\ndifferent domains. One of these ideas is notion of attention\\n[20, 28], which enables deep neural networks to extract lo-\\ncalized features from input data.\\nAnother neural network model that we take advantage\\nof in this work is Long Short-Term Memory (LSTM) [11].\\nLSTMs have been widely adopted by machine learning re-\\nsearchers in the recent years and have shown oustanding re-\\nsults on a wide range of problems from machine translation\\n[3] to speech recognition [24].\\nAll of these ideas have already been applied to visual\\nquestion answering task. In fact the model that we describe\\nin this work is very similar to stacked attention networks\\n[32], nevertheless we show signi\\ufb01cant improvement over\\ntheir result (5.8% on VQA 1.0 dataset). While more re-\\ncently much more complex and expensive attention models\\nhave been explored [6, 22, 18] their advantage is unclear in\\nthe light of the results reported in this paper.\\n3. Method\\nFigure 2 shows an overview of our model. In this section\\nwe formalize the problem and explain our approach in more\\ndetail.\\nWe treat visual question answering task as a classi\\ufb01ca-\\ntion problem. Given an image I and a question q in the form\\nof natural language we want to estimate the most likely an-\\nswer \\u02c6a from a \\ufb01xed set of answers based on the content of\\nthe image.\\n\\u02c6a = arg max\\na\\nP(a|I, q)\\n(1)\\nwhere a \\u2208{a1, a2, ..., aM}. The answers are chosen to be\\nthe most frequent answers from the training set.\\n3.1. Image embedding\\nWe use a pretrained convolutional neural network (CNN)\\nmodel based on residual network architecture [15] to com-\\npute a high level representation \\u03c6 of the input image I.\\n\\u03c6 = CNN(I)\\n(2)\\n\\u03c6 is a three dimensional tensor from the last layer of the\\nresidual network [9] before the \\ufb01nal pooling layer with\\n14 \\u00d7 14 \\u00d7 2048 dimensions. We furthermore perform l2\\nnormalization on the depth (last) dimension of image fea-\\ntures which enhances learning dynamics.\\n3.2. Question embedding\\nWe tokenize and encode a given question q into word\\nembeddings Eq = {e1, e2, ..., eP } where ei \\u2208RD, D is\\nthe length of the distributed word representation, and P is\\nthe number of words in the question. The embeddings are\\nthen fed to a long short-term memory (LSTM) [11].\\ns = LSTM(Eq)\\n(3)\\nWe use the \\ufb01nal state of the LSTM to represent the question.\\n3.3. Stacked attention\\nSimilar to [32], we compute multiple attention distribu-\\ntions over the spatial dimensions of the image features.\\n\\u03b1c,l \\u221dexp Fc(s, \\u03c6l)\\n\\u220b\\nL\\nX\\nl=1\\n\\u03b1c,l = 1\\n(4)\\nxc =\\nX\\nl\\n\\u03b1c,l\\u03c6l\\n(5)\\nEach image feature glimpse xc is the weighted average\\nof image features \\u03c6 over all the spatial locations l =\\n{1, 2, ..., L}. The attention weights \\u03b1c,l are normalized sep-\\narately for each glimpse c = 1, 2, ..., C.\\n\\nFigure 2. An overview of our model. We use a convolutional neural network based on ResNet [9] to embed the image. The input question\\nis tokenized and embedded and fed to a multi-layer LSTM. The concatenated image features and the \\ufb01nal state of LSTMs are then used to\\ncompute multiple attention distributions over image features. The concatenated image feature glimpses and the state of the LSTM is fed to\\ntwo fully connected layers two produce probabilities over answer classes.\\nIn practice F = [F1, F2, ..., FC] is modeled with two\\nlayers of convolution. Consequently Fi\\u2019s share parameters\\nin the \\ufb01rst layer. We solely rely on different initializations\\nto produce diverse attention distributions.\\n3.4. Classi\\ufb01er\\nFinally we concatenate the image glimpses along with\\nthe LSTM state and apply nonlinearities to produce proba-\\nbilities over answer classes.\\nP(ai|I, q) \\u221dexp Gi(x, s)\\n(6)\\nwhere\\nx = [x1, x2, ..., xC].\\n(7)\\nG = [G1, G2, ..., GM] in practice is modeled with two fully\\nconnected layers.\\nOur \\ufb01nal loss is de\\ufb01ned as follows.\\nL = 1\\nK\\nK\\nX\\nk=1\\n\\u2212log P(ak|I, q)\\n(8)\\nNote that we average the log-likelihoods over all the correct\\nanswers a1, a2, ..., aK.\\n4. Experiments\\n4.1. Dataset\\n4.1.1\\nVQA 1.0\\nWe evaluate our model on both balanced and unbalanced\\nversions of VQA dataset.\\nVQA 1.0 [2] is consisted of\\n204,721 images form the MS COCO dataset [17]. We eval-\\nuate our models on the real open ended challenge which\\nconsists of 614,163 questions and 6,141,630 answers. The\\ndataset comes with prede\\ufb01ned train, validation, and test\\nsplits. There is also a 25% subset of the the test set which\\nis referred to as test-dev split. For most of experiments we\\nused the train set as training data and reported the results\\non the validation set. To be comparable to prior work we\\nadditionally train our default model on train and val set and\\nreport the results on test set.\\n4.1.2\\nVQA 2.0\\nWe also evaluate our model on the more recent VQA 2.0\\n[8] which is consisted of 658,111 questions and 6,581,110\\nanswers. This version of the dataset is more balanced in\\ncomparison to VQA 1.0. Speci\\ufb01cally for every question\\nthere are two images in the dataset that result in two dif-\\nferent answers to the question. At this point only the train\\nand validation sets are available. We report the results on\\nvalidation set after training on train set.\\n4.2. Evaluation metric\\nWe evaluate our models on the open ended task of VQA\\nchallenge with the provided accuracy metric.\\nAcc(a) = 1\\nK\\nK\\nX\\nk=1\\nmin(\\nP\\n1\\u2264j\\u2264K,j\\u0338=k 1(a = aj)\\n3\\n, 1)\\n(9)\\nwhere a1, a2, ..., aK are the correct answers provided by the\\nuser and K = 10. Intuitively, we consider an answer cor-\\nrect if at least three annotators agree on the answer. To get\\nsome level of robustness we compute the accuracy over all\\n10 choose 9 subsets of ground truth answers and average.\\n5. Results\\n5.1. Baselines\\nIn this section we describe the details of our default base-\\nline as well as its mutations.\\n\\nSteps\\n1K\\n3K\\n6K\\n12K\\n25K\\n50K\\n100K\\n200K\\nDefault\\n37.16\\n46.96\\n55.07\\n58.12\\n59.76\\n60.65\\n60.94\\n60.95\\nNo l2 normalization\\n42.65\\n44.87\\n49.07\\n51.12\\n51.75\\n52.15\\n53.56\\n54.69\\nNo dropout on FC/Conv layers\\n32.78\\n38.19\\n49.02\\n58.63\\n57.90\\n57.42\\n57.30\\n56.98\\nNo dropout on LSTM layers\\n45.85\\n51.55\\n55.75\\n57.63\\n59.60\\n59.79\\n59.95\\n59.80\\nNo attention\\n38.09\\n48.36\\n51.42\\n54.43\\n56.02\\n57.13\\n57.65\\n57.72\\nSampling loss\\n47.24\\n47.67\\n51.80\\n54.85\\n56.69\\n57.62\\n58.85\\n59.44\\nWith positional features\\n33.26\\n41.37\\n55.36\\n57.95\\n59.75\\n60.44\\n61.02\\n61.09\\nBidirectional LSTM\\n42.33\\n52.38\\n55.93\\n58.32\\n59.99\\n60.63\\n60.69\\n60.63\\nWord embedding size: 100\\n39.53\\n50.24\\n53.94\\n56.74\\n58.92\\n59.96\\n60.75\\n60.90\\nWord embedding size: 300 (default)\\n37.16\\n46.96\\n55.07\\n58.12\\n59.76\\n60.65\\n60.94\\n60.95\\nWord embedding size: 500\\n37.21\\n47.15\\n55.44\\n58.43\\n59.98\\n60.60\\n61.01\\n61.04\\nLSTM state size: 512\\n46.59\\n51.20\\n55.33\\n57.96\\n59.46\\n60.31\\n60.79\\n61.09\\nLSTM state size: 1024 (default)\\n37.16\\n46.96\\n55.07\\n58.12\\n59.76\\n60.65\\n60.94\\n60.95\\nLSTM state size: 2048\\n33.24\\n39.11\\n50.86\\n57.48\\n59.75\\n60.65\\n60.93\\n60.80\\nLSTM state size: 1024 1024\\n37.78\\n48.19\\n54.28\\n57.20\\n59.34\\n60.22\\n60.62\\n60.75\\nAttention size: 512 1\\n36.54\\n45.74\\n54.23\\n57.42\\n59.46\\n60.22\\n60.85\\n60.96\\nAttention size: 512 2 (default)\\n37.16\\n46.96\\n55.07\\n58.12\\n59.76\\n60.65\\n60.94\\n60.95\\nAttention size: 512 3\\n36.26\\n45.16\\n55.22\\n57.96\\n59.77\\n60.60\\n60.87\\n61.12\\nAttention size: 1024 1\\n45.60\\n50.72\\n54.61\\n57.57\\n59.52\\n60.46\\n60.92\\n60.92\\nAttention size: 1024 2\\n35.04\\n42.72\\n55.56\\n58.03\\n59.66\\n60.54\\n61.14\\n61.10\\nClassi\\ufb01er size: 3000\\n30.19\\n43.12\\n53.38\\n56.18\\n57.82\\n58.25\\n58.24\\n58.12\\nClassi\\ufb01er size: 1024 3000 (default)\\n37.16\\n46.96\\n55.07\\n58.12\\n59.76\\n60.65\\n60.94\\n60.95\\nClassi\\ufb01er size: 2048 3000\\n48.28\\n52.57\\n56.02\\n58.51\\n59.96\\n60.46\\n60.84\\n60.95\\nClassi\\ufb01er size: 1024 1024 3000\\n44.51\\n49.53\\n53.25\\n55.95\\n57.59\\n58.83\\n60.05\\n60.66\\nTable 1. This table shows the result of different mutations of our default model. All models are trained on training set of VQA 1.0 [2] and\\nthe accuracy is reported on validation set according to equation 9. Applying l2 normalization, dropout, and using soft-attention signi\\ufb01cantly\\nimproves the accuracy of the model. Some of the previous works such as [6] had used the sampling loss, which we found to be leading\\nto signi\\ufb01cantly worse results and longer training time. Different word embedding sizes and LSTM con\\ufb01gurations were explored but we\\nfound it to be not a major factor. Contrary to results reported by [32] we found using stacked attentions to only marginally improve the\\nresult. We found a two layer deep classi\\ufb01er to be signi\\ufb01cantly better than a single layer, adding more layers or increasing the width did not\\nseem to improve the results.\\nIn all of the baselines input images are scaled while pre-\\nserving aspect ratio and center cropped to 299\\u00d7299 dimen-\\nsions. We found stretching the image to harm the perfor-\\nmance of the model. Image features are extracted from pre-\\ntrained 152 layer ResNet [9] model. We take the last layer\\nbefore the average pooling layer (of size 14 \\u00d7 14 \\u00d7 2048)\\nand perform l2 normalization in the depth dimension.\\nThe input question is tokenized and embedded to a\\nD = 300 dimensional vector. The embeddings are passed\\nthrough tanh nonlinearity before feeding to the LSTM. The\\nstate size of LSTM layer is set to 1024. Per example dy-\\nnamic unrolling is used to allow for questions of different\\nlength, although we cap maximum length of the questions\\nat 15 words.\\nTo compute attention over image features, we concate-\\nnate tiled LSTM state with image features over the depth\\ndimension and pass through a 1 \\u00d7 1 dimensional convolu-\\ntion layer of depth 512 followed by ReLU [21] nonlinearity.\\nThe output feature is passed through another 1 \\u00d7 1 convo-\\nlution of depth C = 2 followed by softmax over spatial di-\\nmensions to compute attention distributions. We use these\\ndistributions to compute two image glimpses by computing\\nthe weighted average of image features.\\nWe further concatenate the image glimpses with the state\\nof the LSTM and pass through a fully connected layer of\\nsize 1024 with ReLU nonlinearity. The output is fed to a\\nlinear layer of size M = 3000 followed by softmax to pro-\\nduce probabilities over most frequent classes.\\nWe only consider top M = 3000 most frequent answers\\nin our classi\\ufb01er. Other answers are ignored and do not con-\\ntribute to the loss during training. This covers 92% of the\\nanswers in the validation set in VQA dataset [2].\\nWe use dropout of 0.5 on input features of all layers in-\\ncluding the LSTM, convolutions, and fully connected lay-\\ners.\\nWe optimize this model with Adam optimizer [14] for\\n100K steps with batch size of 128. We use exponential\\ndecay to gradually decrease the learning rate according to\\nthe following equation.\\nlstep = 0.5\\nstep\\ndecay steps l0\\nThe initial learning rate is set to l0 = 0.001, and the decay\\nsteps is set to 50K. We set \\u03b21 = 0.9 and \\u03b22 = 0.999.\\nDuring training CNN parameters are kept \\ufb01xed. The rest\\n\\nof the parameters are initialized as suggested by Glorot et\\nal. [7].\\nTable 1 shows the performance of different baselines on\\nvalidation set of VQA 1.0 [2] when trained on the training\\nset only. We have reported results for the following muta-\\ntions of our default model:\\n\\u2022 No l2 norm: ResNet features are not l2 normalized.\\n\\u2022 No dropout on FC/Conv: Dropout is not applied to\\nthe inputs of fully connected and convolution layers.\\n\\u2022 No dropout on LSTM: Dropout is not applied to the\\ninputs of LSTM layers.\\n\\u2022 No attention: Instead of using soft-attention we per-\\nform average spatial pooling before feeding image fea-\\ntures to the classi\\ufb01er.\\n\\u2022 Sampled loss: Instead of averaging the log-likelihood\\nof correct answers we sample one answer at a time.\\n\\u2022 With positional features: Image features \\u03c6 are aug-\\nmented with x and y coordinates of each cell along the\\ndepth dimension producing a tensor of size 14 \\u00d7 14 \\u00d7\\n2050.\\n\\u2022 Bidirectional LSTM: We use a bidirectional LSTM to\\nencode the question.\\n\\u2022 Word embedding size: We try word embeddings of\\ndifferent sizes including 100, 300 (default), and 500.\\n\\u2022 LSTM state size: We explore different con\\ufb01gurations\\nof LSTM state sizes, this include a one layer LSTM\\nof size 512, 1024 (default), and 2048 or a stacked two\\nlayer LSTM of size 1024.\\n\\u2022 Attention size: Different attention con\\ufb01gurations are\\nexplored. First number indicates the size of \\ufb01rst convo-\\nlution layer and the second number indicates the num-\\nber of attention glimpses.\\n\\u2022 Classi\\ufb01er size: By default classi\\ufb01er G is consisted of\\na fully connected layer of size 1024 with ReLU non-\\nlinearity followed by a M = 3000 dimensional lin-\\near layer followed by softmax. We explore shallower,\\ndeeper, and wider alternatives.\\nl2 normalization of image features improved learning dy-\\nnamics leading to signi\\ufb01cantly better accuracy while reduc-\\ning the training time.\\nWe observed that applying dropout on multiple lay-\\ners (including fully connected layers, convolutions, and\\nLSTMs) is crucial to avoid over-\\ufb01tting on this dataset.\\nAs widely reported we con\\ufb01rm that using soft-attention\\nsigni\\ufb01cantly improves the accuracy of the model.\\nDifferent word embedding sizes and LSTM con\\ufb01gura-\\ntions were explored but we found it to be not a major factor.\\nA larger embedding size with a smaller LSTM seemed to\\nwork best.\\nSome of the previous works such as [6] had used the\\nsampling loss, which we found to be leading to signi\\ufb01cantly\\nworse results and longer training time.\\nContrary to results reported by [32] we found using\\nstacked attentions to only marginally improve the result.\\nWe found a two layer deep classi\\ufb01er to be signi\\ufb01cantly\\nbetter than a single layer, adding more layers or increasing\\nthe width did not seem to improve the results.\\n5.2. Comparison to state of the art\\nTable 2 shows the performance of our model on VQA\\n1.0 dataset. We trained our model on train and validation set\\nand tested the performance on test-standard set. Our model\\nachieves an overall accuracy of 64.6% on the test-standard\\nset, outperforming best previously reported results by 0.4%.\\nAll the parameters here are the same as the default model.\\nWhile architecturally our default model is almost iden-\\ntical to [32], some details are different. For example they\\nuse the VGG [25] model, while we use ResNet [9] to com-\\npute image features. They do not mention l2 normaliza-\\ntion of image features which found to be crucial to reducing\\ntraining time. They use SGD optimizer with momentum\\n\\u00b5 = 0.9, while we found that Adam [14] generally leads to\\nfaster convergence.\\nWe also reported our results on VQA 2.0 dataset 3. At\\nthis point we only have access to train and validation splits\\nfor this dataset. So we trained the same model on the train-\\ning set and evaluated the model on the validation set. Over-\\nall our model achieves 59.67% accuracy on the validation\\nset which is about 0.5% higher than best previously reported\\nresults.\\n6. Conclusion\\nIn this paper we presented a new baseline for visual ques-\\ntion answering task that outperforms previously reported re-\\nsults on VQA 1.0 and VQA 2.0 datasets. Our model is ar-\\nchitecturally very simple and in essence very similar to the\\nmodels that were tried before, nevertheless we show once\\nthe details are done right this model outperforms all the pre-\\nviously reported results.\\nReferences\\n[1] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neural\\nmodule networks. In 2016 IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR), 2016. 6\\n[2] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.\\nZitnick, and D. Parikh. Vqa: Visual question answering. In\\nInternational Journal of Computer Vision, 2015. 1, 2, 3, 4,\\n5, 6\\n\\nMethod\\nTest-Dev\\nTest-Standard\\nY/N\\nNum\\nOther\\nAll\\nY/N\\nNum\\nOther\\nAll\\nVQA team [2]\\n80.5\\n36.8\\n43.1\\n57.8\\n80.6\\n36.5\\n43.7\\n58.2\\nSAN (VGG) [32]\\n79.3\\n36.6\\n46.1\\n58.7\\n-\\n-\\n-\\n58.9\\nNMN (VGG) [1]\\n81.2\\n38.0\\n44.0\\n58.6\\n-\\n-\\n-\\n58.7\\nACK (VGG) [29]\\n81.0\\n38.4\\n45.2\\n59.2\\n81.1\\n37.1\\n45.8\\n59.4\\nDMN+ (VGG) [30]\\n80.5\\n36.8\\n48.3\\n60.3\\n-\\n-\\n-\\n60.4\\nMRN (ResNet) [13]\\n82.3\\n38.8\\n49.3\\n61.7\\n82.4\\n38.2\\n49.4\\n61.8\\nHieCoAtt (ResNet) [18]\\n79.7\\n38.7\\n51.7\\n61.8\\n-\\n-\\n-\\n62.1\\nDAN (VGG) [22]\\n82.1\\n38.2\\n50.2\\n62.0\\n-\\n-\\n-\\n-\\nRAU (ResNet) [23]\\n81.9\\n39.0\\n53.0\\n63.3\\n81.7\\n38.2\\n52.8\\n63.2\\nMCB (ResNet) [6]\\n82.2\\n37.7\\n54.8\\n64.2\\n-\\n-\\n-\\n-\\nDAN (ResNet) [22]\\n83.0\\n39.1\\n53.9\\n64.3\\n82.8\\n38.1\\n54.0\\n64.2\\nOurs (ResNet)\\n82.2\\n39.1\\n55.2\\n64.5\\n82.0\\n39.1\\n55.2\\n64.6\\nTable 2. This table shows a comparison of our model with state of the art on VQA 1.0 dataset. While our model is architecturally simpler\\nand smaller in terms of trainable parameters than most existing work, nevertheless it outperforms all the previous work.\\n(a) What brand is the shirt?\\n(b) What time is it?\\n(c) How does the man feel?\\n(d) What is the girl doing?\\nFigure 3. Qualitative results on sample images shows that our model can produce reasonable answers to a range of questions.\\n\\nMethod\\nY/N\\nNum\\nOther\\nAll\\nHieCoAtt [18]\\n71.80\\n36.53\\n46.25\\n54.57\\nMCB [6]\\n77.37\\n36.66\\n51.23\\n59.14\\nOurs\\n77.45\\n38.46\\n51.76\\n59.67\\nTable 3. Our results on VQA 2.0 [8] validation set when trained\\non the training set only. Our model achieves an overall accuracy\\nof 59.67% which marginally outperforms state of the art on this\\ndataset.\\n[3] D. Bahdanau, K. Cho, and Y. Bengio.\\nNeural machine\\ntranslation by jointly learning to align and translate. CoRR,\\nabs/1409.0473, 2014. 2\\n[4] K. Chat\\ufb01eld, K. Simonyan, A. Vedaldi, and A. Zisserman.\\nReturn of the devil in the details: Delving deep into convo-\\nlutional nets. In BMVC, 2014. 2\\n[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. Imagenet: A large-scale hierarchical image database.\\nIn 2009 IEEE Conference on Computer Vision and Pattern\\nRecognition, 2009. 2\\n[6] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and\\nM. Rohrbach. Multimodal compact bilinear pooling for vi-\\nsual question answering and visual grounding. In EMNLP,\\n2016. 1, 2, 4, 5, 6, 7\\n[7] X. Glorot and Y. Bengio.\\nUnderstanding the dif\\ufb01culty of\\ntraining deep feedforward neural networks.\\nIn AISTATS,\\n2010. 2, 5\\n[8] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and\\nD. Parikh. Making the v in vqa matter: Elevating the role\\nof image understanding in visual question answering. CoRR,\\nabs/1612.00837, 2016. 1, 2, 3, 7\\n[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\\ning for image recognition. In Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition, pages\\n770\\u2013778, 2016. 1, 2, 3, 4, 5\\n[10] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. Salakhutdinov. Improving neural networks by prevent-\\ning co-adaptation of feature detectors. CoRR, abs/1207.0580,\\n2012. 2\\n[11] S. Hochreiter and J. Schmidhuber. Long short-term memory.\\nNeural Computation, 9:1735\\u20131780, 1997. 2\\n[12] J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully\\nconvolutional localization networks for dense captioning. In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 4565\\u20134574, 2016. 1\\n[13] J.-H. Kim, S.-W. Lee, D. Kwak, M.-O. Heo, J. Kim, J.-W.\\nHa, and B.-T. Zhang. Multimodal residual learning for visual\\nqa. In Advances in Neural Information Processing Systems,\\npages 361\\u2013369, 2016. 6\\n[14] D. Kingma and J. Ba. Adam: A method for stochastic opti-\\nmization. arXiv preprint arXiv:1412.6980, 2014. 2, 4, 5\\n[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\\nImagenet\\nclassi\\ufb01cation with deep convolutional neural networks. In\\nNIPS, 2012. 1, 2\\n[16] Y. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional\\nnetworks and applications in vision. In ISCAS, 2010. 2\\n[17] T.-Y. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ra-\\nmanan, P. Doll\\u00b4ar, and C. L. Zitnick. Microsoft coco: Com-\\nmon objects in context. In ECCV, 2014. 3\\n[18] J. Lu, J. Yang, D. Batra, and D. Parikh.\\nHierarchical\\nquestion-image co-attention for visual question answering.\\nIn Advances In Neural Information Processing Systems,\\npages 289\\u2013297, 2016. 2, 6, 7\\n[19] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and\\nK. Murphy. Generation and comprehension of unambiguous\\nobject descriptions. In 2016 IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR), 2016. 1\\n[20] V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu. Recur-\\nrent models of visual attention. In NIPS, 2014. 2\\n[21] V. Nair and G. E. Hinton. Recti\\ufb01ed linear units improve re-\\nstricted boltzmann machines. In ICML, 2010. 2, 4\\n[22] H. Nam, J.-W. Ha, and J. Kim.\\nDual attention net-\\nworks for multimodal reasoning and matching.\\nCoRR,\\nabs/1611.00471, 2016. 1, 2, 6\\n[23] H. Noh and B. Han.\\nTraining recurrent answering units\\nwith joint loss minimization for vqa. CoRR, abs/1606.03647,\\n2016. 6\\n[24] H. Sak, A. W. Senior, and F. Beaufays.\\nLong short-term\\nmemory recurrent neural network architectures for large\\nscale acoustic modeling. In INTERSPEECH, 2014. 2\\n[25] K. Simonyan and A. Zisserman. Very deep convolutional\\nnetworks for large-scale image recognition. arXiv preprint\\narXiv:1409.1556, 2014. 5\\n[26] C. Szegedy,\\nW. Liu,\\nY. Jia,\\nP. Sermanet,\\nS. Reed,\\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\\nGoing deeper with convolutions. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition,\\npages 1\\u20139, 2015. 1, 2\\n[27] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.\\nRethinking the inception architecture for computer vision.\\nIn Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 2818\\u20132826, 2016. 2\\n[28] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and\\ntell: A neural image caption generator. In 2015 IEEE Confer-\\nence on Computer Vision and Pattern Recognition (CVPR),\\n2015. 1, 2\\n[29] Q. Wu, P. Wang, C. Shen, A. R. Dick, and A. van den Hen-\\ngel. Ask me anything: Free-form visual question answer-\\ning based on knowledge from external sources.\\nIn 2016\\nIEEE Conference on Computer Vision and Pattern Recog-\\nnition (CVPR), 2016. 6\\n[30] C. Xiong, S. Merity, and R. Socher. Dynamic memory net-\\nworks for visual and textual question answering. In ICML,\\n2016. 6\\n[31] K. Xu, J. Ba, J. R. Kiros, K. Cho, A. C. Courville,\\nR. Salakhutdinov, R. S. Zemel, and Y. Bengio. Show, at-\\ntend and tell: Neural image caption generation with visual\\nattention. In ICML, 2015. 1, 2\\n[32] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked\\nattention networks for image question answering. In Pro-\\nceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, pages 21\\u201329, 2016. 1, 2, 4, 5, 6\\n\",\n          \"Estimating Undirected Graphs\\nUnder Weak Assumptions\\nLarry Wasserman, Mladen Kolar and Alessandro Rinaldo\\nCarnegie Mellon University and The University of Chicago\\nSeptember 26 2013\\nWe consider the problem of providing nonparametric con\\ufb01dence guarantees for undirected graphs\\nunder weak assumptions. In particular, we do not assume sparsity, incoherence or Normality. We\\nallow the dimension D to increase with the sample size n. First, we prove lower bounds that\\nshow that if we want accurate inferences with low assumptions then there are limitations on the\\ndimension as a function of sample size. When the dimension increases slowly with sample size, we\\nshow that methods based on Normal approximations and on the bootstrap lead to valid inferences\\nand we provide Berry-Esseen bounds on the accuracy of the Normal approximation. When the\\ndimension is large relative to sample size, accurate inferences for graphs under low assumptions\\nare not possible. Instead we propose to estimate something less demanding than the entire partial\\ncorrelation graph. In particular, we consider: cluster graphs, restricted partial correlation graphs\\nand correlation graphs.\\n1. Introduction.\\nThere are many methods for estimating undirected graphs, such as the\\nglasso (Yuan and Lin, 2007; Friedman and Tibshirani, 2007) and sparse parallel regression\\n(Meinshausen and B\\u00a8uhlmann, 2006). While these methods are very useful, they rely on\\nstrong assumptions, such as Normality, sparsity and incoherence, and they do not come with\\ncon\\ufb01dence guarantees. Recently, some papers \\u2014 such as Liu (2013) and Ren et al. (2013)\\n\\u2014 have provided con\\ufb01dence guarantees. Moreover, they have eliminated the incoherence\\nassumption. But they still rely on Normality, eigenvalue conditions and sparsity.\\nThe purpose of this paper is to construct a nonparametric estimator bG of an undirected\\ngraph G with con\\ufb01dence guarantees that does not make these assumptions. Our approach is\\nvery traditional; when the dimension Dn is less than the sample size n (but increasing with n)\\nwe simply use the bootstrap or the delta method to get con\\ufb01dence intervals for the partial\\ncorrelations. We put an edge between two nodes if zero is not in the con\\ufb01dence interval.\\nWhen Dn is larger than n, we avoid sparsity and eigenvalue conditions and instead, we\\nagain rely on a more traditional method, namely, dimension reduction. We provide explicit\\nBerry-Esseen style bounds for the delta method and the bootstrap. Indeed, while the low\\ndimensional case and high dimensional case have received much attention, the moderate\\ndimensional case \\u2014 where Dn increases with n but is less than n \\u2014 has not received much\\nattention lately. Examples of research for increasing but moderate dimensions are Portnoy\\n(1988) and Mammen (1993). Our results are very much in the spirit of those papers. However,\\nour emphasis is on \\ufb01nite sample Berry-Esseen style bounds.\\nThe con\\ufb01dence guarantee we seek is\\n(1)\\nP n( bG \\u2282G) \\u22651 \\u2212\\u03b1 \\u2212O(rn)\\nwhere n is the sample size, P n denotes the distribution for n observations drawn from P and\\nrn is an explicit rate. The notation bG \\u2282G means that the edges of bG are a subset of the\\n1\\narXiv:1309.6933v1  [math.ST]  26 Sep 2013\\n\\nedges of G. This means that, with high probability, there are no false edges. Of course, one\\ncould use other error measures such as false discovery rates, but we shall use the guarantee\\ngiven by (1).\\nWe focus at \\ufb01rst on partial correlation graphs: a missing edge means that the corresponding\\npartial correlation is 0. We distinguish two cases. In the \\ufb01rst case, Dn can increase with n but\\nis smaller than n. In that case we show that Gaussian asymptotic methods and bootstrap\\nmethods yield accurate con\\ufb01dence intervals for the partial correlations which then yield\\ncon\\ufb01dence guarantees for the graph. The accuracy of the coverage is O(log Dn/n1/8). We\\nalso show that, in principle, one can construct \\ufb01nite sample intervals, but these intervals\\nturn out to be too conservative to be useful.\\nIn the second case, Dn can be large, even larger than n. In this case it is not possible to\\nget valid inferences for the whole graph under weak assumptions. We investigate several\\nways to handle this case including: cluster graphs, restricted partial correlation graphs and\\ncorrelation graphs.\\nContributions. We provide graph estimation methods with these properties:\\n1. The methods provides con\\ufb01dence guarantees.\\n2. The methods do not depend on Normality or other parametric assumptions.\\n3. The methods do not require sparsity or incoherence conditions.\\n4. The methods have valid coverage when the dimension increases with the sample size.\\n5. The methods are very simple and do not require any optimization.\\n6. In Section 6 we develop new results for the delta method and the bootstrap with\\nincreasing dimension.\\nRelated Work. Our approach is similar to the method in Liu (2013), later improved by Ren\\net al. (2013). He uses tests on partial correlations to estimate an undirected graph. His\\napproach has two advantages over other methods: it eliminates the need to choose a tuning\\nparameter (as in the glasso) and it provided error control for the estimated graph. However,\\nthe results in that paper assume conditions like those in most papers on the lasso, namely,\\nsparsity. These conditions might be reasonable in some situations, but our goal is to estimate\\nthe graph without invoking these assumptions. In the special case of \\ufb01xed dimension, our\\nmethod is the same as that in Drton and Perlman (2004).\\nSch\\u00a8afer et al. (2005), building on work by Ledoit and Wolf (2004), consider a shrinkage\\napproach to estimating graphs. They make no sparsity or incoherence assumptions. Their\\nexamples suggest that their approach can work well in high dimensions. From our point\\nof view, their method introduces a bias-validity tradeo\\ufb00: large shrinkage biases the partial\\ncorrelations but have valid asymptotics in high dimensions. Low shrinkage has low bias but\\ncompromises the validity of the asymptotics in high dimensions. Shrinkage graphs are beyond\\nthe scope of this paper, however.\\nOutline. We start with some notation in Section 2. We discuss various assumptions in Sec-\\n2\\n\\ntion 3. We then establish lower bounds in Section 4. Finite sample methods are presented in\\nSection 5. However, these do not work well in practice. Asymptotic methods for the mod-\\nerate dimensional case are considered in Section 6. Speci\\ufb01cally, we develop a delta method\\nand a bootstrap method that accommodate increasing dimension. Recent results on high\\ndimensional random vectors due to Chernozhukov, Chetverikov and Kato (2012, 2013) play\\nan important role in our analysis. Methods for the high-dimensional case are considered in\\nSection 7. In Section 8 we give some numerical experiments and some examples. Concluding\\nremarks are in Section 9.\\n2. Notation.\\nLet Y1, . . . , Yn \\u2208RD be a random sample from a distribution P. Each Yi =\\n(Yi(1), . . . , Yi(D))T is a vector of length D. We allow D \\u2261Dn to increase with n. We do not\\nassume that the Yi\\u2019s are Gaussian. If A is a matrix, we will sometimes let Ajk denote the\\n(j, k) element of that matrix.\\nLet \\u03a3 \\u2261\\u03a3(P) denote the D \\u00d7 D covariance matrix of Yi and let \\u2126= \\u03a3\\u22121. Let \\u0398 = {\\u03b8}jk be\\nthe matrix partial correlations:\\n(2)\\n\\u03b8jk = \\u2212\\n\\u2126jk\\np\\n\\u2126jj\\u2126kk\\n.\\nLet\\n(3)\\nSn = 1\\nn\\nn\\nX\\ni=1\\n(Yi \\u2212Y )(Yi \\u2212Y )T\\nbe the the sample covariance matrix and let b\\u0398n be the matrix of sample partial correlations.\\nGiven a matrix of partial correlations \\u0398 let G \\u2261G(P) be the undirected graph with D nodes\\nand such that there is an edge between nodes j and k if and only if \\u03b8jk \\u0338= 0. Equivalently,\\nthere is an edge if and only if \\u2126jk \\u0338= 0. In Section 7 we consider other graphs.\\nFor any matrix A, let vec(A) denote the vector obtained by stacking the columns of A. We\\nde\\ufb01ne the following quantities:\\n\\u00b5 = E(Y ),\\n\\u03c3 = vec(\\u03a3),\\n\\u03c9 = vec(\\u2126)\\n(4)\\ns = vec(Sn),\\n\\u03b4 = \\u221an(s \\u2212\\u03c3),\\n\\u2206= \\u221an(Y \\u2212\\u00b5).\\n(5)\\nIf A is m \\u00d7 n then there is a unique permutation matrix Kmn \\u2013 called the commutation\\nmatrix \\u2013 such that\\n(6)\\nKmnvec(A) = vec(AT).\\nLet J denote a D \\u00d7 D matrix of one\\u2019s. For matrices L and U with the same dimensions,\\nwe write L \\u2264U to mean that Ljk \\u2264Ujk for all j, k. If A is m \\u00d7 n and B is p \\u00d7 q then the\\nKronecker product A \\u2297B is the mp \\u00d7 nq matrix\\n(7)\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nA11B\\n\\u00b7 \\u00b7 \\u00b7\\nA1nB\\n...\\n...\\nAm1B\\n\\u00b7 \\u00b7 \\u00b7\\nAmnB\\n\\uf8f9\\n\\uf8fa\\uf8fb.\\n3\\n\\nThe Frobenius norm of A is ||A||F =\\nqP\\nj,k A2\\njk, the operator norm is ||A|| = sup||x||=1 ||Ax||\\nand the max norm is ||A||max = maxj,k |Ajk|. Let ||A||1 = maxj\\nPD\\ni=1 |Aij| and\\n(8)\\n|||A||| =\\nX\\njk\\n|Ajk|.\\nWe let \\u03a6 denote the cdf of a standard Normal random variable. Recall that a random vector\\nX \\u2208Rk is sub-Gaussian if there exists \\u03b6 > 0 such that, for all t \\u2208Rk,\\n(9)\\nEetT (X\\u2212\\u00b5) \\u2264e||t||2 \\u03b62/2\\nwhere \\u00b5 = E(X). The smallest and largest eigenvalues of a matrix A are denoted by \\u03bbmin(A)\\nand \\u03bbmax(A). We write an \\u2aafbn to mean that there is some c > 0 such that an \\u2264cbn for all\\nlarge n. We often use C to denote a generic positive constant.\\n3. Assumptions.\\nIn this section we discuss the assumptions we make and we also discuss\\nsome of the commonly used assumptions that we will not use.\\nThe Assumptions. In the case where Dn < n we make the following assumptions:\\n(A1) Y and vec(Y Y T) are sub-Gaussian.\\n(A2) 0 < a \\u2264\\u03bbmin(\\u03a3) \\u2264\\u03bbmax(\\u03a3) \\u2264A < \\u221e.\\n(A3) \\u03bbmin(T) \\u2265c0 > 0 where T is the asymptotic covariance of \\u221an(s \\u2212\\u03c3) and is given\\nin Equation (23). Also assume that minj \\u03b3jj > 0 where \\u03b3, the asymptotic variances of the\\nsample partial correlations, is given in (31).\\n(A4) maxj E|Vi(j)|3 \\u2264C where Vi = vec[(Yi \\u2212\\u00b5)(Yi \\u2212\\u00b5)T] \\u2212\\u03c3.\\nIn the case where Dn > n we do not make these assumptions. Indeed, (A3) requires that\\nDn < n. Instead, when Dn > n, we \\ufb01rst perform a dimension reduction and then we assume\\n(A1)-(A4) on the reduced problem. We remark that the sub-Gaussian assumption is stronger\\nthan needed and is made for simplicity.\\nThe Non-Assumptions. Now we discuss the assumptions that are commonly made for\\nthis problem, but that we will not use.\\n(B1) Normality. Y \\u223cN(\\u00b5, \\u03a3).\\n(B2) Incoherence. The incoherence condition is\\n(10)\\n||\\u0393ScS(\\u0393SS)\\u22121||\\u221e< 1\\nwhere \\u0393 = \\u03a3 \\u2297\\u03a3, S is the set of pairs with edges between them and || \\u00b7 ||\\u221eis the maximum\\nabsolute column sum.\\n4\\n\\n(B3) Sparsity. The typical sparsity assumption is that the maximum degree d of the graph\\nis o(\\u221an).\\n(B4) Eigenvalues. 0 < a \\u2264\\u03bbmin(\\u03a3) \\u2264\\u03bbmax(\\u03a3) \\u2264A < \\u221e.\\n(B5) Donut. It is assumed that each partial correlation is either 0 or is strictly larger than\\np\\nlog D/n, thus forbidding a donut around the origin.\\nDiscussion. The above assumptions may be reasonable in certain specialized cases. However,\\nfor routine data-analysis, we regard these assumptions with some skepticism when Dn > n.\\nThey serve to guarantee that many high-dimensional methods will work, but seem unreal-\\nistic in practice. Moreover, the assumptions are very fragile. The incoherence assumption is\\nespecially troubling although Ren et al. (2013) have been able to eliminate it. The donut\\nassumption ensures that non-zero partial correlations will be detected with high probability.\\nThe eigenvalue assumption (B4) is quite reasonable when Dn < n. But when Dn is much\\nlarger than n, (B4) together with (B3) are very strong and may rule out many situations\\nthat occur in real data analysis practice. To the best of our knowledge, (B3) and (B4) are\\nnot testable when Dn > n. Our goal is to develop methods that avoid these assumptions. Of\\ncourse, our results will also be weaker which is the price we pay for giving up strong assump-\\ntions. They are weaker because we only are able to estimate the graph of a dimension-reduced\\nversion of the original problem.\\n4. Lower Bounds.\\nConstructing a graph estimator for which (1) holds is easy: simply set\\nbG to be identically equal to the empty graph. Then bG will never contain false edges. But to\\nhave a useful estimator we also want to have non-trivial power to detect edges; equivalently,\\nwe want con\\ufb01dence intervals for the partial correlations to have width that shrinks with\\nincreasing sample size. In this section we \\ufb01nd lower bounds on the width of any con\\ufb01dence\\ninterval for partial correlations. This reveals constraints on the dimensions D as a function\\nof the sample size n. Speci\\ufb01cally, we show (without sparsity) that one must have Dn < n\\nto get consistent con\\ufb01dence intervals. This is not surprising, but we could not \\ufb01nd explicit\\nminimax lower bounds for estimating partial correlations so we provide them here.\\nThe problem of estimating a partial correlation is intimately related to the problem of esti-\\nmating regression coe\\ufb03cients. Consider the usual regression model\\n(11)\\nY = \\u03b21X1 + . . . + \\u03b2DXD + \\u03f5\\nwhere \\u03f5 \\u223cN(0, \\u03c32) and where we take the intercept to be 0 for simplicity. (Normality is\\nassumed only in this section.) Suppose we want a con\\ufb01dence interval for \\u03b21.\\nWe will need assumptions on the covariance matrix \\u03a3 for X = (X1, . . . , Xd). Again, since\\nwe are interested in the low assumption case, we do not want to impose strong assumptions\\non \\u03a3. In particular, we do not want to rule out the case where the covariates are highly\\ncorrelated. We do, however, want \\u03a3 to be invertible. Let S denote all symmetric matrices\\nand let\\n(12)\\nS(a, A) =\\nn\\n\\u03a3 \\u2208S : a \\u2264\\u03bbmin(\\u03a3) \\u2264\\u03bbmax(\\u03a3) \\u2264A\\no\\n5\\n\\nwhere 0 < a \\u2264A < \\u221e. To summarize: Y = \\u03b2TX +\\u03f5 where \\u03f5 \\u223cN(0, \\u03c32), and \\u03a3 = Cov(X) \\u2208\\nS(a, A). Let P be all such distributions.\\nA set-valued function Cn is a 1 \\u2212\\u03b1 con\\ufb01dence interval for \\u03b21 if\\n(13)\\nP n(\\u03b21 \\u2208Cn) \\u22651 \\u2212\\u03b1\\nfor all P \\u2208P. Let Cn denote all 1 \\u2212\\u03b1 con\\ufb01dence intervals. Let\\n(14)\\nWn = sup{x : x \\u2208Cn} \\u2212inf{x : x \\u2208Cn}\\nbe the width of Cn.\\nTheorem 1 Assume that Dn < n \\u2212D \\u22121 and that \\u03b1 < 1/3. Then\\n(15)\\ninf\\nCn\\u2208Cn sup\\nP\\u2208P\\nE(W 2\\nn) \\u2265\\nC\\nn \\u2212D + 1\\nfor some C > 0.\\nProof. Let us write the model in vectorized form:\\n(16)\\nY = X\\u03b2 + \\u03f5\\nwhere Y = (Y1, . . . , Yn)T, X is n \\u00d7 D, \\u03b2 = (\\u03b21, . . . , \\u03b2D)T and \\u03f5 = (\\u03f51, . . . , \\u03f5n)T.\\nLet M = N(0, \\u03a3) with \\u03bbmin(\\u03a3) \\u2265a > 0. Let p0(x, y) = p0(y|x)m(x) and p1(x, y) =\\np1(y|x)m(x) where p0(y|x) and p1(y|x) will be speci\\ufb01ed later. Now\\ninf\\nCn\\u2208Cn sup\\nP\\u2208P\\nE(Wn) \\u2265inf\\nCn\\u2208Cn max\\nP\\u2208P0,P1 E(Wn)\\n= inf\\nCn\\u2208Cn max\\nP\\u2208P0,P1\\nZ\\nE(Wn|X = x)dM(x)\\n= inf\\nCn\\u2208Cn max\\nj=0,1\\nZ\\nRj(x)dM(x)\\nwhere Rj(x) = Ej(Wn|X = x). Let\\nA =\\nn\\nx : R0(x) > R1(x)\\no\\n.\\nFor any two real numbers r0, r1, we have that max{r0, r1} \\u2265(r0 + r1)/2. Hence,\\nZ\\nR0(x)dM(x) \\u2228\\nZ\\nR1(x)dM(x) \\u2265\\nZ\\nA\\nR0(x)dM(x) \\u2228\\nZ\\nAc R1(x)dM(x)\\n=\\nZ\\nA\\n[R0(x) \\u2228R1(x)]dM(x) \\u2228\\nZ\\nAc[R0(x) \\u2228R1(x)]dM(x)\\n\\u22651\\n2\\n\\u0012Z\\nA\\n[R0(x) \\u2228R1(x)]dM(x) +\\nZ\\nAc[R0(x) \\u2228R1(x)]dM(x)\\n\\u0013\\n= 1\\n2\\nZ\\n[R0(x) \\u2228R1(x)]dM(x).\\n6\\n\\nHence,\\ninf\\nCn\\u2208Cn sup\\nP\\u2208P\\nE(Wn) \\u2265inf\\nCn\\n1\\n2\\nZ\\n[E0(Wn|X = x) \\u2228E1(Wn|X = x)]dM(x)\\n\\u22651\\n2\\nZ\\ninf\\nCn max\\nP0,P1 EP(Wn|X = x)dM(x).\\nNow we \\ufb01x X = x \\u2208Rn\\u00d7D and lower bound infCn maxP0,P1 EP(Wn|X = x). Assume that\\nxTx is invertible. Consider Equation (16) where the matrix X is taken as \\ufb01xed. Multiplying\\neach term in the equation by (xTx)\\u22121xT we can rewrite the equation as\\nZ = \\u03b2 + \\u03be\\nwhere, given X = x, \\u03be \\u223cN(0, (xTx)\\u22121).\\nLet S = xTx, b > 0, \\u03b42 = 4\\u03b12S\\u22121\\n11 , \\u03b20 = (0, b, . . . , b) and \\u03b21 = (\\u03b4, b, . . . , b) which now de\\ufb01nes\\nP0 and P1. The (conditional) Kullback-Leibler distance between p0(y|x) and p1(y|x) is\\n1\\n2(\\u03b21 \\u2212\\u03b20)T(xTx)(\\u03b21 \\u2212\\u03b20) = 2\\u03b12S\\u22121\\n11 S11.\\nNote that, since D < n \\u22121, xTx is invertible with probability one. The conditional total\\nvariation distance is thus bounded above by TV(x) \\u2261\\u03b1\\np\\nS\\u22121\\n11 S11. Let A0 = {0 \\u2208Cn} and\\nA1 = {\\u03b4 \\u2208Cn}. Note that A0 \\u2229A1 implies that W 2\\nn \\u2265\\u03b42. So, given X = x,\\nP0(W 2\\nn \\u2265\\u03b42|X = x) \\u2265P0(A0 \\u2229A1|X = x)\\n= P0(A0|X = x) + P0(A1|X = x) \\u2212P0(A0 \\u222aA1|X = x)\\n\\u2265P0(A0|X = x) + P0(A1|X = x) \\u22121\\n\\u2265P0(A0|X = x) + P1(A1|X = x) \\u22121 \\u2212TV(x).\\nNote that\\nR\\nTV(x)dM(x) \\u2264\\u03b1\\nR p\\nS\\u22121\\n11 S11dM(X). Now\\nR p\\nS\\u22121\\n11 S11dM(x) \\u21921 as n \\u2192\\u221e.\\nThus, for large enough n,\\nR\\nTV(x)dM(x) \\u22642\\u03b1. Integrating over dM(x) we have\\nP0(W 2\\nn \\u2265\\u03b42) \\u2265P0(A0) + P1(A1) \\u22121 \\u2212\\nZ\\nTV(x)dM(x)\\n\\u2265[1 \\u2212\\u03b1] + [1 \\u2212\\u03b1] \\u22121 \\u22122\\u03b1 = 1 \\u22124\\u03b1.\\nLet E =\\n\\b\\nS\\u22121\\n11 \\u2265\\nC\\nn\\u2212D+1\\n\\t\\nwhere C is a small positive constant. Then,\\nP0(W 2\\nn \\u2265\\u03b42) = P0\\n\\u0000W 2\\nn \\u22654\\u03b12S\\u22121\\n11\\n\\u0001\\n= P0\\n\\u0000W 2\\nn \\u22654\\u03b12S\\u22121\\n11 , E\\n\\u0001\\n+ P0\\n\\u0000W 2\\nn \\u22654\\u03b12S\\u22121\\n11 , Ec\\u0001\\n\\u2264P0\\n\\u0012\\nW 2\\nn \\u2265\\n4C\\u03b12\\nn \\u2212D + 1\\n\\u0013\\n+ P0 (Ec) .\\n7\\n\\nRecalling that C is a small positive constant,\\nP0(Ec) = P0\\n\\u0012\\nS\\u22121\\n11 <\\nC\\nn \\u2212D + 1\\n\\u0013\\n= P0\\n\\u0012 1\\nS\\u22121\\n11\\n> n \\u2212D + 1\\nC\\n\\u0013\\n= P0\\n\\u0012\\n\\u03c72\\nn\\u2212D+1 > n \\u2212D + 1\\nC\\n\\u0013\\n< 1\\nn.\\nSo\\nP0\\n\\u0012\\nW 2\\nn \\u2265\\n4C\\u03b12\\nn \\u2212D + 1\\n\\u0013\\n\\u2265P0(W 2\\nn \\u2265\\u03b42) \\u22121\\nn \\u22651 \\u22124\\u03b1 \\u22121\\nn.\\nBy Markov\\u2019s inequality,\\nE0(W 2\\nn) \\u2265\\n\\u0012\\n1 \\u22124\\u03b1 \\u22121\\nn\\n\\u0013\\n4C\\u03b12\\nn \\u2212D + 1 \\u2ab0\\n1\\nn \\u2212D + 1.\\n\\u25a1\\nNow we establish the analogous upper bound.\\nTheorem 2 Assume that Dn < n \\u2212D + 1 and that \\u03b1 < 1/3. Then\\n(17)\\ninf\\nCn\\u2208Cn sup\\nP\\u2208P\\nE(W 2\\nn) \\u2aaf\\nC\\nn \\u2212D + 1.\\nProof. We derive a sharp \\u2113\\u221ebound on b\\u03b2 \\u2212\\u03b2. Consider the following model\\nY = X\\u03b2 + \\u03f5\\nwhere Y \\u2208Rn, X \\u2208Rn\\u00d7D are jointly Gaussian. In particular, xi \\u223cN(0, \\u03a3) and \\u03f5i \\u223cN(0, \\u03c32\\n\\u03f5).\\nThe OLS estimator is\\nb\\u03b2 = \\u03b2 + (XTX)\\u22121X\\u03f5 = \\u03b2 + Z.\\nSince Z|X \\u223cN(0, \\u03c32\\n\\u03f5(X\\u2032X)\\u22121), we have that\\n|Zj| \\u2264\\nq\\n\\u03c3\\u22121\\n\\u03f5 (XTX)\\u22121\\njj log(2\\u03b1\\u22121)\\nwith probability 1 \\u2212\\u03b1/2, conditional on X. We have that XTX \\u223cWD(\\u03a3, n) and\\n\\u03a3\\u22121\\njj\\n(X\\u2032X)\\u22121\\njj\\n\\u223c\\u03c72\\nn\\u2212D+1.\\nFor T \\u223c\\u03c72\\nD, we have\\nP(|D\\u22121T \\u22121| \\u2265x) \\u2264exp\\u22123\\n16 Dx2 .\\n8\\n\\nTherefore, setting x =\\nq\\n16\\n3\\nlog(2\\u03b1\\u22121)\\nn\\u2212D+1 ,\\n(X\\u2032X)\\u22121\\njj \\u2264\\n\\u03a3\\u22121\\njj\\n(1 \\u2212x)(n \\u2212D + 1)\\nwith probability 1 \\u2212\\u03b1/2. Combining the results, we have that for j \\u2208[p],\\n|Zj| \\u2264\\ns\\n\\u03c32\\n\\u03f5(1 \\u2212x)\\u22121\\u03a3\\u22121\\njj log(2\\u03b1\\u22121)\\nn \\u2212D + 1\\nwith probability 1 \\u2212\\u03b1/2. The second inequality hold under the assumption that D = o(n).\\nUsing the lower quantile we obtain a \\u03b1/2 level lower bound. This yields a con\\ufb01dence interval\\nwith squared length of order O(1/(n \\u2212D + 1)). \\u25a1\\nNow consider estimating a partial correlation corresponding to a covariance matrix \\u03a3.\\nTheorem 3 Let W \\u2208RD where W \\u223cN(0, \\u03a3) with \\u03a3 \\u2208Sa. Let \\u03b8 be the partial correlation\\nbetween two components of W, say, WD and WD\\u22121. Let Cn be the set of 1 \\u2212\\u03b1 con\\ufb01dence\\nintervals for \\u03b8. Assume that Dn \\u2264n and that \\u03b1 < 1/4. Then\\n(18)\\ninf\\nCn\\u2208Cn sup\\nP\\u2208P\\nE(W 2\\nn) \\u2265\\nC\\nn \\u2212D + 1.\\nProof. Let b > 0 be a small positive constant. Let W = (W1, . . . , WD) where\\nW1 = \\u03f51\\nW2 = bW1 + \\u03f52\\nW3 = bW2 + bW1 + \\u03f53\\n... = ...\\nWD = qWD\\u22121 + bWD\\u22122 + \\u00b7 \\u00b7 \\u00b7 + bW1 + \\u03f5D,\\n\\u03f51, . . . , \\u03f5D \\u223cN(0, 1). For P0 take q = 0 and for P1 take q = \\u03b4. So, P0 = N(0, \\u03a30) and\\nP1 = N(0, \\u03a31), say. Then \\u21261 = \\u03a3\\u22121\\n1\\ncorresponds to a complete graph while \\u21260 = \\u03a3\\u22121\\n0\\nhas a\\nmissing edge. See Figure 1. Let us write W = (Y, X) where Y = W1 and X = (W2, . . . , WD).\\nWe note that the marginal distribution of X is the same under P0 and P1. The conditional\\ndistribution of Y given X under Pj can be written\\nY = \\u03b2T\\nj X + \\u03f5\\nwhere \\u03b20 = (0, b, . . . , b) and \\u03b21 = (\\u03b4, b, . . . , b). The rest of the proof follows the proof of\\nTheorem 1. \\u25a1\\nWe conclude that without further assumptions (namely sparsity plus incoherence) we cannot\\nmake reliable inferences unless D < n.\\nRemark 4 These lower bounds were computed under the assumption of Normality. This is\\ngood enough to show the dependence on dimension. However, this makes the minimax lower\\nbound optimistic. When we develop the methods, we shall not assume Normality.\\n9\\n\\nFig 1.\\nThe two graphs in the proof. Left: \\u21261 corresponds to a dense graph. Right: \\u21260 is the same as \\u21261\\nexcept that an edge has been dropped.\\n5. A Finite Sample Method.\\nFor completeness, we give here a \\ufb01nite sample con\\ufb01dence\\ninterval that has length O(\\np\\nD/n). However, the intervals do not work well in practice and\\nwe explore asymptotic methods in the following section. In this section we suppose that\\n|Yij| \\u2264B for some \\ufb01nite constant B. First we recall the following result from Vershynin\\n(2010).\\nTheorem 5 (Vershynin 2010) There exists c\\u03b1, depending only on B, such that\\nP n\\n \\n||S \\u2212\\u03a3|| > c\\u03b1\\nr\\nD\\nn\\n!\\n\\u2264\\u03b1.\\nTheorem 6 Let\\n(19)\\n\\u03f5n = c\\u03b1\\nb\\u03bb2\\nr\\nD\\nn\\n \\n1 \\u2212c\\u03b1\\nb\\u03bb\\nr\\nD\\nn\\n!\\u22121\\nwhere b\\u03bb is the smallest eigenvalue of Sn. Let \\u2206n = 2\\u03f5n/(1 \\u2212\\u03f5n). Then\\n(20)\\ninf\\nP\\u2208P P n(\\u0398 \\u2264\\u0398 \\u2264\\u0398) \\u22651 \\u2212\\u03b1\\nwhere \\u0398 = b\\u0398 + \\u2206nJ and \\u0398 = b\\u0398 \\u2212\\u2206nJ where we recall that J is a D \\u00d7 D matrix of one\\u2019s.\\nProof. By the previous result, ||S \\u2212\\u03a3|| \\u2264c\\u03b1\\nq\\nD\\nn with probability at least 1\\u2212\\u03b1. From Horn\\nand Johnson (1990) page 381,\\n||S\\u22121 \\u2212\\u03a3\\u22121||max \\u2264||S\\u22121|| ||S\\u22121(\\u03a3 \\u2212S)||\\n1 \\u2212||S\\u22121(\\u03a3 \\u2212S)|| .\\nNote that, with probability at least 1 \\u2212\\u03b1,\\n(21)\\n||S\\u22121(\\u03a3 \\u2212S)|| \\u2264||S\\u22121|| ||\\u03a3 \\u2212S|| = ||\\u03a3 \\u2212S||\\nb\\u03bb\\n\\u2264c\\u03b1\\nb\\u03bb\\nr\\nD\\nn .\\n10\\n\\nAlso note that ||S\\u22121|| \\u22641/b\\u03bb. We conclude that\\n||S\\u22121 \\u2212\\u03a3\\u22121||max \\u2264\\u03f5n.\\nFrom Lemma 3 of Harris and Drton (2012), ||b\\u0398 \\u2212\\u0398||max \\u2264\\n2\\u03b4\\n1\\u2212\\u03b4 where \\u03b4 = ||S\\u22121 \\u2212\\u03a3\\u22121||max.\\nThe result follows. \\u25a1\\nDespite the apparent optimal rate, in practice the con\\ufb01dence intervals are gigantic. Instead,\\nwe turn to asymptotic methods.\\n6. Increasing Dimension.\\nWe call the case where Dn is increasing with n but smaller\\nthan n, the moderate dimensional case. Here we derive con\\ufb01dence sets for the partial corre-\\nlations in this case. We deal with the high-dimensional case Dn > n in the next section.\\nOur goal is to show the accuracy of the delta method and the bootstrap. In particular, we\\ndevelop new results on the delta method for multiple non-linear statistics with increasing\\ndimension. The state-of-the-art for delta method results are the papers by Pinelis and Molzon\\n(2013); Chen and Shao (2007) where, in particular, the former applies to the multivariate\\ncase. Rather than adapt those results, we instead develop a slightly di\\ufb00erent approach that\\nleverages recent developments in high dimensional statistics. This allows us to develop a\\nsimultaneous delta method and bootstrap for multiple inference with increasing dimension.\\nThroughout this section, we assume that Dn < n.\\n6.1. Preliminary De\\ufb01nitions and Results.\\nRecall that s = vec(S), \\u03c3 = vec(\\u03c3), \\u03c9 = vec(\\u2126),\\n\\u03b8 = vec(\\u0398) and \\u03b4 = \\u221an(s \\u2212\\u03c3). De\\ufb01ne the map gj by \\u03b8j = gj(\\u03c3). We can write \\u03b8 = G(\\u03c3)\\nwhere G(\\u03c3) = (g1(\\u03c3), . . . , gD2(\\u03c3))T. Note that G : RD2 \\u2192RD2.\\nIf D is \\ufb01xed, the central limit theorem implies that\\n(22)\\n\\u221an(s \\u2212\\u03c3) \\u21ddN(0, T)\\nwhere\\n(23)\\nT \\u2261T(\\u03c3) = E(\\u03f5\\u03f5T \\u2297\\u03f5\\u03f5T) \\u2212\\u03c3\\u03c3T\\nand \\u03f5 \\u223cN(0, \\u03a3). The \\ufb01nite sample variance matrix of \\u03b4 is given by (Boik and Haaland\\n(2006)),\\n(24)\\nTn(\\u03c3) =\\nc1\\nn \\u22121(E(\\u03f5\\u03f5T \\u2297\\u03f5\\u03f5T) \\u2212\\u03c3\\u03c3T) +\\n \\n1 \\u2212D\\n\\u00001 \\u22121\\nn\\n\\u0001\\nn \\u22121\\n!\\n(ID2 \\u2212K(D,D))(\\u03a3 \\u2297\\u03a3)\\nwhere K(D,D) is the commutation matrix de\\ufb01ned in (6) and c1 = D\\n\\u00001 \\u22121\\nn\\n\\u0001\\n.\\nLet eS = n\\u22121 Pn\\ni=1(Yi \\u2212\\u00b5)(Yi \\u2212\\u00b5)T, es = vec(eS), Q = (Y \\u2212\\u00b5)(Y \\u2212\\u00b5)T and q = vec(Q). Note\\nthat\\n(25)\\ns \\u2212\\u03c3 = es \\u2212\\u03c3 \\u2212q = V \\u2212q\\nwhere V = n\\u22121 P\\ni Vi and Vi = vec((Yi \\u2212\\u00b5)(Yi \\u2212\\u00b5)T) \\u2212\\u03c3.\\n11\\n\\nLemma 7 For all \\u03f5 > 0 we have the following inequalities:\\nP(||s \\u2212\\u03c3||\\u221e> \\u03f5) \\u22642D2e\\u2212n\\u03b62\\u03f52/2\\nP(||s \\u2212\\u03c3|| > \\u03f5) \\u22642D2e\\u2212n\\u03b62\\u03f52/(2D2)\\nE||\\u03b4||\\u221e\\u2264\\u03b6\\np\\n2 log(2D2)\\nP(||q||\\u221e> \\u03f5) \\u22644D2e\\u2212n\\u03f5\\u03b62/2.\\nProof. Using the sub-Gaussian property, we have\\nP(||s \\u2212\\u03c3||\\u221e> \\u03f5) = P(||V ||\\u221e> \\u03f5) \\u2264\\nX\\nj\\nP(|V j| > \\u03f5) \\u22642\\nX\\nj\\ne\\u2212n\\u03b62\\u03f52/2 = 2D2e\\u2212n\\u03b62\\u03f52/2.\\nThe second result follows from the \\ufb01rst since ||s \\u2212\\u03c3|| \\u2264D||s \\u2212\\u03c3||\\u221e. The third inequality\\nfollows from a standard inequality; see Lemma 2.2 of Devroye and Lugosi (2001) for example.\\nFor the fourth inequality, note that the absolute value |qj| of each element of q has the form\\n|Y (s) \\u2212\\u00b5(s)| |Y (t) \\u2212\\u00b5(t)|. So P(||q||\\u221e> \\u03f5) \\u2264P\\nj P(|qj| > \\u03f5) \\u22644D2e\\u2212n\\u03f5\\u03b62/2. \\u25a1\\nLemma 8 Let Z \\u223cN(0, 1). Then, for every \\u03f5 > 0,\\nsup\\nz |P(An + Bn < z) \\u2212\\u03a6(z)| \\u2264sup\\nz |P(An < z) \\u2212\\u03a6(z)| + \\u03f5 + P(|Bn| > \\u03f5).\\nProof. Let E =\\nn\\n|Bn| < \\u03f5\\no\\n. Then\\nP(An + Bn < z) \\u2212\\u03a6(z) = P(An + Bn < z, E) + P(An + Bn < z, Ec) \\u2212\\u03a6(z)\\n\\u2264P(An < z + \\u03f5) + P(Ec) \\u2212\\u03a6(z)\\n\\u2264P(An < z + \\u03f5) \\u2212\\u03a6(z + \\u03f5) \\u2212\\u03a6(z) + \\u03a6(z + \\u03f5) + P(|Bn| > \\u03f5)\\n\\u2264P(An < z + \\u03f5) \\u2212\\u03a6(z + \\u03f5) + \\u03f5 + P(|Bn| > \\u03f5).\\nHence,\\nsup\\nz [P(An + Bn < z) \\u2212\\u03a6(z)] \\u2264sup\\nz [P(An < z + \\u03f5) \\u2212\\u03a6(z + \\u03f5)] + \\u03f5 + P(|Bn| > \\u03f5)\\n= sup\\nz [P(An < z) \\u2212\\u03a6(z)] + \\u03f5 + P(|Bn| > \\u03f5).\\nBy a similar argument,\\nsup\\nz [P(An + Bn < z) \\u2212\\u03a6(z)] \\u2265sup\\nz [P(An < z) \\u2212\\u03a6(z)] \\u2212\\u03f5 \\u2212P(|Bn| > \\u03f5).\\n\\u25a1\\nWe need the following recent results on high-dimensional random vectors.\\n12\\n\\nTheorem 9 (High-Dimensional CLT; Chernozhukov, Chetverikov and Kato 2012)\\nLet Y1, . . . , Yn \\u2208Rk be random vectors with mean \\u00b5 and covariance \\u03a3. Let\\nT = max\\nj\\n\\f\\f\\f\\f\\f\\n1\\n\\u221an\\nn\\nX\\ni=1\\n(Yi(j) \\u2212\\u00b5(j))\\n\\f\\f\\f\\f\\f.\\nLet Z \\u2208RD be Gaussian with mean 0 and covariance \\u03a3. Then\\n(26)\\nsup\\nz\\n\\f\\f\\f\\f\\fP(T \\u2264z) \\u2212P(max\\nj\\n|Zj| \\u2264z)\\n\\f\\f\\f\\f\\f \\u2aafM (log D)7/8\\nn1/8\\nwhere M = (E maxj[|Y (j)| + |Z(j)|]3)1/4. Under the sub-Gaussian assumption, M \\u2aaf(log D)1/8.\\nHence the upper bound is log D/n1/8.\\nTheorem 10 (Gaussian Anti-Concentration; Chernozhukov, Chetverikov and Kato 2013)\\nLet Z1, . . . , Zk be centered, not necessarily independent, Gaussian random variables. Then\\n(27)\\nsup\\nz P\\n \\n| max\\nj\\nZj \\u2212z| \\u2264\\u03f5\\n!\\n\\u2264C\\u03f5\\np\\nlog(k/\\u03f5)\\nwhere C depends only on maxj Var(Zj) and minj Var(Zj).\\nAn immediate corollary of this result is the following.\\nLemma 11 Let Z \\u223cN(0, \\u03a3). There exists c > 0 depending only on maxj \\u03a3jj and minj \\u03a3jj\\nbut not on k such that, for every \\u03f5 > 0,\\nsup\\nt\\n\\\"\\nP\\n\\u0010\\nmax\\nj\\n|Zj| \\u2264t + \\u03f5\\n\\u0011\\n\\u2212P\\n\\u0010\\nmax\\nj\\n|Zj| \\u2264t\\n\\u0011#\\n\\u2264c\\u03f5\\np\\nlog(k/\\u03f5)\\nand\\nsup\\nt [P(max\\nj\\nZj \\u2264t + \\u03f5) \\u2212P(max\\nj\\nZj \\u2264t)] \\u2264c\\u03f5\\np\\nlog(k/\\u03f5).\\nProof. Let Y = maxj Zj. Then\\nP\\n\\u0010\\nmax\\nj\\nZj \\u2264t + \\u03f5\\n\\u0011\\n\\u2212P\\n\\u0010\\nmax\\nj\\nZj \\u2264t\\n\\u0011\\n\\u2264P(t \\u2212\\u03f5 \\u2264Y \\u2264t + \\u03f5)\\n= P(\\u2212\\u03f5 \\u2264Y \\u2212t \\u2264\\u03f5)\\n\\u2264P(|Y \\u2212t| \\u2264\\u03f5)\\n\\u22642 sup\\nz P(|Y \\u2212z| \\u2264\\u03f5) \\u2264c\\u03f5\\np\\nlog(k/\\u03f5)\\nwhere the last inequality is precisely the previous anti-concentration inequality. \\u25a1\\n13\\n\\nRemark 12 A union bound would have given a bound of order k\\u03f5 instead of \\u03f5\\np\\nlog k/\\u03f5.\\nLemma 11 leads to much sharper bounds in our delta method and bootstrap bounds.\\nTheorem 13 (Gaussian Comparison; Chernozhukov, Chetverikov and Kato 2013)\\nLet X = (X1, . . . , Xk) \\u223cN(0, \\u03a3X) and Y = (Y1, . . . , Yk) \\u223cN(0, \\u03a3Y ). Let \\u2206= maxj,k |\\u03a3X(j, k)\\u2212\\n\\u03a3Y (j, k). Then\\n(28)\\nsup\\nz\\n\\f\\f\\fP(max\\nj\\nXj \\u2264z) \\u2212P(max\\nj\\nYj \\u2264z)\\n\\f\\f\\f \\u2264C\\u22061/3(1 \\u2228log(k/\\u2206))2/3\\nwhere C is only a function of maxj \\u03a3Y (j, j) and minj \\u03a3Y (j, j).\\n6.2. Berry-Esseen Bounds for High-Dimensional Delta Method.\\nDe\\ufb01ne\\n(29)\\nB =\\nn\\na : ||a \\u2212\\u03c3|| \\u2264C\\np\\nD2 log n/n\\no\\n.\\nIt follows from Lemma 7 that, for large enough C, P(s /\\u2208B) \\u22641/n2. We assume throughout\\nthe analysis that s \\u2208B as the error this incurs is of smaller order than the rest of the error\\nterms. Let \\u0398 and b\\u0398 be the matrix of partial correlations and the matrix of estimate partial\\ncorrelations. Let \\u03b8 = vec(\\u0398) and b\\u03b8 = vec(b\\u0398). Recall that\\n\\u03b8 = (\\u03b81, . . . , \\u03b8D2)T = G(\\u03c3) = (g1(\\u03c3), . . . , gD2(\\u03c3)).\\nBy Taylor expansion and (25),\\n(30)\\n\\u221an(b\\u03b8 \\u2212\\u03b8) = \\u221anL(s \\u2212\\u03c3) + n\\u22121/2R = \\u221anLV \\u2212\\u221anLq + 1\\n\\u221anR\\nwhere L = dvec(G)/d\\u03c3T so that L is the D2 \\u00d7 D2 matrix whose jth row is \\u2113j \\u2261dgj(\\u03c3)/d\\u03c3T.\\nSimilarly, R = (R1, . . . , RD2)T where Rj = 1\\n2\\u03b4THj\\u03b4 and Hj is the Hessian of gj, evaluated at\\nsome point between s and \\u03c3. Let\\n(31)\\n\\u0393 = Var(\\u221anL(s \\u2212\\u03c3)) = LTnLT\\nand\\n\\u03b3 = diag(\\u0393).\\nLet\\nZ = \\u221an\\u03b3\\u22121/2(b\\u03b8 \\u2212\\u03b8) = (Z1, . . . , ZD2)T\\nwhere Zj = \\u221an(b\\u03b8j\\u2212\\u03b8j)/ej is the normalized estimate and ej = \\u03b31/2(j, j) =\\np\\n\\u2113j(\\u03c3)TT(\\u03c3)\\u2113j(\\u03c3).\\nThe covariance of Z is\\ne\\u0393 = \\u03b3\\u22121/2\\u0393\\u03b3\\u22121/2.\\nNote that e\\u0393jj = 1 for all j.\\nTheorem 14 Let W \\u223cN(0, e\\u0393) where W \\u2208RD2 and let\\n\\u03b3n = max\\nj\\nsup\\na\\u2208B\\n|||Hj(a)|||\\np\\n\\u2113j(a)TTn(a)\\u2113j(a)\\nand\\n\\u03ben = max\\nj\\nsup\\na\\u2208B\\n||\\u03b3\\u22121/2\\u2113j(a)||1.\\n14\\n\\nThen,\\n(32)\\nsup\\nz\\n\\f\\f\\fP(max\\nj\\n|Zj| \\u2264z) \\u2212P(max\\nj\\n|Wj| \\u2264z)\\n\\f\\f\\f \\u2aafAn\\nwhere\\n(33)\\nAn = log D\\nn1/8 + 4(\\u03b3n + \\u03ben)\\n\\u03b62\\nr\\nlog(Dn)\\nn\\ns\\nlog\\n\\u0012\\nD\\u03b62\\n4(\\u03b3n + \\u03ben)\\nr\\nn\\nlog(Dn)\\n\\u0013\\n.\\nHence, if z\\u03b1 \\u2261\\u2212\\u03a6\\u22121(\\u03b1/D2) then\\nP(max\\nj\\n|Zj| > z\\u03b1) \\u2264\\u03b1 + An.\\nRemark 15 In the above result, the dimension enters mainly through the terms \\u03b3n and \\u03ben.\\nExcept for these terms, the dependence on D is only logarithmic. We discuss these terms in\\nSection 6.5.\\nProof. By (30),\\nZ = \\u221an\\u03b3\\u22121/2(b\\u03b8 \\u2212\\u03b8) = \\u221an\\u03b3\\u22121/2LV \\u2212\\u221an\\u03b3\\u22121/2Lq + 1\\n\\u221an\\u03b3\\u22121/2R.\\nNote that Var(Wi) = Var(\\u221an\\u03b3\\u22121/2LV ). Fix \\u03f5 > 0 and let\\nE =\\n(\\f\\f\\f\\n\\f\\f\\f\\u03b3\\u22121/2R\\n\\u221an\\n\\f\\f\\f\\n\\f\\f\\f\\n\\u221e\\u2264\\u03f5\\n)\\nand\\nE\\u2032 =\\n(\\n||\\u221an\\u03b3\\u22121/2Lq||\\u221e\\u2264\\u03f5\\n)\\n.\\nNow\\nP(max\\nj\\n|Zj| \\u2264z) = P(||\\u221an\\u03b3\\u22121/2LV \\u2212\\u221an\\u03b3\\u22121/2Lq + 1\\n\\u221an\\u03b3\\u22121/2R||\\u221e\\u2264z)\\n\\u2264P(||\\u221an\\u03b3\\u22121/2LV ||\\u221e\\u2212||\\u221an\\u03b3\\u22121/2Lq||\\u221e\\u2212|| 1\\n\\u221an\\u03b3\\u22121/2R||\\u221e\\u2264z)\\n= P(||\\u221an\\u03b3\\u22121/2LV ||\\u221e\\u2212||\\u221an\\u03b3\\u22121/2Lq||\\u221e\\u2212|| 1\\n\\u221an\\u03b3\\u22121/2R||\\u221e\\u2264z, E)\\n+ P(||\\u221an\\u03b3\\u22121/2LV ||\\u221e\\u2212||\\u221an\\u03b3\\u22121/2Lq||\\u221e\\u2212|| 1\\n\\u221an\\u03b3\\u22121/2R||\\u221e\\u2264z, Ec)\\n\\u2264P(||\\u221an\\u03b3\\u22121/2LV ||\\u221e\\u2212||\\u221an\\u03b3\\u22121/2Lq||\\u221e\\u2264z + \\u03f5) + P(Ec)\\n= P(||\\u221an\\u03b3\\u22121/2LV ||\\u221e\\u2212||\\u221an\\u03b3\\u22121/2Lq||\\u221e\\u2264z + \\u03f5, E\\u2032)\\n+ P(||\\u221an\\u03b3\\u22121/2LV ||\\u221e\\u2212||\\u221an\\u03b3\\u22121/2Lq||\\u221e\\u2264z + \\u03f5, (E\\u2032)c) + P(Ec)\\n\\u2264P(||\\u221an\\u03b3\\u22121/2LV ||\\u221e\\u2264z + 2\\u03f5) + P(Ec) + P((E\\u2032)c).\\n15\\n\\nSo,\\nP(max\\nj\\n|Zj| \\u2264z) \\u2212P(max\\nj\\n|Wj| \\u2264z)\\n\\u2264P\\n \\f\\f\\f\\f\\f\\n\\f\\f\\f\\f\\f\\n\\u221an\\u03b3\\u22121/2LV\\n\\f\\f\\f\\f\\f\\n\\f\\f\\f\\f\\f\\n\\u221e\\n\\u2264z + 2\\u03f5\\n!\\n\\u2212P(max\\nj\\n|Wj| \\u2264z + 2\\u03f5)\\n+ P(max\\nj\\n|Wj| \\u2264z + 2\\u03f5) \\u2212P(max\\nj\\n|Wj| \\u2264z) + P(Ec) + P((E\\u2032)c)\\n\\u2264P\\n \\f\\f\\f\\f\\f\\n\\f\\f\\f\\f\\f\\n\\u221an\\u03b3\\u22121/2LV\\n\\f\\f\\f\\f\\f\\n\\f\\f\\f\\f\\f\\n\\u221e\\n\\u2264z + 2\\u03f5\\n!\\n\\u2212P(max\\nj\\n|Wj| \\u2264z + 2\\u03f5) + C\\u03f5\\np\\nlog D/\\u03f5 + P(Ec) + P((E\\u2032)c)\\n\\u2264C log D\\nn1/8 + C\\u03f5\\np\\nlog D/\\u03f5 + P(Ec) + P((E\\u2032)c)\\nwhere we used Theorem 9 applied to V\\n\\u2217= \\u03b3\\u22121/2LV and Lemma 11. Recall that s \\u2208B\\nexcept on a set of probability 1/n2 and on this set,\\n\\u0012\\u03b3\\u22121/2R\\n\\u221an\\n\\u0013\\nj\\n=\\n\\u03b4THj\\u03b4\\nq\\nn\\u2113T\\nj Tn\\u2113j\\n\\u2264\\u03b3n\\n\\u221an||s \\u2212\\u03c3||2\\n\\u221e\\nand so by Lemma 7,\\nP(Ec) \\u22642D2 exp\\n\\u0012\\n\\u2212\\u2212\\u221an\\u03b62\\u03f52\\n2\\u03b32\\nn\\n\\u0013\\n.\\nChoosing\\n\\u03f5 = 4(\\u03b3n + \\u03ben)\\n\\u03b62\\nr\\nlog(Dn)\\nn\\nwe have P(Ec) \\u2264\\n1\\nn2 and\\n\\u03f5\\np\\nlog D/\\u03f5 \\u22644(\\u03b3n + \\u03ben)\\n\\u03b62\\nr\\nlog(Dn)\\nn\\ns\\nlog\\n\\u0012\\nD\\u03b62\\n4(\\u03b3n + \\u03ben)\\nr\\nn\\nlog(Dn)\\n\\u0013\\n.\\nUsing Holder\\u2019s inequality,\\n|\\u03b3\\u22121/2\\u2113T\\nj q| \\u2264||q||\\u221e||\\u03b3\\u22121/2\\u2113j||1 \\u2264||q||\\u221e\\u03ben\\nso that ||\\u03b3\\u22121/2Lq||\\u221e\\u2264||q||\\u221e\\u03ben. Hence, using Lemma (7),\\nP((E\\u2032)c) \\u2264P(||q||\\u221e> \\u03f5/\\np\\n\\u03benn) \\u22644D2e\\u2212\\u221an\\u03b62\\u03f5/(2\\u03ben) \\u22641\\nn2\\nThe result follows by computing a similar lower bound and taking the supremum over z. For\\nthe last statement, note that Wj \\u223cN(0, 1). So\\nP(max\\nj\\n|Zj| > z\\u03b1) \\u2264P(max\\nj\\n|Wj| > z\\u03b1) + An \\u2264\\nX\\nj\\nP(|Wj| > z\\u03b1) + An \\u2264\\u03b1 + An.\\n\\u25a1\\nIn practice we need to use Tj = \\u221an(b\\u03b8j \\u2212\\u03b8j)/bej where bej =\\np\\n\\u2113j(s)TT(s)\\u2113j(s) \\u2261Uj(s) is the\\nestimated standard error. We have the following result for this case.\\n16\\n\\nTheorem 16 De\\ufb01ne \\u03b3n and \\u03ben as in the previous theorem. Let\\n\\u03c1n = max\\nj\\nsup\\na\\u2208B\\n||U \\u2032\\nj(a)||1\\np\\n\\u2113j(a)TTn(a)\\u2113j(a)\\nwhere Uj(a) =\\nq\\n\\u2113T\\nj (a)T(a)\\u2113j(a). Then,\\nsup\\nz |P(max\\nj\\n|Tj| \\u2264z) \\u2212P(max\\nj\\n|Wj| \\u2264z)| \\u2aafAn + \\u03c1n\\nr\\nlog n\\nn\\nwhere An is de\\ufb01ned in (33). If z \\u2261\\u2212\\u03a6\\u22121(\\u03b1/D2) then\\nsup\\nz |P(max\\nj\\n|Tj| > z)| \\u2264\\u03b1 + An + \\u03c1n\\nr\\nlog n\\nn\\n.\\nProof. Let E = {maxj ej/bej < 1+\\u03f5} and F = {max Zj < u/\\u03f5} where \\u03f5 = (4\\u03c1n/\\u03b6)\\np\\nlog n/(n\\u03b62)\\nand u = \\u03f5\\np\\nlog(n). Note that ej \\u2212bej = Uj(\\u03c3) \\u2212Uj(s) = (\\u03c3 \\u2212s)TU \\u2032\\nj where U \\u2032 is the gradient\\nof U evaluated at some point between s and \\u03c3. Then, for 0 < \\u03f5 \\u22641,\\nP(Ec) \\u2264P\\n\\u0012\\nmax\\nj\\nej \\u2212bej\\nej\\n>\\n\\u03f5\\n1 + \\u03f5\\n\\u0013\\n= P\\n\\u0012\\nmax\\nj\\nUj(\\u03c3) \\u2212Uj(s)\\nej\\n>\\n\\u03f5\\n1 + \\u03f5\\n\\u0013\\n= P\\n \\nmax\\nj\\n(\\u03c3 \\u2212s)TU \\u2032\\nj\\nej\\n>\\n\\u03f5\\n1 + \\u03f5\\n!\\n\\u2264P\\n\\u0012||s \\u2212\\u03c3||\\u221emaxj ||U \\u2032\\nj||1\\nej\\n>\\n\\u03f5\\n1 + \\u03f5\\n\\u0013\\n\\u2264P\\n\\u0012\\n||s \\u2212\\u03c3||\\u221e\\u03c1n >\\n\\u03f5\\n1 + \\u03f5\\n\\u0013\\n= P\\n\\u0012\\n||s \\u2212\\u03c3||\\u221e>\\n\\u03f5\\n2\\u03c1n\\n\\u0013\\n\\u2264D2e\\u2212n\\u03f52/(2\\u03c12\\nn) \\u22641\\nn2.\\n17\\n\\nNow,\\nP\\n \\nmax\\nj\\n\\u221an(b\\u03b8j \\u2212\\u03b8j)\\nbej\\n\\u2264z\\n!\\n\\u2212P(max Wj \\u2264z)\\n= P\\n\\u0012\\nmax\\nj\\nZj\\n\\u0012ej\\nbej\\n\\u0013\\n\\u2264z\\n\\u0013\\n\\u2212P(max Wj \\u2264z)\\n\\u2264P\\n\\u0012\\nmax\\nj\\nZj(1 \\u2212\\u03f5) \\u2264z\\n\\u0013\\n+ P(Ec) \\u2212P(max Wj \\u2264z)\\n= P\\n\\u0012\\nmax\\nj\\nZj \\u2212Zj\\u03f5 \\u2264z\\n\\u0013\\n+ P(Ec) \\u2212P(max Wj \\u2264z)\\n\\u2264P\\n\\u0012\\nmax\\nj\\nZj \\u2264z + u\\n\\u0013\\n+ P(F c) + P(Ec) \\u2212P(max Wj \\u2264z)\\n\\u2264P\\n\\u0012\\nmax\\nj\\nZj \\u2264z + u\\n\\u0013\\n\\u2212P(max Wj \\u2264z + u)\\n+ Cu\\np\\nlog D/u + P(F c) + P(Ec)\\n\\u2264sup\\nz\\n\\\"\\nP\\n\\u0012\\nmax\\nj\\nZj \\u2264z\\n\\u0013\\n\\u2212P(max\\nj\\nWj \\u2264z)\\n#\\n+ Cu\\np\\nlog D/u + P(F c) + P(Ec)\\n\\u2264An + Cu\\np\\nlog D/u + P(F c) + P(Ec)\\nwhere An is de\\ufb01ned in (33). Next,\\nP(F c) = P(max\\nj\\nZj > u/\\u03f5) \\u2264P(max\\nj\\nWj > u/\\u03f5) + An\\n= P(max\\nj\\nWj >\\np\\nlog n) + An\\n\\u2264E(maxj Wj)\\n\\u221alog n\\n+ An \\u2aaf\\n\\u221alog D\\n\\u221alog n + An \\u2aafAn.\\nSo\\nsup\\nz |P(max\\nj\\n|Tj| \\u2264z) \\u2212P(max\\nj\\n|Wj| \\u2264z)| \\u2aafsup\\nz [P\\n\\u0012\\nmax\\nj\\nZj \\u2264z\\n\\u0013\\n\\u2212P(max Wj \\u2264z)] + An\\n+ 1\\nn2 + Cu\\np\\nlog D\\n\\u2aafAn + \\u03c1n\\nr\\nlog n\\nn\\n.\\nA similar lower bound completes the proof. \\u25a1\\n6.3. The Bootstrap.\\nIn this section we assume that maxj |Y (j)| \\u2264B for some B < \\u221e.\\nThis is not necessary but it simpli\\ufb01es the proofs. We do not require that B be known.\\n18\\n\\nLet Y \\u2217\\n1 , . . . , Y \\u2217\\nn be a sample from the empirical distribution and let s\\u2217be the correspond-\\ning (vectorized) sample covariance. Now let b\\u03b8\\u2217be the partial correlations computed from\\nY \\u2217\\n1 , . . . , Y \\u2217\\nn \\u223cPn where Pn is the empirical distribution. The (un-normalized) bootstrap\\nrectangle for \\u03b8 is\\nRn =\\n(\\n\\u03b8 : ||\\u03b8 \\u2212b\\u03b8||\\u221e\\u2264Z\\u03b1\\n\\u221an\\n)\\nwhere Z\\u03b1 = bF \\u22121(1 \\u2212\\u03b1) and\\n(34)\\nbF(z) = P\\n\\u0012\\u221an||b\\u03b8\\u2217\\u2212b\\u03b8||\\u221e\\u2264z\\n\\f\\f\\f\\f Y1, . . . , Yn\\n\\u0013\\nis the bootstrap approximation to\\nF(z) = P(\\u221an||b\\u03b8 \\u2212\\u03b8||\\u221e\\u2264z).\\nThe accuracy of the coverage of the bootstrap rectangle depends on supz | bF(z) \\u2212F(z)|.\\nLet\\n\\u0393 = Var(\\u221anL(s \\u2212\\u03c3)) = LTnLT.\\nLet Z \\u223cN(0, \\u0393) where Z \\u2208RD2. First we need the following limit theorem for the un-\\nnormalized statistics.\\nTheorem 17 De\\ufb01ne \\u03b3\\u2032\\nn = maxj supa\\u2208B |||Hj(a)||| and \\u03be\\u2032\\nn = maxj supa\\u2208B ||\\u2113j(a)||1. Then\\nsup\\nz\\n\\f\\f\\f\\f\\fP(\\u221an||b\\u03b8 \\u2212\\u03b8||\\u221e\\u2264z) \\u2212P(||Z||\\u221e\\u2264z)\\n\\f\\f\\f\\f\\f \\u2aaflog D\\nn1/8 + A\\u2032\\nn\\nwhere\\n(35)\\nA\\u2032\\nn = log D\\nn1/8 + 4(\\u03b3\\u2032\\nn + \\u03be\\u2032\\nn)\\n\\u03b62\\nr\\nlog(Dn)\\nn\\ns\\nlog\\n\\u0012\\nD\\u03b62\\n4(\\u03b3\\u2032\\nn + \\u03be\\u2032\\nn)\\nr\\nn\\nlog(Dn)\\n\\u0013\\n.\\nProof. The proof is the same as the proof of Theorem 14 with \\u03b3\\u2032\\nn and \\u03be\\u2032\\nn replacing \\u03b3n and\\n\\u03ben. \\u25a1\\nNow we bound supz | bF(z) \\u2212F(z)|.\\nTheorem 18\\nsup\\nz | bF(z) \\u2212F(z)| \\u2aaflog D\\nn1/8 + (\\u03b3\\u2032\\nn + \\u03be\\u2032\\nn)\\np\\nlog n/n + OP\\n \\u0012log D\\nn\\n\\u00131/6!\\nand hence\\nP(\\u03b8 /\\u2208R) \\u2264\\u03b1 + log D\\nn1/8 + (\\u03b3\\u2032\\nn + \\u03be\\u2032\\nn)\\np\\nlog n/n + O\\n \\u0012log D\\nn\\n\\u00131/6!\\n.\\n19\\n\\nProof. Let Z \\u223cN(0, \\u0393) and let Z\\u2032 \\u223cN(0, \\u0393n) where \\u0393n = Var(\\u221anL(s\\u2217\\u2212s)|Y1, . . . , Yn).\\nThen\\nsup\\nz | bF(z) \\u2212F(z)| \\u2264sup\\nz\\n\\f\\f\\fF(z) \\u2212P(||Z||\\u221e\\u2264z)\\n\\f\\f\\f + sup\\nz\\n\\f\\f\\f bF(z) \\u2212P(||Z\\u2032||\\u221e\\u2264z)\\n\\f\\f\\f\\n+ sup\\nz\\n\\f\\f\\fP(||Z\\u2032||\\u221e\\u2264z) \\u2212P(||Z||\\u221e\\u2264z)\\n\\f\\f\\f\\n= I\\n+\\nII\\n+\\nIII.\\nIn the previous theorem, we showed that I \\u2264log D\\nn1/8 + A\\u2032\\nn. For II, we proceed exactly as in\\nthe proof for of the previous theorem but with Pn replacing P (and with Y1, . . . , Yn \\ufb01xed).\\nThis yields, for any \\u03f5 > 0,\\nbF(z) \\u2212P(||Z\\u2032||\\u221e\\u2264z) \\u2aaflog D\\nn1/8 + \\u03f5\\np\\nlog D/\\u03f5\\n+ P(\\u221an||Lq\\u2217||\\u221e> \\u03f5|Y1, . . . , Yn) + P(n\\u22121/2||R\\u2217||\\u221e> \\u03f5|Y1, . . . , Yn)\\nwhere q\\u2217= vec((Y\\n\\u2217\\u2212Y )(Y\\n\\u2217\\u2212Y )T), R\\u2217\\nj = (1/2)\\u03b4TH\\u2217\\nj \\u03b4\\u2217, \\u03b4\\u2217= \\u221an(s\\u2217\\u2212s) and H\\u2217\\nj is the\\nHessian of gj evaluated at a point between s and s\\u2217.\\nSince all the Yi\\u2019s are contained in the bounded rectangle B \\u00d7 \\u00b7 \\u00b7 \\u00b7 \\u00d7 B, it follows that under\\nthe empirical measure Pn, Y \\u2217\\ni is sub-Gaussian with \\u03b6 = B. It then follows that s\\u2217\\u2208B expect\\non a set of probability at most 1/n. Choosing\\n\\u03f5 = 4(\\u03b3\\u2032\\nn + \\u03be\\u2032\\nn)\\nB2\\nr\\nlog(Dn)\\nn\\nand arguing as in the proof of Theorem 14 we conclude that\\nbF(z) \\u2212P(||Z\\u2032||\\u221e\\u2264z) \\u2aaflog D\\nn1/8 + \\u03f5\\np\\nlog D/\\u03f5\\n+ P(\\u221an||Lq\\u2217||\\u221e> \\u03f5|Y1, . . . , Yn) + P(n\\u22121/2||R\\u2217||\\u221e> \\u03f5|Y1, . . . , Yn)\\n\\u2264log D\\nn1/8 + OP(A\\u2032\\nn).\\nFor III, we use Theorem 13 which implies that\\nIII \\u2264C \\u22061/3(1 \\u2228log(k/\\u2206))2/3\\nwhere \\u2206= maxs,t |\\u0393(s, t)\\u2212\\u0393n(s, t)|. Each element of \\u0393n(s, t) is a sample moment and \\u0393(s, t)\\nis corresponding population moment, and so, since Pn is sub-Gaussian, \\u2206= OP(\\np\\nlog D/n).\\nHence, III = OP\\n\\u0000 log D\\nn\\n\\u00011/6 . \\u25a1\\n6.4. A Super-Accurate Bootstrap.\\nNow we describe a modi\\ufb01ed approach to the boot-\\nstrap that has coverage error only O(log D/n1/8) which is much more accurate than the\\n20\\n\\nusual bootstrap as described in the last section. The idea is very simple. Let R be the 1 \\u2212\\u03b1\\nbootstrap con\\ufb01dence rectangle for \\u03c3 described in Section 7.1. Write \\u03b8 = G(\\u03c3) and de\\ufb01ne\\nT =\\nn\\nG(\\u03c3) : \\u03c3 \\u2208R\\no\\n.\\nBy construction, T inherits the coverage properties of R and so we have immediately:\\nCorollary 19\\nP(\\u03b8 \\u2208T ) \\u22651 \\u2212\\u03b1 \\u2212O\\n\\u0012log D\\nn1/8\\n\\u0013\\n\\u2212O\\n\\u0012log D\\nn\\n\\u00131/6\\n.\\nThe set T then de\\ufb01nes con\\ufb01dence sets for each \\u03b8j, namely,\\nCj =\\nh\\ninf{gj(\\u03c3) : \\u03c3 \\u2208R},\\nsup{gj(\\u03c3) : \\u03c3 \\u2208R}\\ni\\n.\\nWe should stress that, in general, obtaining a con\\ufb01dence set by mapping a con\\ufb01dence rect-\\nangle can lead to wide intervals. However, our foremost concern in this paper is coverage\\naccuracy.\\nConstructing the set T can be di\\ufb03cult. But it is easy to get an approximation. We draw a\\nlarge sample \\u03c31, . . . , \\u03c3N from a uniform distribution on the rectangle R. Now let\\n\\u03b8j = min\\n1\\u2264s\\u2264N gj(\\u03c3s),\\n\\u03b8j = max\\n1\\u2264s\\u2264N gj(\\u03c3s).\\nThen [\\u03b8j, \\u03b8j] approximates the con\\ufb01dence interval for \\u03b8j. Alternatively, we take \\u03c31, . . . , \\u03c3N to\\nbe the bootstrap replications that are contained in R. Note that there is no need for a multiple\\ncomparison correction as the original con\\ufb01dence rectangle is a simultaneous con\\ufb01dence set.\\n6.5. Comments on the Error Terms.\\nThe accuracy of the delta method depends on the\\ndimension D mainly through the terms \\u03b3n, \\u03ben and \\u03c1n. Similarly, the accuracy of the (\\ufb01rst\\nversion of the) bootstrap depends on \\u03b3\\u2032\\nn and \\u03be\\u2032\\nn. In this section we look at the size of these\\nterms. We focus on \\u03b3\\u2032\\nn and \\u03be\\u2032\\nn.\\nRecall that \\u2113j = d\\u03b8j/d\\u03c3T. Then\\n\\u2113j(\\u03c3) = d\\u03b8j\\nd\\u03c3T = d\\u03b8j\\nd\\u03c9T\\nd\\u03c9\\nd\\u03c3T .\\nLet (s, t) be such that \\u03b8j = \\u0398st. Then,\\nd\\u03b8j\\nd\\u03c9T is 1\\u00d7D2 and\\nd\\u03c9\\nd\\u03c3T is D2 \\u00d7D2. Now\\nd\\u03c9\\nd\\u03c3T = \\u2212\\u2126\\u2297\\u2126\\nand\\nd\\u03b8j\\nd\\u03c9T is 0 except for three entries, namely,\\nd\\u03b8j\\nd\\u2126ss\\n= \\u2212\\u03b8j\\n2\\u2126ss\\n,\\nd\\u03b8j\\nd\\u2126tt\\n= \\u2212\\u03b8j\\n2\\u2126ss\\n,\\nd\\u03b8j\\nd\\u2126st\\n= \\u03b8j\\n\\u2126st\\n.\\n21\\n\\nDe\\ufb01ne (J, K, M) by \\u03c3J = \\u03a3ss, \\u03c3K = \\u03a3tt and \\u03c3M = \\u03a3st. Then\\n(36)\\n\\u2113j = d\\u03b8j\\nd\\u03c3T =\\n\\u03b8j\\n2\\u2126ss\\n[\\u2126\\u2297\\u2126]J + \\u03b8j\\n\\u2126st\\n[\\u2126\\u2297\\u2126]M + \\u03b8j\\n2\\u2126tt\\n[\\u2126\\u2297\\u2126]K = fj(\\u2126\\u2297\\u2126)\\nwhere [A]j denotes the jth row of A and fj is a sparse vector that is 0 except for three entries.\\nNow the Hessian is Hj =\\n\\u0010\\nd\\u21131\\nd\\u03c3T , . . . ,\\nd\\u2113D2\\nd\\u03c3T\\n\\u0011T\\nwhere\\nd\\u2113j\\nd\\u03c3T = d\\u2113j\\nd\\u03c9T\\nd\\u03c9\\nd\\u03c3T = \\u2212d\\u2113j\\nd\\u03c9T (\\u2126\\u2297\\u2126).\\nNow\\nd\\u2113j\\nd\\u03c9T =\\n \\u0012 d\\u03c9\\nd\\u03c3T\\n\\u0013T\\n\\u2297I\\n!\\nd\\nd\\u03c9T\\n\\u0012d\\u03b8j\\nd\\u03c9\\n\\u0013\\n+\\n\\u0012\\nI \\u2297d\\u03b8j\\nd\\u03c9T\\n\\u0013\\nd\\nd\\u03c9T\\nd\\u03c9\\nd\\u03c3T\\n= \\u2212(\\u2126\\u2297\\u2126\\u2297I)fj \\u2212(I \\u2297fj) d\\nd\\u03c9T (\\u2126\\u2297\\u2126)\\n= \\u2212(\\u2126\\u2297\\u2126\\u2297I)fj \\u2212(I \\u2297fj)(ID \\u2297K(D,D) \\u2297ID)(ID2 \\u2297vec(\\u2126) : vec(\\u2126) \\u2297ID2);\\nwhere we used the fact that\\ndvec(\\u2126\\u2297\\u2126)\\nd\\u03c9T\\n= (ID \\u2297K(D,D) \\u2297ID)(ID2 \\u2297vec(\\u2126) : vec(\\u2126) \\u2297ID2);\\nsee, for example, p 185 of Magnus and Neudecker (1988) Note that ||fj||0 = O(1) independent\\nof D. The presence of this sparse vector helps to prevent the gradient and Hessian from\\ngetting too large.\\nBy direct examination of \\u2113j and Hj we see that the size of \\u03b3\\u2032\\nn and \\u03be\\u2032\\nn depends on how dense\\n\\u2126is. In particular, when \\u2126is diagonally dominant, \\u03b3\\u2032\\nn and \\u03be\\u2032\\nn are both O(1). In this case the\\nerror terms have size O((log Dn)/n1/8). However, if \\u2126is dense, then ||\\u2113j||1 can be of order\\nO(D2) and and |||Hj||| can be of order O(D4). In this case the error can be as large as D4/n1/8.\\nOn the other hand, the bootstrap in Section 6.4 always has accuracy O((log Dn)/n1/8). But\\nthe length of the intervals could be large when \\u2126is dense. And note that even in the\\nfavorable case, we still require Dn < n for the results to hold. (We conjecture that this\\ncan be relaxed by using shrinkage methods as in Sch\\u00a8afer et al. (2005).) These observations\\nmotivate the methods in the next section which avoid direct inferences about the partial\\ncorrelation graph in the high-dimensional case.\\nIt is interesting to compare the size of the errors to other work on inference with increasing\\ndimension. For example, Portnoy (1988) gets accuracy\\np\\nD3/2/n for maximum likelihood es-\\ntimators in exponential families and Mammen (1993) gets accuracy\\np\\nD2/n for the bootstrap\\nfor linear models.\\n6.6. Back To Graphs.\\nFinally, we can use the above methods for estimating a graph\\nwith con\\ufb01dence guarantees. We put an edge between j and k only if 0 is excluded from the\\ncon\\ufb01dence interval for \\u03b8jk. The desired guarantee stated in (1) then holds.\\n22\\n\\n7. The High Dimensional Case.\\nNow we consider the case where Dn > n. We present\\nthree methods for dealing with the high-dimensional case:\\n1. Correlation graphs. This is a common technique in biostatistics. We connect two\\nnodes if the con\\ufb01dence interval for two variables excludes [\\u2212\\u03f5, \\u03f5] for some threshold\\n\\u03f5 \\u2208[0, 1]. Our contribution here is to provide con\\ufb01dence guarantees using the bootstrap\\nthat are valid as long as D = o(en1/7). In this paper we use \\u03f5 = 0.\\n2. Cluster graphs. We cluster the features and average the features within each cluster.\\nAs long as the number of clusters L is o(n) we get valid inferences. Related to cluster\\ngraphs are block graphs. In this case, we again cluster the nodes. But then we make\\nno connections between clusters and we use an undirected graph within clusters.\\n3. Restricted Graphs. De\\ufb01ne the restricted partial correlation\\n\\u03b8jk \\u2261sup\\n|S|\\u2264L\\n|\\u03b8(Yj, Yk|YS)|\\nwhere L is some \\ufb01xed number, \\u03b8(Yj, Yk|YS) is the partial correlation between Yj and Yk\\ngiven the set of variables YS where S varies over all subsets of {1, . . . , D}\\u2212{j, k} of size\\nL These are sometimes called lower-order partial correlations. Now construct a graph\\nbased on the restricted partial correlations. Note that L = 0 is a correlation graph\\nand L = D is a partial correlation graph. (This is similar to the idea in Castelo and\\nRoverato, 2006). The bootstrap leads to valid inferences only requiring D = o(en1/7).\\nRemark 20 Following Sch\\u00a8afer et al. (2005), we could estimate U = (1\\u2212\\u03bb)\\u03a3+\\u03bbT where T\\nis, for example, a diagonal matrix. The graph is constructed from biased partial correlations\\ncorresponding to U \\u22121. When \\u03bb is close to 1, high-dimensional asymptotic con\\ufb01dence intervals\\nhave accurate coverage. Thus we have a bias-validity tradeo\\ufb00. Investigating this tradeo\\ufb00is\\nquite involved and so we will examine this method elsewhere.\\nIn this section we make the following assumptions.\\n(A1) Y and vec(Y Y T) are sub-Gaussian.\\n(A2) maxj E|Vi(j)|3 \\u2264C where Vi = vec[(Yi \\u2212\\u00b5)(Yi \\u2212\\u00b5)T] \\u2212\\u03c3.\\n(A3) Dn = o(en1/7).\\nThe proofs of the results in this section are similar to those in Section 6 but they are easier\\nas the error terms are, by design, not dependent on dimension sensitive quantities like \\u03b3n\\nand \\u03ben. Because of this, we shall only present proof outlines.\\n7.1. Correlation graphs.\\nThe simplest approach to constructing graphs is to use correla-\\ntion or covariances rather than partial correlation. Let \\u03c1jk denoted the correlation between\\n23\\n\\n1. Select a threshold \\u03f5.\\n2. Compute the sample covariance matrix R.\\n3. Construct a 1 \\u2212\\u03b1 bootstrap con\\ufb01dence rectangle R for the correlations.\\n4. Put an edge between nodes j and k if [\\u2212\\u03f5, \\u03f5] is not in the con\\ufb01dence interval for \\u03c1jk.\\nFig 2. The Correlation Graph Algorithm.\\nY (j) and Y (k). The true graph G\\u03f5 connects j and k if |\\u03c1(j, k)| > \\u03f5 where 0 \\u2264\\u03f5 \\u22641 is some\\nuser-speci\\ufb01ed threshold. The algorithm is in Figure 2. Of course, we can use either \\u03c1 or \\u03c3;\\nwe get the same graph from either.\\nTheorem 21 Let rjk denote the sample correlation between Y (j) and Y (k) and let r be the\\nD2 \\u00d7 1 vector of correlations. Similarly, let \\u03c1 be the vector of true correlations. De\\ufb01ne Z\\u03b1 by\\nthe bootstrap equation\\n(37)\\nP\\n\\u0010\\nmax\\njk\\n\\u221an|r\\u2217\\njk \\u2212rjk| > Z\\u03b1\\n\\f\\f\\f Y1, . . . , Yn\\n\\u0011\\n= \\u03b1.\\nLet\\nR =\\nn\\na \\u2208RD2 : ||a \\u2212r||\\u221e\\u2264Z\\u03b1\\n\\u221an\\no\\n.\\nThen\\nP(\\u03c1 \\u2208R) \\u22651 \\u2212\\u03b1 \\u2212O\\n\\u0012log D\\nn1/8\\n\\u0013\\n\\u2212O\\n\\u0012log D\\nn\\n\\u00131/6\\n.\\nWe thus have\\n(38)\\nP( bG\\u03f5 \\u2282G\\u03f5 for all \\u03f5) \\u22651 \\u2212\\u03b1 + log D\\nn1/8 + O\\n\\u0012log D\\nn\\n\\u00131/6\\n.\\nRemark 22 A very re\\ufb01ned Berry-Esseen result for a single correlation was obtained by\\nPinelis and Molzon (2013).\\nProof Outline. The proof is the same as the proof of Theorem 18. However, in this case,\\nit is easy to see that \\u03b3\\u2032\\nn and \\u03be\\u2032\\nn are O(1), independent of the D since the gradient \\u2113j and\\nHessian Hj is a function only of the bivariate distribution of (Y (s), Y (t)) corresponding to\\nthe correlation. \\u25a1\\n7.2. Cluster Graphs and Block Graphs.\\nThe idea here is to partition the features into\\nclusters, average the features within each cluster and then form the graph for the new derived\\nfeatures. If the clusters are su\\ufb03ciently few, then valid inference is possible.\\nThere are many clustering methods. Here we consider choosing a set of representative features\\n\\u2014 or prototypes \\u2014 using the L-centers algorithm, which we describe below. Then we assign\\neach feature to its nearest center. We average the features within each cluster and then \\ufb01nd\\n24\\n\\n1. Choose L = o(n).\\n2. Randomly split the data into two halves D1 and D2.\\n3. Using D1 select L proto-features:\\n(a) Choose a feature j randomly and set S = {j} and C = {1, . . . , D} \\u2212S.\\n(b) Repeat until S has L elements:\\ni. For each j \\u2208C compute the minimum distance dj = mini\\u2208S d(i, j).\\nii. Find j \\u2208C to maximize dj. Move j from C to S.\\n(c) For L clusters by assigning each feature to its closest center.\\n(d) Average the features within each clusters.\\n4. Using D2, construct a con\\ufb01dence graph for the L new features using either the delta method or the bootstrap\\nfrom Section 6.\\n5. (Optional): Construct a correlation graph for the features within each cluster.\\nFig 3. The Cluster Graph Algorithm\\nthe undirected graph of these new L derived features. Let eG be the graph for these new\\nfeatures. We estimate eG using con\\ufb01dence intervals for the partial correlations. Note that the\\ngraph eG as well as the estimated graph bG are both random.\\nTo ensure the validity of the con\\ufb01dence intervals, we use data spitting. We split the data\\nrandomly into two halves. The \\ufb01rst half is used for clustering. The con\\ufb01dence intervals are\\nconstructed from the second half of the data.\\nThe cluster-graph algorithm is described in Figure 3. It is assumed in the algorithm that the\\nnumber of features L = o(n) is speci\\ufb01ed by the user. An improvement is to use a data-driven\\napproach to choosing L. We leave this to future work.\\nThe asymptotic validity of the method follows from the results in Section 6 together with\\nthe data-splitting step. Without the data-splitting step, the proofs in Section 6 would not be\\nvalid since the feature selection process would introduce a bias. The independence introduced\\nby the splitting thus seems critical. Whether it is possible to eliminate the data-splitting is\\nan open problem. Let us state, without proof, the validity assuming the bootstrap is used.\\nA similar result holds for the delta method.\\nTheorem 23 Let \\u03b8 be the vector of k partial correlations for the features selected from the\\n\\ufb01rst half of the data. Let R be the con\\ufb01dence rectangle using the second half of the data.\\nThen\\n(39)\\nP(\\u03b8 /\\u2208R) \\u2264\\u03b1 + (log L)\\nn1/8\\n+ (\\u03b3\\u2032\\nn + \\u03be\\u2032\\nn)\\np\\nlog n/n + O\\n \\u0012log L\\nn\\n\\u00131/6!\\nwhere \\u03b3\\u2032\\nn and \\u03be\\u2032\\nn are functions of the distribution of the selected features.\\nAn alternative is to use block graphs. For block graphs, we \\ufb01rst cluster the nodes. Then we\\nmake no connections between clusters and we use an undirected graph within clusters based\\n25\\n\\non the bootstrap. In this case, it is required that the number of nodes within each block be\\no(n). However, our experiments with block graphs have been disappointing and we do not\\npursue block graphs further.\\nYet another possibility is as follows. For each (j, k) let Zjk be a dimension reduction of the\\nvariables (Y (s) : s \\u0338= j, k). Then we could estimate the partial correaltion of Y (j) and Y (k)\\ngiven Zjk. This would require a separate dimension reduction step for each pair (j, k).\\n7.3. Restricted Partial Correlations.\\nInstead of building a graph from partial corre-\\nlations, we can use a weaker measure of dependence. Motivated by Castelo and Roverato\\n(2006), we de\\ufb01ne\\n(40)\\n\\u03b8jk = sup\\n|S|\\u2264L\\n|\\u03b8(Xi, Xj|XS)|.\\nFor L = 0 we get a correlation graph. For L = D we get back the usual partial correlation\\ngraph. By choosing L = o(n) we get something in between these two cases while still retaining\\nvalidity of the con\\ufb01dence intervals.\\nThe estimate of \\u03b8jk is the sample version\\n(41)\\nb\\u03b8jk = sup\\n|S|\\u2264k\\n|b\\u03b8(Xi, Xj|XS)|.\\nTheorem 24 De\\ufb01ne Z\\u03b1 by the bootstrap equation\\n(42)\\nP\\n\\u0010\\nmax\\njk\\n\\u221an|b\\u03b8\\u2217\\u2212b\\u03b8| > Z\\u03b1\\n\\f\\f\\f Y1, . . . , Yn\\n\\u0011\\n= \\u03b1.\\nLet\\nR =\\nn\\na \\u2208RD2 : ||a \\u2212b\\u03b8||\\u221e\\u2264Z\\u03b1\\n\\u221an\\no\\n.\\nThen\\nP(\\u03b8 \\u2208R) \\u22651 \\u2212\\u03b1 \\u2212O\\n\\u0012(log L)\\nn1/8\\n\\u0013\\n\\u2212O\\n \\u0012log L\\nn\\n\\u00131/6!\\n.\\nThe proof is basically the same as the proof of Theorem 21. We remark, however, that in\\nthis case, L has to be \\ufb01xed and chosen in advance.\\nWe think that the restricted partial correlation idea is very promising but currently we have\\nno e\\ufb03cient way to compute the graph this way. To compute the restricted partial correlation\\nwe would need to do the following: for each pair (j, k) we have to search over the\\n\\u0000D\\u22122\\nL\\n\\u0001\\nsubsets and \\ufb01nd the maximum. This is repeated for all D2 pairs. Then the entire procedure\\nneeds to be bootstrapped. Despite the fact that the method is currently not computationally\\nfeasible, we include it because we believe that it may be possible in the future to \\ufb01nd e\\ufb03cient\\ncomputational approximations.\\n26\\n\\n8. Experiments.\\nIn this section we illustrate the methods with some simple examples.\\nWe consider three models:\\n1. Dense Model: \\u2126jk = a for all j \\u0338= k.\\n2. Markov Chain: Xj = aXj+1 + \\u03f5j.\\n3. Structural Equation Model: Xj = a Pj\\u22121\\ns=1 Xs + \\u03f5j, j = 2, . . . , D.\\nThe purpose of the experiments is to get some intuitive sense of how much information in\\nthe original graph is captured in the dimension reduced graph.\\nIn each case we show results for bootstrap. We stopped when the results became numerically\\nunstable. Then we increased the dimension and switched to the high dimensional methods,\\nnamely, the cluster graphs, the correlation graphs and the restricted graphs. (We do not\\ninclude the block graphs which did not work well.) The results are in Figures 4, 5, 6, 7, 8\\nand 9.\\nThe results for the dense model are good up to D = 50. After that, the cluster graph method\\nis used and it clearly captures the qualitative features of the graph. or the Markov graph,\\nvalidity holds as D increases but the power starts to decrease leading to missing edges. The\\ncluster graph is interesting here as it obviously cannot reconstruct the Markov structure but\\nstill does capture interesting qualitative features of the underlying graph. The SEM model\\nis di\\ufb03cult; it is a complete graph but some edges are harder to detect. The power again falls\\no\\ufb00as D increases. Again we see that the cluster graph loses information but permits us to\\n\\ufb01nd a graph with qualitative features similar to the true graph with higher dimensions.\\nThe correlation graph for the dense and SEM models, while preserving validity has essentially\\nno power. More precisely, the graphical model leaves a very small imprint in the correlation\\nmatrix. For example, the covariance in the dense model is easily seen to be O(a/D). So\\nwhile the inverse covariance matrix is dense, the covariance matrix has small entries. The\\ncorrelation graph for the Markov model does contain useful information as shown in Figure\\n10. Of course, there are extra edges due to the induced correlations. Nevertheless, most of\\nthe essential structure is apparent.\\nWe also considered the behavior of the correlation graph for a few other models. Figure 11\\nshows the correlation graph for a null model, a dense covariance matrix, a four-block model\\nand a partial Markov chain (10 edges). In each case, n = 100 and D = 12. Figure 12 shows\\nthe same models but with D = 200. For these models the method does very well even with\\nD > n.\\nAs mentioned earlier, the restricted partial correlation graph is so computationally intensive\\nthat it is not yet practical. We believe the method is promising which is why we have included\\nit in the paper but at this point we do not have numerical experiments.\\nFinally, as a sanity check, we checked the coverage of the bootstrap for two models: the null\\nmodel (no edges) and the Markov model. We declare an error if there is even a single wrong\\n27\\n\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nFig 4. Bootstrap based undirected graph for Dense model with \\u03b1 = .9, a = .9, n = 100 and dimensions\\n20,30,40,50.\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nFig 5. Bootstrap based undirected graph for Markov model with \\u03b1 = .9, a = .9, n = 100 and dimensions\\n20,30,40,50.\\n28\\n\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nFig 6. Bootstrap based undirected graph for SEM model with \\u03b1 = .9, a = .5, n = 100 and dimensions\\n8,12,16,20.\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nFig 7. Cluster graph for Dense model with \\u03b1 = .9, a = .9, n = 100 and dimensions 70, 80, 90, 100 and\\nL = 20.\\n29\\n\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nFig 8. Cluster graph for Markov model with \\u03b1 = .9, a = .9, n = 100 and dimensions 70, 80, 90, 100 and\\nL = 20.\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nFig 9. Cluster graph for SEM model with \\u03b1 = .9, a = .5, n = 100 and dimensions 28, 32, 36, 40 and L = 10.\\n30\\n\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG G\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG G\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG G G G G G G G G G G G G G G G\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG G G G G G G G G G G G G G G G G G G G G\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nFig 10. Correlation graph for Markov model with \\u03b1 = .9, a = .9, n = 100 and dimensions 70, 80, 90, 100\\nand L = 20.\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nFig 11. Correlation Graphs, n=100, D=12.\\n31\\n\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\\nFig 12. Correlation Graphs, n=100, D=200.\\nedge. Using \\u03b1 = .10 and n = 100 we have the following error rates:\\nModel/Dimension\\nD = 20\\nD = 50\\nNull\\n.01\\n.01\\nMarkov\\n.00\\n.01\\nThe error rates is well under \\u03b1. Indeed, we see that the coverage is conservative as we would\\nexpect.\\n9. Conclusion.\\nWe have described methods for inferring graphs that use weak assump-\\ntions and that have con\\ufb01dence guarantees. Our methods are atavistic: we use very traditional\\nideas that have been swept aside in light of the newer sparsity-based approaches. We do not\\nmean in any way to criticize sparsity-based methods which we \\ufb01nd fascinating. But our main\\nmessage is that the older methods still have a role to play especially if we want methods\\nthat use weaker assumptions.\\nThere are several open problems that we will address in the future. We brie\\ufb02y describe a\\nfew here. First, we do not have any theory to characterize how the original graph relates to\\nthe graph of the dimension reduced problem. It would be useful to have some general theory\\nwhich shows which features of the original graph are preserved.\\nPerhaps the most important extension is to go beyond linear measures of dependence. Fol-\\n32\\n\\nlowing Bergsma (2011), write\\nY = g(X) + \\u03f5Y\\nand\\nZ = h(X) + \\u03f5Z\\nand de\\ufb01ne the nonparametric partial correlation\\n\\u03b8Y Z.X =\\nE(\\u03f5Y \\u03f5Z)\\np\\nE(\\u03f52\\nY )E(\\u03f52\\nZ)\\n.\\nLet\\nb\\u03f5Yi = Yi \\u2212bg(Xi)\\nand\\nb\\u03f5Zi = Yi \\u2212bh(Xi).\\nLet\\nb\\u03b8Y Z.X =\\nP\\ni b\\u03f5Yib\\u03f5Zi\\nqP\\ni b\\u03f52\\nYi\\nP\\ni b\\u03f52\\nZi\\n.\\nBergsma shows that, for some q1, q2 > 0,\\n\\u221an(b\\u03b8Y Z.X \\u2212\\u03b8Y Z.X) = \\u221an(rY Z.X \\u2212\\u03b8Y Z.X) + OP\\n\\u0000n\\u2212min(q1,q2)\\u0001\\nwhere\\nrY Z.X =\\nP\\ni \\u03f5Yi\\u03f5Zi\\nqP\\ni \\u03f52\\nYi\\nP\\ni \\u03f52\\nZi\\nand\\nnq1(bg(x) \\u2212g(x)) = OP(1),\\nnq2(bh(x) \\u2212h(x)) = OP(1).\\nOne can then extend the techniques in this paper to get con\\ufb01dence measures.\\nOther problems for future development are: the development of computationally e\\ufb03cient\\nmethods for computing the restricted partial correlation graph and the extension of our\\ntheory to shrinkage graphs.\\n10. Appendix: Alternative Delta Method.\\nIf one is only interested in a single partial\\ncorrelation, then one can use use a Taylor series together with the Berry-Esseen theorem.\\nWe provide this analysis here. At the end, we can turn this into a joint con\\ufb01dence set for all\\npartial correlations using the union bound but this leads to a larger error than our earlier\\nanalysis. So the main interest of this section is single partial correlations.\\nLet us write \\u03b8jk = gjk(\\u03c3) where gjk : RD\\u00d7D \\u2192[\\u22121, 1]. Let \\u2113jk and Hjk denote the gradient\\nand Hessian of gjk. Both \\u2113jk and Hjk are bounded continuous functions as long as \\u03a3 is\\ninvertible. The linearization of \\u03b8jk is\\n(43)\\n\\u221an(b\\u03b8jk \\u2212\\u03b8jk) = \\u03b4T\\u2113jk + Rjk\\n\\u221an\\nwhere \\u2113jk \\u2261\\u2113jk(\\u03c3) and the remainder term Rjk is\\n(44)\\nRjk = 1\\n2\\u03b4THjk(e\\u03c3)\\u03b4\\n33\\n\\nfor some e\\u03c3 between \\u03c3 and s. We compute \\u2113jk and Hjk explicitly in Section 6.5.\\nLet\\ns2\\njk = U(\\u03c3),\\nbs2\\njk = U(s)\\nwhere\\n(45)\\nUjk(\\u03c3) = \\u2113jk(\\u03c3)TT(\\u03c3)\\u2113jk(\\u03c3).\\nThe asymptotic variance of the linearized partial correlation \\u03b4T\\u2113jk is s2\\njk and its estimate is\\nbs2\\njk.\\nDe\\ufb01ne B =\\nn\\na : ||a\\u2212\\u03c3|| \\u2264C\\np\\nD2 log n/n\\no\\n. It follows from Lemma 7 that, for large enough\\nC, s \\u2208B except on a set of probability at most 1/n. Let\\n\\u03ben = sup\\na\\u2208B\\nmax\\njk ||\\u2113jk(a)||1\\n\\u03b3n = sup\\na\\u2208B\\nmax\\njk\\ns\\n|||Hjk(a)|||\\nsjk(a)\\n\\u03c1n = sup\\na\\u2208B\\nmax\\njk\\n||Q\\u2032\\njk(a)||1\\nsjk\\n.\\nNote that these constants are also functions of D.\\nWe begin by approximating the distribution of a single partial correlation. Let\\nTjk =\\n\\u221an(b\\u03b8jk \\u2212\\u03b8jk)\\nsjk\\n.\\nWe start by assuming that s2\\njk = \\u2113jk(\\u03c3)TT(\\u03c3)\\u2113jk(\\u03c3) is known.\\nLemma 25 We have\\nmax\\nj,k sup\\nz |P(Tjk \\u2264z) \\u2212\\u03a6(z)| \\u2aaf\\n1\\n\\u221an + 2\\u03b3n\\n\\u221an log(nD2).\\nProof. We have\\nTjk = U\\nsjk\\n+\\nRjk\\nsjk\\n\\u221an\\nwhere U = \\u221anaT(s \\u2212\\u03c3) = n\\u22121 P\\ni Vi where Vi = vec(YiY T\\ni ) \\u2212\\u03c3 and a = \\u2113jk. By Lemma 8,\\nfor every \\u03f5 > 0,\\nsup\\nz |P(Tjk \\u2264z) \\u2212\\u03a6(z)| \\u2264sup\\nz\\n\\f\\f\\f\\fP\\n\\u0012 U\\nsjk\\n\\u2264z\\n\\u0013\\n\\u2212\\u03a6(z)\\n\\f\\f\\f\\f + \\u03f5 + P\\n\\u0012\\f\\f\\f\\f\\nRjk\\nsjk\\n\\u221an\\n\\f\\f\\f\\f > \\u03f5\\n\\u0013\\n.\\nNote that Var(Vi) = s2\\njk and\\nE|Vi|3 \\u2264C\\nX\\ni\\n|aj|3.\\n34\\n\\nLet Z \\u223cN(0, 1). By the Berry-Esseen theorem,\\nsup\\nt\\n\\f\\f\\f\\f\\fP\\n\\u0012Un\\nsjk\\n\\u2264t\\n\\u0013\\n\\u2212P(Z \\u2264t)\\n\\f\\f\\f\\f\\f \\u2aaf\\nP\\nj |aj|3\\n\\u221an(aTTa)3/2 \\u2264\\nP\\nj |aj|3\\n\\u221anc3/2\\n0 ||a||3 \\u2264\\n1\\n\\u221an\\nsince ||a||3 \\u2264||a||2 and\\nP\\nj |aj|3\\n||a||3\\n= ||a||3\\n3\\n||a||3\\n2. Now\\n\\f\\f\\f\\f\\nRjk\\nsjk\\n\\u221an\\n\\f\\f\\f\\f = 1\\n2\\n\\u03b4THjk\\u03b4\\nsjk\\n\\u221an \\u2264\\u03b3n||\\u03b4||2\\nmax\\n\\u221an\\n.\\nFrom Lemma 8,\\nP\\n\\u0012\\f\\f\\f\\f\\nRjk\\nsjk\\n\\u221an\\n\\f\\f\\f\\f > \\u03f5\\n\\u0013\\n\\u2264P\\n\\u0012\\u03b3n||\\u03b4||2\\nmax\\n\\u221an\\n> \\u03f5\\n\\u0013\\n= P(||s \\u2212\\u03c3||\\u221e>\\n\\u221a\\u03f5\\nn1/4\\u221a\\u03b3 )\\n\\u2264D2e\\u2212n\\u03f5/(\\u03b3\\u221an).\\nLet \\u03f5 =\\n\\u03b3\\n\\u221an log(nD2). Then D2e\\u2212n\\u03f5/(\\u03b3\\u221an) \\u2264\\u03f5. The result follows. \\u25a1\\nNow let\\nZjk =\\n\\u221an(b\\u03b8jk \\u2212\\u03b8jk)\\nbsjk\\nwhere bs2\\njk = \\u2113jk(s)TT(s)\\u2113jk(s).\\nTheorem 26\\nmax\\nj,k sup\\nz\\n\\f\\f\\f\\f\\fP\\n \\u221an(b\\u03b8jk \\u2212\\u03b8jk)\\nbsjk\\n\\u2264z\\n!\\n\\u2212\\u03a6(z)\\n\\f\\f\\f\\f\\f \\u2aaf\\nr\\u03c1n\\nn log(nD2) + \\u03b3n\\n\\u221an log(nD2).\\nProof. Let E = {sjk/bsjk > 1 + \\u03f5} and F = {Tjk > u/\\u03f5} where \\u03f5 =\\np\\n\\u03c1n/n log(nD2) and\\nu = \\u03f5 log(n). Note that sjk \\u2212bsjk = U(\\u03c3) \\u2212U(s) = (\\u03c3 \\u2212s)TQ\\u2032 where Q\\u2032 is the gradient of Q\\nevaluated at some point between s and \\u03c3. Then, for 0 < \\u03f5 \\u22641,\\nP(Ec) = P\\n\\u0012sjk \\u2212bsjk\\nsjk\\n>\\n\\u03f5\\n1 + \\u03f5\\n\\u0013\\n= P\\n\\u0012U(\\u03c3) \\u2212U(s)\\nsjk\\n>\\n\\u03f5\\n1 + \\u03f5\\n\\u0013\\n= P\\n\\u0012(\\u03c3 \\u2212s)TQ\\u2032\\nsjk\\n>\\n\\u03f5\\n1 + \\u03f5\\n\\u0013\\n\\u2264P\\n\\u0012||s \\u2212\\u03c3||\\u221e||Q\\u2032||1\\nsjk\\n>\\n\\u03f5\\n1 + \\u03f5\\n\\u0013\\n\\u2264P\\n\\u0012\\n||s \\u2212\\u03c3||\\u221e\\u03c1n >\\n\\u03f5\\n1 + \\u03f5\\n\\u0013\\n= P\\n\\u0012\\n||s \\u2212\\u03c3||\\u221e>\\n\\u03f5\\n2\\u03c1n\\n\\u0013\\n\\u2264D2e\\u2212n\\u03f52/(4\\u03c12\\nn) \\u2264\\u03f5.\\n35\\n\\nNow,\\nP\\n \\u221an(b\\u03b8jk \\u2212\\u03b8jk)\\nbsjk\\n\\u2264z\\n!\\n\\u2212\\u03a6(z) = P\\n\\u0012\\nTjk\\n\\u0012sjk\\nbsjk\\n\\u0013\\n\\u2264z\\n\\u0013\\n\\u2212\\u03a6(z)\\n\\u2264P (Tjk(1 \\u2212\\u03f5) \\u2264z) + P(Ec) \\u2212\\u03a6(z)\\n= P (Tjk \\u2212Tjk\\u03f5) \\u2264z) + P(Ec) \\u2212\\u03a6(z)\\n\\u2264P (Tjk \\u2264z + u) + P(F c) + P(Ec) \\u2212\\u03a6(z)\\n\\u2264P (Tjk \\u2264z + u) \\u2212\\u03a6(z + u) + P(F c) + P(Ec) + u\\n\\u2264P (Tjk \\u2264z + u) \\u2212\\u03a6(z + u) + P(F c) + \\u03f5 + u.\\nNow\\nP(F c) = P(Tjk > u/\\u03f5) \\u2264P(Z > u/\\u03f5) + \\u03b3n\\n\\u221an log(nD2)\\n= P(Z > log n) + \\u03b3n\\n\\u221an log(nD2)\\n\\u2aaf\\u03b3n\\n\\u221an log(nD2).\\nSo,\\nP\\n \\u221an(b\\u03b8jk \\u2212\\u03b8jk)\\nbsjk\\n\\u2264z\\n!\\n\\u2212\\u03a6(z) \\u2264P (Tjk \\u2264z + u) \\u2212\\u03a6(z + u) + \\u03f5 + u + 1\\nn + \\u03b3n\\n\\u221an log(nD2)\\n\\u2aaf\\nr\\u03c1n\\nn log(nD2) + \\u03b3n\\n\\u221an log(nD2).\\nTaking the supremum over z gives an upper. A similar lower bound completes the proof. \\u25a1\\nNow we turn to bounding P(maxjk |Zjk| > z). We use the union bound. So,\\nP(max\\njk |Zjk| > z) \\u2264\\nX\\njk\\nP(|Zjk| > z)\\n= D2\\u03a6(z) +\\nX\\njk\\n[P(|Zjk| > z) \\u2212\\u03a6(z)]\\n\\u2264D2\\u03a6(z) + D2\\n\\u0014r\\u03c1n\\nn log(nD2) + \\u03b3n\\n\\u221an log(nD2)\\n\\u0015\\n.\\nSetting z = \\u2212\\u03a6(\\u03b1/D2) we have that\\nP(max\\njk |Zjk| > z) \\u2264\\u03b1 + D2\\n\\u0014r\\u03c1n\\nn log(nD2) + \\u03b3n\\n\\u221an log(nD2)\\n\\u0015\\n.\\nCorollary 27 Let z \\u2261z\\u03b1/D2 and let\\nR =\\nO\\nj,k\\nh\\nb\\u03b8jk \\u2212zbsjk\\n\\u221an , b\\u03b8jk + zbsjk\\n\\u221an\\ni\\n.\\n36\\n\\nThen\\nP(\\u03b8 \\u2208R) = 1 \\u2212\\u03b1 + D2\\n\\u0014r\\u03c1n\\nn log(nD2) + \\u03b3n\\n\\u221an log(nD2)\\n\\u0015\\n.\\nNote the presence of the D2 term. This term is avoided in the analysis in Section 6.\\nReferences.\\nBergsma, W. (2011). A note on the distribution of the partial correlation coe\\ufb03cient with nonparametrically\\nestimated marginal regressions. arXiv:1101.4616.\\nBoik, R. and Haaland, B. (2006). Second-order accurate inference on simple, partial, and multiple corre-\\nlations. Journal of Modern Applied Statistical Methods 5 283\\u2013308.\\nCastelo, R. and Roverato, A. (2006). A robust procedure for Gaussian graphical model search from\\nmicroarray data with p larger than n. The Journal of Machine Learning Research 7 2621\\u20132650.\\nChen, L. H. and Shao, Q.-M. (2007). Normal approximation for nonlinear statistics using a concentration\\ninequality approach. Bernoulli 581\\u2013599.\\nChernozhukov, V., Chetverikov, D. and Kato, K. (2012). Central Limit Theorem and Multiplier\\nBoostrap When p is Much Larger Than n. arXiv:1212.6906.\\nChernozhukov, V., Chetverikov, D. and Kato, K. (2013). Comparison and anti-concentration bounds\\nfor maxima of Gaussian random vectors. arXiv:1301.4807.\\nDrton, M. and Perlman, M. D. (2004). Model selection for Gaussian concentration graphs. Biometrika\\n91 591\\u2013602.\\nFriedman, J. and Tibshirani, R. (2007). Graphical lasso.\\nHarris, N. and Drton, M. (2012). PC algorithm for Gaussian copula graphical models. arXiv preprint\\narXiv:1207.0242.\\nHorn, R. A. and Johnson, C. R. (1990). Matrix analysis. Cambridge university press.\\nLedoit, O. and Wolf, M. (2004). A well-conditioned estimator for large-dimensional covariance matrices.\\nJournal of multivariate analysis 88 365\\u2013411.\\nLiu, W. (2013). Gaussian Graphical Model Estimation With False Discovery Rate Control. arXiv preprint\\narXiv:1306.0976.\\nMagnus, X. and Neudecker, H. (1988). Matrix di\\ufb00erential calculus. New York.\\nMammen, E. (1993). Bootstrap and wild bootstrap for high dimensional linear models. The Annals of\\nStatistics 255\\u2013285.\\nMeinshausen, N. and B\\u00a8uhlmann, P. (2006). High-dimensional graphs and variable selection with the\\nlasso. The Annals of Statistics 34 1436\\u20131462.\\nPinelis, I. and Molzon, R. (2013). Berry-Esseen bounds for general nonlinear statistics, with applications\\nto Pearson\\u2019s and non-central Student\\u2019s and Hotelling\\u2019s. arXiv preprint arXiv:0906.0177.\\nPortnoy, S. (1988). Asymptotic behavior of likelihood methods for exponential families when the number\\nof parameters tends to in\\ufb01nity. The Annals of Statistics 16 356\\u2013366.\\nRen, Z., Sun, T., Zhange, C.-H. and Zhou, H. (2013). Asymptotic normality and optimalities in esti-\\nmation of large Gaissian graphical models. manuscript.\\nSch\\u00a8afer, J., Strimmer, K. et al. (2005). A shrinkage approach to large-scale covariance matrix estimation\\nand implications for functional genomics. Statistical applications in genetics and molecular biology 4 32.\\nVershynin, R. (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv preprint\\narXiv:1011.3027.\\nYuan, M. and Lin, Y. (2007). Model selection and estimation in the Gaussian graphical model. Biometrika\\n94 19\\u201335.\\n37\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Text\n"
      ],
      "metadata": {
        "id": "Lkuy0agP4ZGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove non-UTF characters\n",
        "    text = text.encode('ascii', 'ignore').decode()\n",
        "\n",
        "    # Remove extra whitespace and line breaks\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Optional: Remove common header/footer junk from arXiv papers\n",
        "    text = re.sub(r'(References|Bibliography|Appendix).*$', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Apply to the dataframe\n",
        "df_with_text['cleaned_paper_text'] = df_with_text['paper_text'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "IwldoUWd-PzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_with_text.loc[0, \"cleaned_paper_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGw2dZ0R-gHi",
        "outputId": "6e195896-bcd0-4da3-e2af-76ac77f1ffee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Graphs and Manifolds Alexander N. Gorban University of Leicester, United Kingdom Andrei Y. Zinovyev Institut Curie, Paris, France ABSTRACT In many physical, statistical, biological and other investigations it is desirable to approximate a system of points by objects of lower dimension and/or complexity. For this purpose, Karl Pearson invented principal component analysis in 1901 and found lines and planes of closest fit to system of points. The famous k-means algorithm solves the approximation problem too, but by finite sets instead of lines and planes. This chapter gives a brief practical introduction into the methods of construction of general principal objects, i.e. objects embedded in the middle of the multidimensional data set. As a basis, the unifying framework of mean squared distance approximation of finite datasets is selected. Principal graphs and manifolds are constructed as generalisations of principal components and k- means principal points. For this purpose, the family of expectation/maximisation algorithms with nearest generalisations is presented. Construction of principal graphs with controlled complexity is based on the graph grammar approach. INTRODUCTION In many fields of science, one meets with multivariate (multidimensional) distributions of vectors representing some observations. These distributions are often difficult to analyse and make sense of due to the very nature of human brain which is able to visually manipulate only with the objects of dimension no more than three. This makes actual the problem of approximating the multidimensional vector distributions by objects of lower dimension and/or complexity while retaining the most important information and structures contained in the initial full and complex data point cloud. The most trivial and coarse approximation is collapsing the whole set of vectors into its mean point. The mean point represents the most typical properties of the system, completely forgetting variability of observations. The notion of the mean point can be generalized for approximating data by more complex types of objects. In 1901 Pearson proposed to approximate multivariate distributions by lines and planes (Pearson, 1901). In this way the Principal Component Analysis (PCA) was invented, nowadays a basic statistical tool. Principal lines and planes go through the middle of multivariate data distribution and correspond to the first few modes of the multivariate Gaussian distribution approximating the data. Starting from 1950s (Steinhaus, 1956; Lloyd, 1957; and MacQueen, 1967), it was proposed to approximate the complex multidimensional dataset by several mean points. Thus k-means algorithm was suggested and nowadays it is one of the most used clustering methods in machine learning (see a review presented by Xu & Wunsch, 2008). Both these directions (PCA and K-Means) were further developed during last decades following two major directions: 1) linear manifolds were generalised for non-linear ones (in simple words, initial lines and planes were bended and twisted), and 2) some links between the mean points were introduced. This led to appearance of several large families of new statistical methods; the most famous from them are Principal Curves, Principal Manifolds and Self-Organising Maps (SOM). It was quickly realized that the objects that are constructed by these methods are tightly connected theoretically. This observation allows now to develop a common framework called Construction of Principal Objects. The geometrical nature of these objects can be very different but all of them serve as data approximators of controllable complexity. It allows using them in the tasks of dimension and complexity reduction. In Machine Learning this direction is connected with terms Unsupervised Learning and Manifold Learning. In this chapter we will overview the major directions in the field of principal objects construction. We will formulate the problem and the classical approaches such as PCA and k-means in a unifying framework, and show how it is naturally generalised for the Principal Graphs and Manifolds and the most general types of principal objects, Principal Cubic Complexes. We will systematically introduce the most used ideas and algorithms developed in this field. APPROXIMATIONS OF FINITE DATASETS Definition. Dataset is a finite set X of objects representing N multivariate (multidimensional) observations. These objects xiX, i =1N, are embedded in Rm and in the case of complete data are vectors xiRm. We will also refer to the individual components of xi as i kx such that ) ,..., , ( 2 1 i m i i i x x x x ; we can also represent dataset as a data matrix } { i jx X . Definition. Distance function dist(x,y) is defined for any pair of objects x, y from X such that three usual axioms are satisfied: dist(x,x) = 0, dist(x,y) = dist(y,x), dist(x,y)+dist(y,z) dist(x,z). Definition. Mean point MF(X) for X is a vector MFRm such that 2 .. 1 )) , ( dist ( min arg ) ( N i i R F m X x y M y . In this form the definition of the mean point goes back to Frchet (1948). Notice that in this definition the mean point by Frchet can be non-unique. However, this definition allows multiple useful generalisations including using it in the abstract metric spaces. It is easy to show that in the case of complete data and the Euclidean distance function m i i i 1 2) ( ) , ( dist y x y x , or, more generally, in the case of any quadratic distance function (for example, Mahalanobis distance), the mean point is the standard expectation ) ( 1 ) ( 1 X N X N i i F E x . Definition. Orthogonal projection P(x,Y) is defined for an object x and a set (not necessarily finite) of vectors Y as a vector in Y such that ) , ( min arg ) , ( y x x y dist Y P Y . Notice that in principle, one can have non-unique and even infinitely many projections of x on Y. Definition. Mean squared distance MSD(X,Y) between a dataset X and a set of vectors Y is defined as N i i i Y P N Y X 1 2 )) , ( , ( dist 1 ) , MSD( x x . We will also consider a simple generalisation of MSD: weighted mean squared distance N i i i i N i i W Y P w w Y X 1 2 1 )) , ( , ( dist 1 ) , ( MSD x x , where wi > 0 is a weight for the object xi. Our objective in the rest of the chapter is to briefly describe the methods for constructing various approximations (principal objects) for a dataset X. In almost all cases the principal objects will be represented as a finite or infinite set of vectors YRm such that 1) it approximates the finite dataset X in the sense of minimisation of MSD(X,Y), and 2) it answers some regularity conditions that will be discussed below. PROBABILISTIC INTERPRETATION OF STATISTICS AND NOTION OF SELF- CONSISTENCY In his original works, Pearson followed the principle that the only reality in data analysis is the dataset, embedded in a multidimensional metric space. This approach can be called geometrical. During the 20th century, probabilistic interpretation of statistics was actively developed. Accordingly to this interpretation, a dataset X is one particular of i.i.d. sample from a multidimensional probability distribution F(x) which defines a probability of appearance of a sample in the point xRm. The probability distribution, if can be estimated, provides a very useful auxiliary object allowing to define many notions in the theory of statistical data analysis. In particular, it allows us to define principal manifolds as self-consistent objects. The notion of self-consistency in this context was first introduced by Efron (1967) and developed in the works of Flury (Tarpey & Flury, 1996), where it is claimed to be one of the most fundamental in statistical theory. Definition. Given probability distribution F(x) and a set of vectors Y we say that Y is self-consistent with respect to F(x) if ) ) , ( ( y x x E y Y P F for every vector yY. In words, it means that any vector yY is a conditional mean expectation of point x under condition that x is orthogonally projected in y. The disadvantage of this definition for finite datasets is that it is not always possible to calculate the conditional mean, since typically for points yY it is only one or zero point projected from X. This means that for finite datasets we should develop coarse-grained self-consistency notion. Usually it means that for every point yY one defines some kind of neighbourhood and introduces a modified self-consistency with respect to this neighbourhood instead of y itself. Concrete implementations of this idea are described further in this chapter. In all cases, the effective size of the neighbourhood is a fundamental parameter in controlling the complexity of the resulting approximator Y. FOUR APPROACHES TO CLASSICAL PCA We can define linear principal manifolds as mean squared distance data approximators, constructed from linear manifolds embedded in Rm. In fact, this corresponds to the original definition of principal lines and planes by Pearson (Pearson, 1901). However, PCA method was re-invented in other fields and even obtained different names (Karhunen-Love or KL decomposition (Karhunen, 1946; Love, 1955), Hotteling transform (Hotelling, 1933), Proper Orthogonal Decomposition (Lumley, 1967)) and others. Here we formulate four equivalent ways to define principal components that the user can meet in different applications. Let us consider a linear manifold Lk of dimension k in the parametric form Lk = {a0 + 1a1 + + kak | iR }, a0Rm and {a1,, ak} is a set of orthonormal vectors in Rm. Definition of PCA problem #1 (data approximation by lines and planes): PCA problem consists in finding such sequence Lk (k=1,2,,m-1) that the sum of squared distances from data points to their orthogonal projections on Lk is minimal over all linear manifolds of dimension k embedded in Rm: min ) , MSD( kL X (k=1,2,,m-1). Definition of PCA problem #2 (variance maximisation): For a set of vectors X and for a given ai, let us construct a one-dimensional distribution i = {: = (x,ai), xX} where (,) denotes scalar vector product. Then let us define empirical variance of X along ai as Var(Bi), where Var( ) is the standard empirical variance. PCA problem consists in finding such Lk that the sum of empirical variances of X along a1,, ak would be maximal over all linear manifolds of dimension k embedded in Rm: max ) ( Var .. 1 k i i . Let us also consider an orthogonal complement {ak+1 , , am} of the basis {a1 , , ak}. Then an equivalent definition (minimization of residue variance) is min ) ( Var 1 m k i i . Definition of PCA problem #3 (mean point-to-point squared distance maximisation): PCA problem consists in finding such sequence Lk that the mean point-to-point squared distance between the orthogonal projections of data points on Lk is maximal over all linear manifolds of dimension k embedded in Rm: max )) , ( ), , ( ( dist 1 1 , 2 N j i k j k i L P L P N x x . Having in mind that all orthogonal projections onto lower-dimensional space lead to contraction of all point-to-point distances (except for some that do not change), this is equivalent to minimisation of mean squared distance distortion: min ))] , ( ), , ( ( dist ) , ( [dist 1 , 2 2 N j i k j k i j i L P L P x x x x . In the three above mentioned definitions, the basis vectors are defined up to an arbitrary rotation that does not change the manifold. To make the choice less ambiguous, in the PCA method the following principle is applied: given {a0, a1,, ak}, any embedded linear manifold of smaller dimension s in the form Ls = {a0 + 1a1 + + sas| iR, s < k}, must be itself a linear principal manifold of dimension s for X (a flag of principal subspaces). Definition of PCA problem #4 (correlation cancellation): Find such an orthonormal basis (a1,, as) in which the covariance matrix for x is diagonal. Evidently, in this basis the distributions (ai,x) and (aj,x), for i j, have zero correlation. Definitions 1-3 were given for finite datasets while definition 4 is sensible both for finite datasets and random vector x. For finite datasets the empiric correlation should be cancelled. The empiric principal components which annul empiric correlations could be considered as an approximation to the principal components of the random vector. Equivalence of the above-mentioned definitions in the case of complete data and Euclidean space follows from Pythagorean Theorem and elementary algebra. However, in practice this or that definition can be more useful for computations or generalisations of the PCA approach. Thus, only definitions #1 and #3 are suitable for working with incomplete data since they are defined with use of only distance function that can be easily calculated for the gapped data vectors (see further). The definition #1 can be generalized by weighting data points (Cochran & Horne, 1977), while the definition #3 can be generalized by weighting pairs of data points (Gabriel & Zamir, 1979). More details about PCA and generalisations could be found in the fundamental book by Jollliffe (2002). BASIC EXPECTATION/MAXIMISATION ITERATIVE ALGORITHM FOR FINDING PRINCIPAL OBJECTS Most of the algorithms for finding principal objects for a given dataset X are constructed accordingly to the classical expectation/maximisation (EM) splitting scheme that was first formulated as a generic method by Dempster et al (1977): Generic Expectation-Maximisation algorithm for estimating principal objects 1) Initialisation step. Some initial configuration of the principal object Y is generated; 2) Expectation (projection) step. Given configuration of Y, calculate orthogonal projections P(x,Y), for all xX; 3) Maximisation step. Given the calculated projections, find more optimal configuration of Y with respect to X. 4) (Optional) adaptation step. Using some strategy, change the properties of Y (typically, add or remove points to Y). 5) Repeat steps 2-4 until some convergence criteria would be satisfied. For example, for the principal line, we have the following implementation of the above mentioned bi-iteration scheme (Bauer, 1957; for generalisations see works of Roweis (1998) and Gorban & Rossiev (1999)). Iterative algorithm for calculating the first principal component 1) Set a0 = MF(X) (i.e., zero order principal component is the mean point of X); 2) Choose randomly a1; 3) Calculate 2 1 1 0 ) , ( a a a xi ib , i =1N ; 4) Given bi, find new a1, such that min ) ( 1 1 2 1 0 a a a x N i i i b , i.e. N i i N i i N i i i b b b .. 1 2 .. 1 0 .. 1 1 a x a ; 5) Re-normalize || || : 1 1 1 a a a . 6) Repeat steps 3-5 until the direction of a1 do not change more than on some small angle . Remark. To calculate all other principal components, deflation approach is applied: after finding a1, one calculates new X(1) = X - a0 - a1(x,a1), and the procedure is repeated for X(1). Remark. The basic EM procedure has good convergence properties only if the first eigenvalues of the empirical covariance matrix XTX are sufficiently well separated. If this is not the case, more sophisticated approaches are needed (Bau & Trefethen, 1997). The PCA method can be treated as spectral decomposition of the symmetric and positively defined empirical covariance data matrix (defined in the case of complete data) X X N C T 1 1 or N k k j k i ij x x N C 1 1 1 , where without loss of generality we suppose that the data are centered. Definition. We call a singular value for the data matrix X iff there exist two vectors of unit length a and b such that T X b a and T X a b . Then the vectors a { ) ( ) ( 1 , , m a a } and b { ) ( ) ( 1 , , N b b } are called left and right singular vectors for the singular value . If we know all p singular values of X, where p = rank(X) min(N, m), then we can represent X as p l l l l X 1 ) ( ) ( a b or p l l i l k l k i a b x 1 ) ( ) ( . It is called the singular value decomposition (SVD) of X. It is easy to check that the vectors alcorrespond to the principal vectors of X and the eigenvectors of the empirical covariance matrix C, whereas bl contain projections of N points onto the corresponding principal vector. Eigenvalues l of C and singular values l of X and are connected by 2) ( 1 1 l l N . The mathematical basis for SVD was introduced by Sylvester (1889) and it represents a solid mathematical foundation for PCA (Strang, 1993). Although formally the problems of spectral decomposition of X and eigen decomposition of C are equivalent, the algorithms for performing singular decomposition directly (without explicit calculation of C) can be more efficient and robust (Bau III & Trefethen, 1997). Thus, the iterative EM algorithm for calculating the first principal component described in the previous chapter indeed performs singular decomposition (for centered data we simply put a0 = 0) and finds right singular (principal) and left singular vectors one by one. K-MEANS AND PRINCIPAL POINTS K-means clustering goes back to 1950s (Steinhaus (1956); Lloyd (1957); and MacQueen (1967)). It is another extreme in its simplicity case of finding a principal object. In this case it is simply an unstructured finite (and usually, much smaller than the number of points N in the dataset X) set of vectors (centroids). One can say that the solution searched by the k-means algorithm is a set of k principal points (Flury, 1990). Definition. A set of k points Y={y1,..,yk}, yiRm is called a set of principal points for dataset X if it approximates X with minimal mean squared distance error over all sets of k-points in Rm (distortion): min )) , ( , ( dist 2 X Y P x x x , where P(x,Y) is the point from Y closest to x. Note that the set of principal points can be not unique. The simplest implementation of the k-means procedure follows the classical EM scheme: Basic k-means algorithm 1) Choose initial position of y1,..,yk randomly from xiX (with equal probabilities); 2) Partition X into subsets Ki, i=1..k of data points by their proximity to yk: )} , ( dist min arg : { j Y i i j K y x y x y ; 3) Re-estimate i K i i K x x y | | 1 , i = 1..k; 4) Repeat steps 2-3 until complete convergence. The method is sensitive to the initial choice of y1,..,yk . Arthur & Vassilvitskii (2007) demonstrated that the special construction of probabilities instead of equidistribution gives serious advantages. The first centre, y1, they select equiprobable from X. Let the centres y1,..,yj are chosen (j < k) and D(x) be the squared shortest distance from a data point x to the closest centre we have already chosen. Then, we select the next centre, yj+1, from xiX with probability ) ( ) ( ) ( x x x x X i i D D p . Evidently, any solution of k-means procedure converges to a self-consistent set of points Y={y1,..,yk} (because Y = E[P(X,Y)]), but this solution may give a local minimum of distortion and is not necessary the set of principal points (which is the globally optimal approximator from all possible k-means solutions). Multiple generalisations of k-means scheme have been developed (see, for example, a book of Mirkin (2005) based on the idea of data recovering). The most computationally expensive step of the algorithm, partitioning the dataset by proximity to the centroids, can be significantly accelerated using kd-tree data structure (Pelleg & Moore, 1999). Analysis of the effectiveness of EM algorithm for the k-means problem was given by Ostrovsky et al. (2006). Notice that the case of principal points is the only in this chapter when self- consistency and coarse-grained self-consistency coincide: centroid yk is the conditional mean point for the data points belonging to the Voronoi region associated with yk. LOCAL PCA The term Local PCA was first used by Braverman (1970) and Fukunaga & Olsen (1971) to denote the simplest cluster-wise PCA approach which consists in 1) applying k-means or other type of clustering to a dataset and 2) calculating the principal components for each cluster separately. However, this simple idea performs rather poorly in applications, and more interesting approach consists in generalizing k-means by introducing principal hyperplane segments proposed by Diday (1979) and called k-segments or local subspace analysis in a more advanced version (Liu, 2003). The algorithm for their estimation follows the classical EM scheme. Further development of the local PCA idea went in two main directions. First, Verbeek (2002) proposed a variant of the k-segment approach for one- dimensional segments accompanied by a strategy to assemble disconnected line segments into the global piecewise linear principal curve. Einbeck et al (2008) proposed an iterative cluster splitting and joining approach (recursive local PCA) which helps to select the optimal number and configuration of disjoined segments. Second direction is associated with a different understanding of locality. It consists in calculating local mean points and local principal directions and following them starting from (may be multiple) seed points. Locality is introduced using kernel functions defining the effective radius of neighborhood in the data space. Thus, Delicado (2001) introduced principal oriented points (POP) based on the variance maximisation-based definition of PCA (#2 in our chapter). POPs are different from the principal points introduced above because they are defined independently one from another, while the principal points are defined globally, as a set. POPs can be assembled into the principal curves of oriented points (PCOP). Einbeck (2005) proposed a simpler approach based on local tracing of principal curves by calculating local centers of mass and the local first principal components. SOM APPROACH FOR PRINCIPAL MANIFOLD APPROXIMATION AND ITS GENERALISATIONS Kohonen in his seminal paper (Kohonen, 1982) proposed to modify the k-means approach by introducing connections between centroids such that a change in the position of one centroid would also change the configuration of some neighboring centroids. Thus Self-Organizing Maps (SOM) algorithm was developed. With the SOM algorithm (Kohonen, 1982) we take a finite metric space V with metric and try to map it into Rm with combinations of two criteria: (1) the best preservation of initial structure in the image of V and (2) the best approximation of the dataset X. In this way, SOMs give the most popular approximations for principal manifolds: we can take for V a fragment of a regular s-dimensional grid and consider the resulting SOM as the approximation to the s-dimensional principal manifold (Mulier & Cherkassky, 1995; Ritter et al, 1992; Yin H. 2008). The SOM algorithm has several setup variables to regulate the compromise between these goals. In the original formulation by Kohonen, we start from some initial approximation of the map, 1: V Rm. Usually this approximation lies on the s-dimensional linear principal manifold. On each k-th step of the algorithm we have a chosen datapoint xX and a current approximation k: V Rm. For these x and k we define an owner of x in V: ) ( min arg v v k V v x x . The next approximation, k+1, is k+1(v) = hkw((v,vx))(x k(v)). Here hk is a step size, 0 w((v,vx)) 1 is a monotonically decreasing neighborhood function. This process proceeds in several epochs, with neighborhood radius decreasing during each next epoch. The idea of SOM is flexible, was applied in many domains of science, and it lead to multiple generalizations (see the review paper by Yin (2008)). Some of the algorithms for constructing SOMs are of EM type described above, such as the Batch SOM Algorithm (Kohonen, 1997): it includes projecting step exactly the same as in k-means and the maximization step at which all k(v) are modified simultaneously. One source of theoretical dissatisfaction with SOM is that it is not possible to define an optimality criterion (Erwin et al, 1992): SOM is a result of the algorithm at work and there does not exist any objective function that is minimized by the training process. In attempt to resolve this issue, Bishop et al. (1998) developed the optimization- based Generative Topographic Mapping (GTM) method. In this setting, it is supposed that the observed data is i.i.d. sample from a mixture of Gaussian distributions with the centers aligned along a two-dimensional grid, embedded in the data space. Parameters of this mixture are determined by EM-based maximization of the likelihood function (probability of observing X within this data model). PRINCIPAL MANIFOLDS BY HASTIE AND STUELZE Principal curves and principal two-dimensional surfaces for a probability distribution F(x) were introduced in the PhD thesis by Trevor Hastie (1984) as a self-consistent (non-linear) one- and two-dimensional globally parametrisable smooth manifolds without self-intersections. Definition. Let G be the class of differentiable 1-dimensional curves in Rm, parameterized by R1 and without self-intersections. The Principal Curve of the probability distribution F(x) is such a Y()G that is self-consistent. Remark. Usually, a compact subset of Rm and a compact interval of parameters R1 are considered. To discuss unbounded regions, it is necessary to add a condition that Y() has finite length inside any bounded subset of Rm (Kgl, 1999). Definition. Let G2 be the class of differentiable 2-dimensional surfaces in Rm, parameterized by R2 and without self-intersections. The Principal Surface of the probability distribution F(x) is such a Y()G2 that is self-consistent. (Again, for unbounded regions it is necessary to assume that for any bounded set B from Rm the set of parameters for which Y()B is also bounded.) First, Hastie and Stuelze proposed an algorithm for finding the principal curves and principal surfaces for a probability distribution F(x), using the classical EM splitting. We do not provide this algorithm here because for a finite dataset X it can not be directly applied because in a typical point on Y() only zero or one data point is projected, hence, one can not calculate the expectation. As mentioned above, in this case we should use some kind of coarse-grained self-consistency. In the original approach by Hastie (1984), this is done through introducing smoothers. This gives the practical formulation of the HS algorithm for estimating the principal manifolds from a finite dataset X: Hastie and Stuelze algorithm for finding principal curve for finite dataset 1) Initialize Y() = a0+a where a0 is a mean point and a1 is the first principal component; 2) Project every data point xi onto Y(): i.e., for each xi find i such that 2 || ) ( || inf arg ) ( i i Y Y x . In practice it requires interpolation procedure because Y() is determined in a finite number of points { N}. The simplest is the piecewise interpolation procedure, but more sophisticated procedures can be proposed (Hastie, 1984); 3) Calculate new Y() in the finite number of internal coordinates { N} (found at the previous step) as the local average of points xi and some other points, that have close to i projections onto Y. To do this, 1) a span [wN] is defined ( [.] here is integer part ), where 0 < w << 1 is a parameter of the method (coarse-grained self-consistency neighbourhood radius); 2) for [wN] internal coordinates } ,..., { N] [w 1 i i closest to i and the corresponding } ,..., { N] [w 1 i i x x calculate weighted least squares linear regression y() = a(i) b(i); 3) define Y( i) as the value of the linear regression in i: Y( i) = a(i) ib(i). 4) Reassign Y() Y() 5) Repeat steps 2)-4) until Y does not change (approximately). Remark. For the weights in the regression at the step 3) Hastie proposed to use some symmetric kernel function that vanishes on the borders of the neighbourhood. For example, for xi let us denote as N] [w i the most distant value of the internal coordinate from [wN] ones closest to i. Then we can define weight for the pair ( j j , i i x ) as otherwise. ,0 ) ) (| - (1 3 1 3 |, | | | if , | |/| N j j N j j i i i i / i i i i i j Remark. At the step 3) an alternative approach was also proposed with use of cubic splines to approximate the smooth function Y() from all pairs ( i,xi), i = 1..N. Non-linear Principal Manifolds constructed by this algorithm are usually called Hastie-Stuelze (HS) principal manifolds. However, the global optimality of HS principal manifolds is not guaranteed (only self-consistency in the case of distribution or coarse-grained self-consistency in the case of dataset is guaranteed by construction). For example, the second principal component of a sample X from a normal distribution is self-consistent and will be correct HS principal curve but of course not the optimal one. We should also underline that our view on what is the object constructed by the HS algorithm for a dataset X depends on 1) probabilistic interpretation of the nature of X, and 2) the chosen heuristic approach to coarse-grained self- consistency. If we do not suppose that the dataset is generated by i.i.d. sampling from F(x) then the definition of HS principal manifold is purely operational: HS principal manifold for X is the result of application of HS algorithm for finite datasets. Analogous remark is applicable for all principal manifold approximators constructed for finite datasets and described further in this chapter. In his PhD thesis Hastie noticed that the HS principal curve does not coincide with the generating curve in a very simple additive data generation model X = f()+, (1) where f() is some curve embedded in data space and is noise distribution independent on Because of the fact that if f() is not a straight line then it is not self-consistent, HS principal curves were claimed to be biased. This inspired Tibshirani (1992) to introduce an alternative definition of the principal curve, based directly on a continuous mixture model (1) and maximising regularized likelihood. KGL-KRYZHAK IMPROVEMENT Kgl in his PhD thesis supervised by Kryzhak (Kgl, 1999) revised the existing methods for estimating the principal curves. In particular, this led to the definition of principal curves with limited length. Definition. Principal curve YL() of length L is such a curve that the mean squared distance from the dataset X to the curve YL() is minimal over all curves of length less than or equal to L: min )) , ( , ( dist 1 2 N i L i i Y P x x . Theorem. Assume that X has finite second moments, i.e. N i T i i 1 ) (x x . Then for any L > 0 there exists a principal curve of length L. Principal curves of length L as defined by Kgl, are globally optimal approximators as opposite to the HS principal curves that are only self-consistent. However, all attempts to construct a practical algorithm for finding globally optimal principal curves of length L were not successful. Instead Kgl developed an efficient heuristic Polygonal line algorithm for constructing piecewise linear principal curves. Let us consider a piecewise curve Y composed from vertices located in points {y1,,yk+1} and k segments connecting pairs of vertices {yj,yj+1}, j=1..k. Kgls algorithm searches for a (local) optimum of the penalised mean squared distance error function: 1 1 ) CP( 1 ) , MSD( ) , ( k i i k Y X Y X U , (2) where CP(i) is a curvature penalty function for a vertex i chosen as 1 if 1 1 if )) ( cos 1( 1 if ) ( 2 1 2 2 2 1 k i k i i r i i CP k k y y y y , where || |||| || ) , ( ) ( cos 1 1 1 1 i i i i i i i i i y y y y y y y y is the cosines of the angle between two neighbouring segments at the vertex i, )) ( , dist( max X r X F x M x is the radius of the dataset X, and is a parameter controlling the curve global smoothness. The Polygonal line algorithm (Kgl, 1999) follows the standard EM splitting scheme: Polygonal line algorithm for estimating piece-wise linear principal curve 1) The initial approximation is constructed as a segment of principal line. The length of the segment is the difference between the maximal and the minimal projection value of X onto the first principal component. The segment is positioned such that it contains all of the projected data points. Thus in the initial approximation one has two vertices {y1,y2} and one segment between them (k = 1). 2) Projection step. The dataset X is partitioned into 2k+1 )} , ( dist min arg : { segments vertices z z K z z x x subsets constructed by their proximity to k+1 vertices and k segments. If a segment i and a vertex j are equally distant from x then x is placed into Kj only. 3) Optimisation step. Given partitioning obtained at the step 2, the functional U(X,Y) is optimised by use of a gradient technique. Fixing partitioning into Ki is needed to calculate the gradient of U(X,Y) because otherwise it is not a differentiable function with respect to the position of vertices {yi}. 4) Adaptation step. Choose the segment with the largest number of points projected onto it. If more than one such segment exists then the longest one is chosen. The new vertex is inserted in the midpoint of this segment; all other segments are renumerated accordingly. 5) Stopping criterion. The algorithm stops when the number of segments exceeds ) , MSD( 3 / 1 Y X r N . Heuristically, the default parameters of the method have been proposed = 0.3, r Y X N k ) , MSD( ' 3 / 1 , = 0.13. The details of implementation together with convergence and computational complexity study are provided elsewhere (Kgl, 1999). Smola et al. (2001) proposed a regularized principal manifolds framework, based on minimization of quantization error functional with a large class of regularizers that can be used and a universal EM-type algorithm. For this algorithm, the convergence rates were analyzed and it was showed that for some regularizing terms the convergence can be optimized with respect to the Kegls polygonal line algorithm. ELASTIC MAPS APPROACH In a series of works (Gorban & Rossiev, 1999; Gorban et al., 2001, 2003; Gorban & Zinovyev, 2005, 2008a; Gorban et al., 2007, 2008), the authors of this chapter used metaphor of elastic membrane and plate to construct one-, two- and three-dimensional principal manifold approximations of various topologies. Mean squared distance approximation error combined with the elastic energy of the membrane serves as a functional to be optimised. The elastic map algorithm is extremely fast at the optimisation step due to the simplest form of the smoothness penalty. It is implemented in several programming languages as software libraries or front-end user graphical interfaces freely available from the web-site http://bioinfo.curie.fr/projects/vidaexpert. The software found applications in microarray data analysis, visualization of genetic texts, visualization of economical and sociological data and other fields (Gorban et al, 2001, 2003; Gorban & Zinovyev 2005, 2008a; Gorban et al, 2007, 2008). Let G be a simple undirected graph with set of vertices V and set of edges E. Definition. k-star in a graph G is a subgraph with k + 1 vertices v0,1,...,k V and k edges {(v0, vi)|i = 1, .., k} E. The rib is by definition a 2-star. Definition. Suppose that for each k 2, a family Sk of k-stars in G has been selected. Then we define an elastic graph as a graph with selected families of k-stars Sk and for which for all E(i) E and ) ( j k S Sk, the corresponding elasticity moduli i > 0 and kj > 0 are defined. Definition. Primitive elastic graph is an elastic graph in which every non-terminal node (with the number of neighbours more than one) is associated with a k-star formed by all neighbours of the node. All k-stars in the primitive elastic graph are selected, i.e. the Sk sets are completely determined by the graph structure. Definition. Let E(i)(0), E(i)(1) denote two vertices of the graph edge E(i) and ) ( j k S (0), ..., ) ( j k S (k) denote vertices of a k-star ) ( j k S (where ) ( j k S (0) is the central vertex, to which all other vertices are connected). Let us consider a map :V Rm which describes an embedding of the graph into a multidimensional space. The elastic energy of the graph embedding in the Euclidean space is defined as ) ( ) ( :) ( G U G U G U R E , (3) ) ( 2 ) ( ) ( )) 1( ( )) 0 ( ( :) ( i E i i i E E E G U , (4) ) ( 2 1 ) ( ) ( || )) ( ( 1 )) 0 ( ( || :) ( j k S k i j k j k kj E i S k S G U . (5) Fig. 1. Elastic nets used in practice. Definition. Elastic net is a particular case of elastic graph which (1) contains only ribs (2-stars) (the family Sk are empty for all k>2); and (2) the vertices of this graph form a regular small-dimensional grid (Fig.1). The elastic net is characterised by internal dimension dim(G). Every node vi in the elastic net is indexed by the discrete values of internal coordinates } ,..., { ) dim( 1 i G i in such a way that the nodes close on the graph have similar internal coordinates. The purpose of the elastic net is to introduce point approximations to manifolds. Historically it was first explored and used in applications. To avoid confusion, one should notice that the term elastic net was independently introduced by several groups: for solving the traveling salesman problem (Durbin &Willshaw, 1987), in the context of principal manifolds (Gorban et al, 2001) and recently in the context of regularized regression problem (Zhou & Hastie, 2005). These three notions are completely independent and denote different things. Definition. Elastic map is a continuous manifold YRm constructed from the elastic net as its grid approximation using some between-node interpolation procedure. This interpolation procedure constructs a continuous mapping c:{ 1,, dim(G)} Rm from the discrete map V Rmused to embed the graph in Rm, and the discrete values of node indices } ,..., { ) dim( 1 i G i , i = 1...|V|. For example, the simplest piecewise linear elastic map is built by piecewise linear map c. Definition. Elastic principal manifold of dimension s for a dataset X is an elastic map, constructed from an elastic net Y of dimension s embedded in Rm using such a map opt:Y Rmthat corresponds to the minimal value of the functional ) ( ) , ( MSD ) , ( W G U Y X Y X U , (6) where the weighted mean squared distance from the dataset X to the elastic net Y is calculated as the distance to the finite set of vertices {y1= v1 yk= vk}. In the Euclidean space one can apply an EM algorithm for estimating the elastic principal manifold for a finite dataset. It is based in turn on the general algorithm for estimating the locally optimal embedding map for an arbitrary elastic graph G, described below. Optimisation of the elastic graph algorithm: 1) Choose some initial position of nodes of the elastic graph {y1= v1 yk= vk}, where k is the number of graph nodes k = |V|; 2) Calculate two matrices eij and sij , using the following sub-algorithm: i. Initialize the sij matrix to zero; ii. For each k-star ) (i k S with elasticity module ki, outer nodes vN1 , ... ,vNk and the central node vN0, the sij matrix is updated as follows (1 l,m k): k s s k s s k s s s s ki N N N N ki N N N N ki N N N N ki N N N N l l l l m l m l 0 0 0 0 0 0 0 0 , , 2 iii. Initialize the eij matrix to zero; iv. For each edge E(i) with weight i, one vertex vk1 and the other vertex vk2, the ejk matrix is updated as follows: i k k k k i k k k k i k k k k i k k k k e e e e e e e e 1 2 1 2 2 1 2 1 2 2 2 2 1 1 1 1 , , 3) Partition X into subsets Ki, i=1..k of data points by their proximity to yk: )} , ( dist min arg : { j Y i i j K y x y x y ; 4) Given Ki , calculate matrix js js N i i js j js s e w n a 1 , where j i K x i j w n , js is the Kroneckers symbol. 5) Find new position of {y1 yk} by solving the system of linear equations j i K i i N i i k s s js w w a x x y 1 1 1 6) Repeat steps 3-5 until complete or approximate convergence of node positions {y1 yk}. As usual, the EM algorithm described above gives only locally optimal solution. One can expect that the number of local minima of the energy function U grows with increasing the softness of the elastic graph (decreasing kj parameters). Because of this, in order to obtain a solution closer to the global optimum, the softening strategy has been proposed, used in the algorithm for estimating the elastic principal manifold. Algorithm for estimating the elastic principal manifold 1) Define a decreasing set of numbers {m1,,mp}, mp=1 (for example, {103, 102, 10, 1}), defining p epochs for softening; 2) Define the base values of the elastic moduli ) (base i and ) (base i ; 3) Initialize positions of the elastic net nodes {y1 yk} on the linear principal manifold spanned by first dim(G) principal components; 4) Set epoch_counter = 1 5) Set the elastic moduli ) ( _ base i counter epoch i m and ) ( _ base i counter epoch i m ; 6) Modify the elastic net using the algorithm for optimisation of the elastic graph; 7) Repeat steps 5-6 for all values of epoch_counter = 2, , p. Remark. The values i and j are the coefficients of stretching elasticity of every edge E(i) and of bending elasticity of every rib ) ( 2 j S . In the simplest case 1 = 2 = ... = s = (s), 1 = 2 = ... = r = (r), where s and r are the numbers of edges and ribs correspondingly. Approximately dependence on graph resolution is given by Gorban & Zinovyev (2007): ) dim( ) dim( 2 0 ) dim( ) dim( 2 0 ) ( , ) ( G G G G r s s s . This formula is applicable, of course, only for the elastic nets. In general a case i and i are often made variable in different parts of the graph accordingly to some adaptation strategy (Gorban & Zinovyev, 2005). Remark. ) (G U E penalizes the total length (or, indirectly, square, volume, etc.) of the constructed manifold and provides regularization of distances between node positions at the initial steps of the softening. At the final stage of the softening i can be put to zero with little effect on the manifold configuration. Elastic map post-processing such as map extrapolation can be applied to increase its usability and avoid the border effect, for details see (Gorban & Zinovyev, 2008a). PLURIHARMONIC GRAPHS AS IDEAL APPROXIMATORS Approximating datasets by one dimensional principal curves is not satisfactory in the case of datasets that can be intuitively characterized as branched. A principal object which naturally passes through the middle of such a data distribution should also have branching points that are missing in the simple structure of principal curves. Introducing such branching points converts principal curves into principal graphs. Principal graphs were introduced by Kgl & Krzyzak (2002) as a natural extension of one-dimensional principal curves in the context of skeletonisation of hand-written symbols. The most important part of this definition is the form of the penalty imposed onto deviation of the configuration of the branching points embedment from their ideal configuration (end, line, corner, T-, Y- and X- configuration). Assigning types for all vertices serves for definition of the penalty on the total deviation from the graph ideal configuration (Kgl, 1999). Other types of vertices were not considered, and outside the field of symbol skeletonization applicability of such a definition of principal graph remains limited. Gorban & Zinovyev (2005), Gorban et al. (2007), and Gorban et al. (2008) proposed to use a universal form of non-linearity penalty for the branching points. The form of this penalty is defined in the previous chapter for the elastic energy of graph embedment. It naturally generalizes the simplest three-point second derivative approximation squared: for a 2-star (or rib) the penalty equals 2 ) ( 2 ) ( 2 ) ( 2 || ))) 2 ( ( )) 1( ( ( 2 1 )) 0 ( ( || j j j S S S , for a 3-star it is 2 ) ( 3 ) ( 3 ) ( 3 ) ( 3 || ))) 3 ( ( )) 2 ( ( )) 1( ( ( 3 1 )) 0 ( ( || j j j j S S S S , etc. For a k-star this penalty equals to zero iff the position of the central node coincides with the mean point of its neighbors. An embedment (G) is ideal if all such penalties equal to zero. For a primitive elastic graph this means that this embedment is a harmonic function on graph: its value in each non-terminal vertex is a mean of the value in the closest neighbors of this vertex. For non-primitive graphs we can consider stars which include not all neighbors of their centers. For example, for a square lattice we create elastic graph (elastic net) using 2-stars (ribs): all vertical 2-stars and all horizontal 2-stars. For such elastic net, each non-boundary vertex belongs to two stars. For a general elastic graph G with sets of k-stars k S we introduce the following notion of pluriharmoning function. Definition. A map V Rm defined on vertices of G is pluriharmonic iff for any k-star k j k S S ) ( with the central vertex ) ( j k S (0) and the neighbouring vertices ) ( j k S (i), i = 1...k, the equality holds: k i j k j k i S k S 1 ) ( ) ( )) ( ( 1 )) 0 ( ( . (7) Pluriharmonic maps generalize the notion of linear map and of harmonic map, simultaneously. For example: 1) 1D harmonic functions are linear; 2) If we consider an nD cubic lattice as a primitive graph (with 2n-stars for all non-boundary vertices), then the correspondent pluriharmonic functions are just harmonic ones; 3) If we create from nD cubic lattice a standard nD elastic net with 2-stars (each non-boundary vertex is a center of n 2-stars, one 2-stars for each coordinate direction), then pluriharmonic functions are linear. Pluriharmonic functions have many attractive properties, for example, they satisfy the following maximum principle. A vertex v of an elastic graph is called a corner point or an extreme point of G iff v is not a centre of any k-star from k S for all k>0. Theorem. Let V Rm be a pluriharmonic map, F be a convex function on Rm, and a = maxxVF((x)). Then there is a corner point v of G such that F( v))=a. Convex functions achieve their maxima in corner points. Even a particular case of this theorem with linear functions F is quite useful. Linear functions achieve their maxima and minima in corner points. In the theory of principal curves and manifolds the penalty functions were introduced to penalise deviation from linear manifolds (straight lines or planes). We proposed to use pluriharmonic embeddings (pluriharmonic graphs) as ideal objects instead of manifolds and to introduce penalty (5) for deviation from this ideal form. GRAPH GRAMMARS AND THREE TYPES OF COMPLEXITY FOR PRINCIPAL GRAPHS Principal graphs can be called data approximators of controllable complexity. By complexity of the principal objects we mean the following three notions: 1) Geometric complexity: how far a principal object deviates from its ideal configuration; for the elastic principal graphs we explicitly measure deviation from the ideal pluriharmonic graph by the elastic energy U(G) (3) (this complexity may be considered as a measure of non-linearity); 2) Structural complexity measure: it is some non-decreasing function of the number of vertices, edges and k-stars of different orders SC(G)=SC(|V|,|E|,|S2|,,|Sm|); this function penalises for number of structural elements; 3) Construction complexity is defined with respect to a graph grammar as a number of applications of elementary transformations necessary to construct given G from the simplest graph (one vertex, zero edges). The construction complexity is defined with respect to a grammar of elementary transformation. The graph grammars (Lwe, 1993; Nagl, 1976) provide a well- developed formalism for the description of elementary transformations. An elastic graph grammar is presented as a set of production (or substitution) rules. Each rule has a form A B, where A and B are elastic graphs. When this rule is applied to an elastic graph, a copy of A is removed from the graph together with all its incident edges and is replaced with a copy of B with edges that connect B to the graph. For a full description of this language we need the notion of a labeled graph. Labels are necessary to provide the proper connection between B and the graph (Nagl, 1976). An approach based on graph grammars to constructing effective approximations of an elastic principal graph has been recently proposed (Gorban et al, 2007). Let us define graph grammar O as a set of graph grammar operations O={o1,..,os}. All possible applications of a graph grammar operation oi to a graph G gives a set of transformations of the initial graph oi(G) = {G1, G2, , Gp}, where p is the number of all possible applications of oi to G. Let us also define a sequence of r different graph grammars }} ,..., { , , } ,..., { { ) ( ) ( 1 ) ( ) 1 ( ) 1 ( 1 ) 1 ( 1 r s r r s r o o O o o O . Let us choose a grammar of elementary transformations, predefined boundaries of structural complexity SCmax and construction complexity CCmax , and elasticity coefficients i and kj . Definition. Elastic principal graph for a dataset X is such an elastic graph G embedded in the Euclidean space by the map V Rm that SC(G) SCmax , CC(G) CCmax , and U(G) min over all possible elastic graphs G embeddings in Rm . Algorithm for estimating the elastic principal graph 1) Initialize the elastic graph G by 2 vertices v1 and v2 connected by an edge. The initial map is chosen in such a way that (v1) and (v2) belong to the first principal line in such a way that all the data points are projected onto the principal line segment defined by (v1), (v2); 2) For all j=1r repeat steps 3-6: 3) Apply all grammar operations from O(j) to G in all possible ways; this gives a collection of candidate graph transformations {G1, G2, }; 4) Separate {G1, G2, } into permissible and forbidden transformations; permissible transformation Gk is such that SC(Gk) SCmax , where SCmax is some predefined structural complexity ceiling; 5) Optimize the embedment and calculate the elastic energy U(G) of graph embedment for every permissible candidate transformation, and choose such a graph Gopt that gives the minimal value of the elastic functional: ) ( inf arg k set e permissibl G opt G U G k ; 6) Substitute G Gopt ; 7) Repeat steps 2-6 until the set of permissible transformations is empty or the number of operations exceeds a predefined number the construction complexity. PRINCIPAL TREES AND METRO MAPS Let us construct the simplest non-trivial type of the principal graphs, called principal trees. For this purpose let us introduce a simple Add a node, bisect an edge graph grammar (see Fig. 2) applied for the class of primitive elastic graphs. Definition. Principal tree is an acyclic primitive elastic principal graph. Definition. Remove a leaf, remove an edge graph grammar O(shrink) applicable for the class of primitive elastic graphs consists of two operations: 1) The transformation remove a leaf can be applied to any vertex v of G with connectivity degree equal to 1: remove v and remove the edge (v,v) connecting v to the tree; 2) The transformation remove an edge is applicable to any pair of graph vertices v, v connected by an edge (v, v): delete edge (v, v), delete vertex v, merge the k-stars for which v and v are the central nodes and make a new k-star for which v is the central node with a set of neighbors which is the union of the neighbors from the k-stars of v and v. Fig. 2. Illustration of the simple add node to a node or bisect an edge graph grammar. a) We start with a simple 2-star from which one can generate three distinct graphs shown. The Op1 operation is adding a node to a node, operations Op1 and Op2 are edge bisections (here they are topologically equivalent to adding a node to a terminal node of the initial 2-star). For illustration let us suppose that the Op2 operation gives the biggest elastic energy decrement, thus it is the optimal operation. b) From the graph obtained one can generate 5 distinct graphs and choose the optimal one. c) The process is continued until a definite number of nodes are inserted. Definition. Add a node, bisect an edge graph grammar O(grow) applicable for the class of primitive elastic graphs consists of two operations: 1) The transformation add a node can be applied to any vertex v of G: add a new node z and a new edge (v, z); 2) The transformation bisect an edge is applicable to any pair of graph vertices v, v connected by an edge (v, v): delete edge (v, v), add a vertex z and two edges, (v, z) and (z, v). The transformation of the elastic structure (change in the star list) is induced by the change of topology, because the elastic graph is primitive. Consecutive application of the operations from this grammar generates trees, i.e. graphs without cycles. Also we should define the structural complexity measure SC(G)=SC(|V|,|E|,|S2|,,|Sm|). Its concrete form depends on the application field. Here are some simple examples: 1) SC(G) = |V| : i.e., the graph is considered more complex if it has more vertices; 2) otherwise , 0 and | | if |, | = ) SC( 4 max 3 3 m k kS b S S G , i.e., only bmax simple branches (3-stars) are allowed in the principal tree. h) Fig. 3. Principal manifold and principal tree for the Iris dataset. a) View of the principal manifold projected on the first two principal components, the data points are shown projected into the closest vertex of the elastic net; b) visualization of data points in the internal coordinates, here classes are represented in the form of Hinton diagrams: the size of the diagram is proportional to the number of points projected, the shape of the diagram denote three different point classes; c) same as a), but the data points are shown projected into the closest point of the piecewise linearly interpolated elastic map; d) same as b), but based on projection shown in c); e)-g) First 50 iterations of the principal tree algorithm, the tree is shown projected onto the principal plane; h) metro map representation of the Iris dataset. Using the sequence {O(grow), O(grow), O(shrink)} in the above-described algorithm for estimating the elastic principal graph gives an approximation to the principal trees. Introducing the tree trimming grammar O(shrink) allows to produce principal trees closer to the global optimum, trimming excessive tree branching and fusing k-stars separated by small bridges. Principal trees can have applications in data visualization. A principal tree is embedded into a multidimensional data space. It approximates the data so that one can project points from the multidimensional space into the closest node of the tree. The tree by its construction is a one-dimensional object, so this projection performs dimension reduction of the multidimensional data. The question is how to produce a planar tree layout? Of course, there are many ways to layout a tree on a plane without edge intersection. But it would be useful if both local tree properties and global distance relations would be represented using the layout. We can require that 1) In a two-dimensional layout, all k-stars should be represented equiangular; this is the small penalty configuration; 2) The edge lengths should be proportional to their length in the multidimensional embedding; thus one can represent between-node distances. This defines a tree layout up to global rotation and scaling and also up to changing the order of leaves in every k-star. We can change this order to eliminate edge intersections, but the result can not be guaranteed. In order to represent the global distance structure, it was found (Gorban et al., 2008) that a good approximation for the order of k-star leaves can be taken from the projection of every k-star on the linear principal plane calculated for all data points, or on the local principal plane in the vicinity of the k-star, calculated only for the points close to this star. The resulting layout can be further optimized using some greedy optimization methods. The point projections are then represented as pie diagrams, where the size of the diagram reflects the number of points projected into the corresponding tree node. The sectors of the diagram allow us to show proportions of points of different classes projected into the node (see an example on Fig. 3). This data display was called a metro map since it is a schematic and idealized representation of the tree and the data distribution with inevitable distortions made to produce a 2D layout. However, using this map one can still estimate the distance from a point (tree node) to a point passing through other points. This map is inherently unrooted (as a real metro map). It is useful to compare this metaphor with trees produced by hierarchical clustering where the metaphor is closer to a genealogy tree. PRINCIPAL CUBIC COMPLEXES Elastic nets introduced above are characterized by their internal dimension dim(G). The way to generalize these characteristics on other elastic graphs is to utilize the notion of cubic complex (Gorban et al, 2007). Definition. Elastic cubic complex K of internal dimension r is a Cartesian product G1 Gr of elastic graphs G1, . . .Gr . It has the vertex set V1 . . . Vr. Let 1 i r and vj Vj (j i). For this set of vertices, {vj}j i, a copy of Gi in G1 ... Gr is defined with vertices (v1, , vi1, v, vi+1, , vr) (v Vi), edges ((v1, , vi1, v, vi+1, , vr), (v1, , vi1, v, vi+1, , vr)), (v, v) Ei , and, similarly, k-stars of the form (v1, , vi1, Sk, vi+1, , vr), where Sk is a k-star in Gi. For any Gi there are i j j V | | copies of Gi in G. Sets of edges and k-stars for Cartesian product are unions of that set through all copies of all factors. A map : V1 . . . Vr Rm maps all the copies of factors into Rm too. Remark. By construction, the energy of the elastic graph product is the energy sum of all factor copies. It is, of course, a quadratic functional of . If we approximate multidimensional data by an r-dimensional object, the number of points (or, more generally, elements) in this object grows with r exponentially. This is an obstacle for grammarbased algorithms even for modest r, because for analysis of the rule A B applications we should investigate all isomorphic copies of A in G. Introduction of a cubic complex is useful factorization of the principal object which allows to avoid this problem. The only difference between the construction of general elastic graphs and factorized graphs is in the application of the transformations. For factorized graphs, we apply them to factors. This approach significantly reduces the amount of trials in selection of the optimal application. The simple grammar with two rules, add a node to a node, or bisect an edge, is also powerful here, it produces products of primitive elastic trees. For such a product, the elastic structure is defined by the topology of the factors. INCOMPLETE DATA Some of the methods described above allow us to use incomplete data in a natural way. Let us represent an incomplete observation by ) @,..., @,..., ,..., ( 1 m x x x , where the @ symbol denotes a missing value. Definition. Scalar product between two incomplete observations x and y is m i i i y x @ ) , ( y x . Then the Euclidean distance is m i i i y x @ 2 2 ) ( ) ( y x . Remark. This definition has a very natural geometrical interpretation: an incomplete observation with k missing values is represented by a kdimensional linear manifold Lk, parallel to k coordinate axes corresponding to the missing data. Thus, any method which uses only scalar products or/and Euclidean distances can be applied for incomplete data with some minimal modifications subject to random and not too dense distribution of missing values in X. For example, the iterative method for SVD for incomplete data matrix was developed (Roweis (1998); Gorban & Rossiev, 1999). There are, of course, other approaches to incomplete data in unsupervised learning (for example, those presented by Little & Rubin (1987)). IMPLICIT METHODS Most of the principal objects introduced in this paper are constructed as explicit geometrical objects embedded in Rm to which we can calculate the distance from any object in X. In this way, they generalize the data approximation-based (#1) and the variation-maximization-based (#2) definitions of linear PCA. There also exists the whole family of methods, which we only briefly mention here, that generalize the distance distortion minimization definition of PCA (#3). First, some methods take as input a pairwise distance (or, more generally, dissimilarity) matrix D and construct such a configuration of points in a low- dimensional Euclidean space that the distance matrix D in this space reproduce D with maximal precision. The most fundamental in this series is the metric multidimensional scaling (Kruskal, 1964). The next is the Kernel PCA approach (Schlkopf et al., 1997) which takes advantage of the fact that for the linear PCA algorithm one needs only the matrix of pairwise scalar products (Gramm matrix) but not the explicit values of coordinates of X. It allows to apply the kernel trick (Aizerman et al., 1964) and substitute the Gramm matrix by the scalar products calculated with use of some kernel functions. Kernel PCA method is tightly related to the classical multidimensional scaling (Williams, 2002). Local Linear Embedding or LLE (Roweis & Saul, 2000) searches for such a NN matrix A that approximates given xi by a linear combination of n vectors- neighbours of xi: min || || 2 1 1 N i N k k i k i A x x , where only such 0 i k A , if k is one of the n closest to xi vectors. After one constructs such a configuration of points in Rs, s << m, that N k k i k i A 1 y y , yiRs , for all i = 1N. The coordinates of such embedding are given by the eigenvectors of the matrix (1-A)T(1-A). ISOMAP (Tenenbaum et al., 2000) and Laplacian eigenmap (Belkin & Niyogi, 2003; Nadler et al., 2008) methods start with construction of the neighbourhood graph, i.e. the graph in which close in some sense data points are connected by (weighted) edges. This weighted graph can be represented in the form of a weighted adjacency matrix W= {Wij}. From this graph, ISOMAP constructs a new distance matrix D(ISOMAP), based on the path lengths between two points in the neighbourhood graph, and the multidimensional scaling is applied to D(ISOMAP). The Laplacian map solves the eigenproblem f f S L , where } , , { 1 1 0 N j Nj N j j W W diag S , L = S W is the Laplacian matrix. The trivial constant solution corresponding to the smallest eigenvalue = 0 is discarded, while the elements of the eigenvectors s f f f , , , 2 1 , where s ... 2 1 , give the s-dimensional projection of xi, i.e. P(xi)= { ) ( , ), ( ), ( 2 1 i i i s f f f }. Finally, one can implicitly construct projections into smaller dimensional spaces by training auto-associative neural networks with narrow hidden layer. An overview of the existing Neural PCA methods can be found in the recent collection of review papers (Gorban et al, 2008). EXAMPLE: PRINCIPAL OBJECTS FOR THE IRIS DATASET On Fig. 3 we show application of the elastic principal manifolds and principal trees algorithms to the standard Iris dataset (Fisher, 1936). As expected, two- dimensional approximation of the principal manifold in this case is close to the linear principal plane. One can also see that the principal tree illustrates well the fact of almost complete separation of classes in data space. EXAMPLE: PRINCIPAL OBJECTS FOR MOLECULAR SURFACES A molecular surface defines the effective region of space which is occupied by a molecule. For example, the Van-der-Waals molecular surface is formed by surrounding every atom in the molecule by a sphere of radius equal to the characteristic radius of the Van-der-Waals force. After all the interior points are eliminated, this forms a complicated non-smooth surface in 3D. In practice, this surface is sampled by a finite number of points. Using principal manifolds methodology, we constructed a smooth approximation of such molecular surface for a small piece of a DNA molecule (several nucleotides long). First, we have made an approximation of this dataset by a 1D principal curve. Interestingly, this curve followed the backbone of the molecule, forming a helix (see Fig. 4). Second, we approximated the molecular surface by a 2D manifold. The topology of the surface is expected to be spherical, so we applied spherical topology of the elastic net for optimisation. Fig. 4. Principal objects approximating molecular surface of a short stretch of DNA molecule. a) stick-and-balls model of the DNA stretch and the initial molecular surface (black points); b) one- and two-dimensional spherical principal manifolds for the molecular surface; c) simple principal cubic complex (product of principal trees) which does not have any branching in this case. We should notice that since it is impossible to make the lengths of all edges equal for the spherical grid, corrections were performed for the edge elasticities during the grid initialization (shorter edges are given larger is). Third, we applied the method for constructing principal cubic complexes, namely, graph product of principal trees, which produced somewhat trivial construction (because no branching was energetically optimal): product of two short elastic principal curves, forming a double helix. Fig. 5. Seven cluster structures presented for 4 selected genomes. A genome is represented as a collection of points (text fragments represented by their triplet frequencies) in the 64-multidimensional space. Color codes denote point classes corresponding to 6 possible frameshifts when a random fragment overlaps with a coding gene (3 in the forward and 3 in the backward direction of the gene), and the black color corresponds to non-coding regions. For every genome a principal tree (metro map layout) is shown together with 2D PCA projection of the data distribution. Note that the clusters that appear to be mixed on the PCA plot for Escherichia coli (they remain mixed in 3D PCA as well) are well separated on the metro map. This proves that they are well-separated in R64. EXAMPLE: PRINCIPAL OBJECTS DECIPHER GENOME A dataset X can be constructed for a string sequence using a short word frequency dictionary approach in the following way: 1) the notion of word is defined; 2) the set of all possible short words is defined, let us say that we have m of them; 3) a number N of text fragments of certain width is sampled from the text; 4) in each fragment the frequency of occurrences of all possible short words is calculated and, thus, each fragment is represented as a vector in multidimensional space Rm. The whole text then is represented as a dataset of N vectors embedded in Rm. We systematically applied this approach to available bacterial genomic sequences (Gorban & Zinovyev, 2008b). In our case we defined: 1) a word is a sequence of three letters from the {A,C,G,T} alphabet (triplet); 2) evidently, there are 64 possible triplets in the {A,C,G,T} alphabet; 3) we sampled 5000-10000 fragments of width 300 from a genomic sequence; 4) we calculated the frequencies of non- overlapping triplets for every fragment. The constructed datasets are interesting objects for data-mining, because 1) they have a non-trivial cluster structure which usually contains various configurations of 7 clusters (see Fig. 5); 2) class labels can be assigned to points accordingly to available genome annotations; in our case we put information about presence (in one of six possible frameshifts) or absence of the coding information in the current position of a genome; 3) using data mining techniques here has immediate applications in the field of automatic gene recognition and in others, see, for example, (Carbone et al, 2003). On Fig. 5 we show application of both classical PCA and the metro map methods for several bacterial genomes. Look at http://www.ihes.fr/~zinovyev/7clusters web-site for further information. EXAMPLE: NON-LINEAR PRINCIPAL MANIFOLDS FOR MICROARRAY DATA VISUALIZATION DNA microarray data is a rich source of information for molecular biology (an expository overview is provided by Leung & Cavalieri (2003)). This technology found numerous applications in understanding various biological processes including cancer. It allows to screen simultaneously the expression of all genes in a cell exposed to some specific conditions (for example, stress, cancer, treatment, normal conditions). Obtaining a sufficient number of observations (chips), one can construct a table of \"samples vs genes\", containing logarithms of the expression levels of, typically several thousands (n) of genes, in typically several tens (m) of samples. On Fig. 6 we provide a comparison of data visualization scatters after projection of the breast cancer dataset, provided by Wang et al. (2003), onto the linear two- and non-linear two-dimensional principal manifold. The latter one is constructed by the elastic maps approach. Each point here represents a patient treated from cancer. Before dimension reduction it is represented as a vector in Rn, containing the expression values for all n genes in the tumor sample. Linear and non-linear 2D principal manifolds provide mappings Rn R2, drastically reducing vector dimensions and allowing data visualization. The form, the shape and the size of the point on the Fig.6 represent various clinical data (class labels) extracted from the patients disease records. Practical experience from bioinformatics studies shows that two-dimensional data visualization using non-linear projections allow to catch more signals from data (in the form of clusters or specific regions of higher point density) than linear projections, see Fig. 6 and a good example by Ivakhno & Armstrong (2008). Figure 6. Visualization of breast cancer microarray dataset using elastic maps. Ab initio classifications are shown using points size (ER, estrogen receptor status), shape (Group A patients with aggressive cancer, Group B patients with non-aggressive cancer) and color (TYPE, molecular type of breast cancer). a) Configuration of nodes projected into the three-dimensional principal linear manifold. One clear feature is that the dataset is curved such that it can not be mapped adequately onto a two-dimensional principal plane. b) The distribution of points in the internal non-linear manifold coordinates is shown together with estimation of the two-dimensional density of points. c) The same as b) but for the linear two-dimensional manifold. One can notice that the ``basal'' breast cancer subtype is much better separated on the non-linear mapping and some features of the distribution become better resolved. In addition to that, Gorban & Zinovyev (2008a) performed a systematic comparison of performance of low-dimensional linear and non-linear principal manifolds for microarray data visualization, using the following four criteria: 1) mean-square distance error; 2) distortions in mapping the big distances between points; 3) local point neighbourhood preservation; 4) compactness of point class labels after projection. It was demonstrated that non-linear two-dimensional principal manifolds provide systematically better results accordingly to all these criteria, achieving the performance of three- and four- dimensional linear principal manifolds (principal components). The interactive ViMiDa (Visualization of Microarray Data) and ViDaExpert software allowing microarray data visualization with use of non-linear principal manifolds are available on the web-site of Institut Curie (Paris): http://bioinfo.curie.fr/projects/vidaexpert and http://bioinfo.curie.fr/projects/vimida. CONCLUSION In this chapter we gave a brief practical introduction into the methods of construction of principal objects, i.e. objects embedded in the middle of the multidimensional data set. As a basis, we took the unifying framework of mean squared distance approximation of finite datasets which allowed us to look at the principal graphs and manifolds as generalizations of the mean point notion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_text = df_with_text.drop(\"paper_text\",axis=1);\n",
        "df_with_text =df_with_text.rename(columns={\"cleaned_paper_text\":\"paper_text\"})\n",
        "df_with_text.to_csv(\"sample_5000_with_cleaned_text.csv\", index=False)"
      ],
      "metadata": {
        "id": "TnVkujjh-oZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"sample_5000_with_cleaned_text.csv\")"
      ],
      "metadata": {
        "id": "MhY4EyskOi1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['author','pdf_link','tags'])"
      ],
      "metadata": {
        "id": "uNzp461ixdR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Data Analysis"
      ],
      "metadata": {
        "id": "vb5o2mmJ4gQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate word counts\n",
        "df['summary_word_count'] = df['summary'].apply(lambda x: len(str(x).split()))\n",
        "df['paper_text_word_count'] = df['paper_text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Calculate upper whisker for paper_text_word_count\n",
        "q1 = np.percentile(df['paper_text_word_count'], 25)\n",
        "q3 = np.percentile(df['paper_text_word_count'], 75)\n",
        "iqr = q3 - q1\n",
        "upper_whisker = q3 + 1.5 * iqr\n"
      ],
      "metadata": {
        "id": "1pjHJfyjxflo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['paper_text_word_count'] <= upper_whisker]\n",
        "df=df.drop(columns=['paper_text_word_count','summary_word_count'])\n",
        "print(df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr2z80xdxloM",
        "outputId": "0a50075e-bf15-4cf3-b8b2-6d36f360cd69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4817, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate evaluate rouge_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIqSJ0baxq9J",
        "outputId": "f69e8b1b-c39c-4ee3-cf9f-59b734d6069a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=1fbd62d6b1cd287e30a93143bb6bb26025fa76d0fb80d5a2e35dfb7eee841927\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, rouge_score, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge_score-0.1.2 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Based preprocessing"
      ],
      "metadata": {
        "id": "ScL2SmXH4vVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess_for_t5(df):\n",
        "    df = df[[\"paper_text\", \"summary\"]].dropna()\n",
        "    df[\"input_text\"] = \"summarize: \" + df[\"paper_text\"].str.strip()\n",
        "    df[\"target_text\"] = df[\"summary\"].str.strip()\n",
        "    return df[[\"input_text\", \"target_text\"]]\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "processed_df = preprocess_for_t5(df)\n",
        "dataset = Dataset.from_pandas(processed_df)\n"
      ],
      "metadata": {
        "id": "FuZQGlTkxwl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "eXmH6KuVx_XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL4F2VujyKfy",
        "outputId": "69831959-45e5-4ff5-f842-9bb4edeb7951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
              "        num_rows: 3852\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
              "        num_rows: 963\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = dataset['test'][0]['input_text']\n",
        "reference = dataset['test'][0]['target_text']\n",
        "print(reference)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYu1V9_SzZDi",
        "outputId": "f4cca379-e7fe-4977-ba3f-137a6cf2bb40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Nystrom method is an efficient technique to speed up large-scale learning\n",
            "applications by generating low-rank approximations. Crucial to the performance\n",
            "of this technique is the assumption that a matrix can be well approximated by\n",
            "working exclusively with a subset of its columns. In this work we relate this\n",
            "assumption to the concept of matrix coherence and connect matrix coherence to\n",
            "the performance of the Nystrom method. Making use of related work in the\n",
            "compressed sensing and the matrix completion literature, we derive novel\n",
            "coherence-based bounds for the Nystrom method in the low-rank setting. We then\n",
            "present empirical results that corroborate these theoretical bounds. Finally,\n",
            "we present more general empirical results for the full-rank setting that\n",
            "convincingly demonstrate the ability of matrix coherence to measure the degree\n",
            "to which information can be extracted from a subset of columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"] = dataset[\"train\"].select(range(900))\n",
        "dataset[\"test\"] = dataset[\"test\"].select(range(100))"
      ],
      "metadata": {
        "id": "KoRC57tU-fpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparision of LLMs\n"
      ],
      "metadata": {
        "id": "0qhnpG5R466y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t5-small"
      ],
      "metadata": {
        "id": "Vx5m4mZR5CII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration\n",
        "from evaluate import load\n",
        "\n",
        "# Load ROUGE evaluation metric\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "# Load T5-small model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Initialize summarizer pipeline\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=-1)  # Use device=-1 for CPU\n",
        "\n",
        "try:\n",
        "    # Perform summarization\n",
        "    summary_output = summarizer(text, max_length=1024, min_length=50, do_sample=False)\n",
        "    summary = summary_output[0]['summary_text']\n",
        "except Exception as e:\n",
        "    print(\"Error during summarization:\", e)\n",
        "    summary = None\n",
        "\n",
        "# Compute ROUGE scores if summary is generated\n",
        "if summary:\n",
        "    scores = rouge.compute(\n",
        "        predictions=[summary],\n",
        "        references=[reference],\n",
        "        use_stemmer=True\n",
        "    )\n",
        "    print(\"ROUGE scores:\", scores)\n",
        "else:\n",
        "    print(\"Summarization failed, skipping ROUGE computation.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yXYvXI1zPXU",
        "outputId": "a693644d-899b-4620-b8b8-19b2aee2df18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (7983 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE scores: {'rouge1': np.float64(0.3627906976744186), 'rouge2': np.float64(0.1971830985915493), 'rougeL': np.float64(0.21395348837209308), 'rougeLsum': np.float64(0.3627906976744186)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "YQnyXpyD0MuU",
        "outputId": "f260a8a7-253b-4fc8-deda-f278c5855222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we propose a novel extension to ADAN by adding a topic-specic utterance . we measure the ability of a bot to converse on a wide range of topics . a ne-grained conversational bot is able to sustain coherent conversations . this is based on the data from the internal Question data .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bart-large-cnn\n"
      ],
      "metadata": {
        "id": "COMV31lx5Fm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from evaluate import load\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "# Use CPU\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
        "\n",
        "\n",
        "ttext = text[:2000]\n",
        "\n",
        "try:\n",
        "    summary_output = summarizer(ttext, max_length=1024, min_length=50, do_sample=False)\n",
        "    summary = summary_output[0]['summary_text']\n",
        "except Exception as e:\n",
        "    print(\"Error during summarization:\", e)\n",
        "    summary = None\n",
        "\n",
        "\n",
        "reference = dataset['test'][0]['target_text']\n",
        "\n",
        "if summary:\n",
        "    scores = rouge.compute(\n",
        "        predictions=[summary],\n",
        "        references=[reference],\n",
        "        use_stemmer=True\n",
        "    )\n",
        "    print(\"ROUGE scores:\", scores)\n",
        "else:\n",
        "    print(\"Summarization failed, skipping ROUGE computation.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiVnqA_ATrQx",
        "outputId": "b5597acf-9cec-463f-fbdc-a408acf0d0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 1024, but your input_length is only 417. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=208)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE scores: {'rouge1': np.float64(0.41314553990610325), 'rouge2': np.float64(0.33175355450236965), 'rougeL': np.float64(0.35680751173708924), 'rougeLsum': np.float64(0.41314553990610325)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "No9IAKrbjCW0",
        "outputId": "ca4a2577-6839-4ba6-a75e-2c7e29735180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Achieving sustained, coherent and engaging dialog is the next frontier for conversational AI. We propose to evaluate dialog quality using topic-based metrics. We compare our proposed topic based metrics with the ratings provided by users. We show that our metrics both correlate with and complement human judgment.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### long-t5-tglobal-base"
      ],
      "metadata": {
        "id": "ADXkedKS5ST7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import torch\n",
        "\n",
        "# Use CPU if no GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device set to:\", device)\n",
        "\n",
        "# Load LongT5 model & tokenizer\n",
        "model_name = \"google/long-t5-tglobal-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    max_length=4096,  # or 2048 if needed\n",
        "    padding=\"max_length\"\n",
        ").to(device)\n",
        "\n",
        "# Generate summary\n",
        "try:\n",
        "    summary_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=512,\n",
        "        min_length=50,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    print(\"Generated Summary:\\n\", summary)\n",
        "except Exception as e:\n",
        "    print(\"Error during summarization:\", e)\n",
        "\n",
        "# Compute ROUGE\n",
        "from evaluate import load\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "scores = rouge.compute(\n",
        "    predictions=[summary],\n",
        "    references=[reference],\n",
        "    use_stemmer=True\n",
        ")\n",
        "print(\"ROUGE scores:\", scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GnsAvy1VN1y",
        "outputId": "b27833ff-641b-447f-e284-6204228c06ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to: cuda\n",
            "Generated Summary:\n",
            " summarize: Topic-based Evaluation for Conversational Bots Fenfei Guo, 1 Angeliki Metallinou,2 Chandra Khatri,2 Anirudh Raju,2 Anu Venkatesh,2 Ashwin Ram2 1University of Maryland, Department of Computer Science and UMIACS, 2Amazon Alexa fenfeigo@cs.umd.edu, ametalli,ckhatri,ranirudh,anuvenk,ashwram@amazon.com Abstract Dialog evaluation is a challenging problem, especially for non task-oriented dialogs where conversational success is not well-dened. The utterance representation sk per topic is computed through weighted average: [k,1, , k,L] = softmax([wk,1, , wk,L]), sk = 1 L L X i=1 k,iei (1) In Figure 2, the topic-specic sentence representation S is a K D matrix where each row is a topic specic representation sk and D is the embedding dimension. Assuming an utterance [e1, , eL] of length L, where ei is the embedding of the ith word, we compute the attention weights [k,1, , k,L] by normalizing the saliency wk,i with a softmax function (we normalize across utterance of length L as opposed to across the whole vocabulary to reduce computation cost). The utterance representation sk per topic is computed through weighted average: [k,1, , k,L] = softmax([wk,1, , wk,L]), sk = 1 L L X i=1 k,iei (1) In Figure 2, the topic-specic sentence representation S is a K D matrix where each row is a topic specic representation sk and D is the embedding dimension. The utterance representation sk per topic is computed through weighted average: [k,1, , k,L] = softmax([wk,1, , wk,L]), sk = 1 L\n",
            "ROUGE scores: {'rouge1': np.float64(0.30516431924882625), 'rouge2': np.float64(0.10849056603773585), 'rougeL': np.float64(0.18309859154929578), 'rougeLsum': np.float64(0.2723004694835681)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "fkyHAlefW2-W",
        "outputId": "ff155cfa-776d-40e7-c950-7691acae837b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'summarize: Topic-based Evaluation for Conversational Bots Fenfei Guo, 1 Angeliki Metallinou,2 Chandra Khatri,2 Anirudh Raju,2 Anu Venkatesh,2 Ashwin Ram2 1University of Maryland, Department of Computer Science and UMIACS, 2Amazon Alexa fenfeigo@cs.umd.edu, ametalli,ckhatri,ranirudh,anuvenk,ashwram@amazon.com Abstract Dialog evaluation is a challenging problem, especially for non task-oriented dialogs where conversational success is not well-dened. The utterance representation sk per topic is computed through weighted average: [k,1, , k,L] = softmax([wk,1, , wk,L]), sk = 1 L L X i=1 k,iei (1) In Figure 2, the topic-specic sentence representation S is a K D matrix where each row is a topic specic representation sk and D is the embedding dimension. Assuming an utterance [e1, , eL] of length L, where ei is the embedding of the ith word, we compute the attention weights [k,1, , k,L] by normalizing the saliency wk,i with a softmax function (we normalize across utterance of length L as opposed to across the whole vocabulary to reduce computation cost). The utterance representation sk per topic is computed through weighted average: [k,1, , k,L] = softmax([wk,1, , wk,L]), sk = 1 L L X i=1 k,iei (1) In Figure 2, the topic-specic sentence representation S is a K D matrix where each row is a topic specic representation sk and D is the embedding dimension. The utterance representation sk per topic is computed through weighted average: [k,1, , k,L] = softmax([wk,1, , wk,L]), sk = 1 L'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### comparing the above 3 LLMs performance"
      ],
      "metadata": {
        "id": "EfudCzbW5WZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ROUGE scores for each model\n",
        "scores_t5 = [0.3627906976744186, 0.1971830985915493, 0.21395348837209308, 0.3627906976744186]\n",
        "scores_bart = [0.41314553990610325, 0.33175355450236965, 0.35680751173708924, 0.41314553990610325]\n",
        "scores_longT5 = [0.30516431924882625, 0.10849056603773585, 0.18309859154929578, 0.2723004694835681]\n",
        "\n",
        "score_types = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-Lsum']\n",
        "\n",
        "x = np.arange(len(score_types))\n",
        "\n",
        "width = 0.2\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.bar(x - width, scores_t5, width, label='T5', color='b')\n",
        "ax.bar(x, scores_bart, width, label='BART', color='g')\n",
        "ax.bar(x + width, scores_longT5, width, label='LongT5', color='r')\n",
        "\n",
        "ax.set_xlabel('ROUGE Score Type')\n",
        "ax.set_ylabel('ROUGE Score')\n",
        "ax.set_title('Comparison of ROUGE Scores for T5, BART, and LongT5')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(score_types)\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "yFpamyY02PcE",
        "outputId": "d24156b3-24e1-49be-f9e3-158c7fd982c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above Bar graph it clearly shows that Bart LLM as outperformed other 2 LLMs"
      ],
      "metadata": {
        "id": "_SytNmyZ5hUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning T5-small model"
      ],
      "metadata": {
        "id": "l538NSHO5xGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "\n",
        "def tokenize(example):\n",
        "    inputs = tokenizer(\n",
        "        example[\"input_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=384\n",
        "    )\n",
        "    targets = tokenizer(\n",
        "        example[\"target_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Proper way to add labels for batched input\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318,
          "referenced_widgets": [
            "6ce9ea08be334953b6bf999a6fe6ada5",
            "1d0e7aeda1174f13823ea9ef0ff2afe6",
            "c60c53b5a24448b9adbb69ed90a93992",
            "802f9d0e4f53417cae064106140e065e",
            "d0cf02904e5b4681a24851809f233a2e",
            "d10542ca38984ad0a98324b1558fb12f",
            "585440e64c9f4597b477e5c7984ce300",
            "64f41a26ebca4532aa9ffe35db3609d3",
            "398c1fe23e284363b6f015a4b171141f",
            "7dbb71d025ea49cd8e46a41239aebc02",
            "93b29d6d13fb4b0a9bcdd47cd002b916",
            "97753a604737418395aff4c108ffa887",
            "eda6ee03d2ed43eebcd5e87d09ddc15a",
            "0e917a4b3dd54f90a2ac871614491753",
            "0532cf790f404eefa45007ab1df4e82e",
            "5cc5aaba0a534dacb048766dbe57b340",
            "e25b88960fe541c6a63e0edf4e28f420",
            "8f64ec3713134961b9e40525f1fdcc57",
            "777addac3e3541e39ebf652c7f736605",
            "d65dead427994f84902128f914b7e5ba",
            "05938bed8cf04192a912d63776ee3533",
            "35267f0476054d1f967de14e5d71371f",
            "f3476f43db6549b79852ca7dd734f1d3",
            "e6c36a774c994c0ab49514b7e3047cda",
            "865ab2c6005c4dc197bcc301291822ca",
            "1d5e9616a1be4153820fe01deff772bd",
            "015c8b2863b548c88a41b6ee46af3ebe",
            "3f5e6ec4f9c2444f895ee57c141ddadb",
            "be20ca3ae3b940c2a69fec5205ce1013",
            "897a7ff873224e6b88ab4f7b1c7f0c35",
            "e6a517bf022e4d468265755c145ba96b",
            "0295937399fc4d488d2631b51432a3cc",
            "e2ef7401d1e34efc824914be4045bd7d",
            "3c1fc49d373743f091da47dddf358e8d",
            "5c887fa8ca05423cb320ae97bfabd3b9",
            "2fbeff637aa24505bcf43abdebaeab66",
            "10ed113627e04e7581d6d367a0dfdffe",
            "b0fcf7a8e1214f7b9a30a6ea7775da8b",
            "bbb0f921670042e1bf26396916be3a4d",
            "31cf3c5c03b74e6083e3fc3846041c35",
            "f57514db582b48359721521162062e81",
            "07ce8b9cf7d1467783dc0704b1120391",
            "724a28e7828b4062a66361b532ae480d",
            "16ae5b2c275c4e368aad7371408fa586",
            "43c2af1429dc43e38ec9c15855be21d8",
            "5e81912b099745a58bc83763b26fcd68",
            "870eaa0d069047d4ab322457f36cbee8",
            "410d1feb88664bcebdea5de3ca16d3c3",
            "5b20b4f029f04327b0b7745b483361d3",
            "3877b5c97bd24e52b12dafa4d9b1e80d",
            "f0ebc3f9f3a94a9e9543af9e7e6a0cb4",
            "2ba8a9673d2a419987d0905b845ace6f",
            "4da8a3cc68f441818a2da2f967cd7b46",
            "3d697d8bdebf432aafdcfaf0d1b90fdc",
            "b539b3cd4b54487892e101a305c549fc"
          ]
        },
        "id": "NYnSiuGVyNNZ",
        "outputId": "8d76decc-fb99-41c2-9a83-c9e476b753d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ce9ea08be334953b6bf999a6fe6ada5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97753a604737418395aff4c108ffa887"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3476f43db6549b79852ca7dd734f1d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c1fc49d373743f091da47dddf358e8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43c2af1429dc43e38ec9c15855be21d8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "import numpy as np\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "    if predictions.ndim == 3:\n",
        "        predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute ROUGE\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    result = {key: value * 100 for key, value in result.items()}\n",
        "\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "fQUlWrFAyQqX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "81b078190f2843e4af37469a63d803c4",
            "cf5b880f9141418c89efd3667d3b5bb4",
            "a5ffe2f072af4a7dabcca6996af438ee",
            "ab22df832a874879b8517f8153c3a053",
            "6c57c76f34e04087bc22f270a91ffaec",
            "7416145b420c422ca8bae797b061fcf7",
            "d937c5ad91a94a6186b8b4aaf96bf6f5",
            "9cead67a81c74e6b969e697b43133d12",
            "3d201453052b41f184767183a51c97f4",
            "7c09511ed62d4acbb96b0f048ae7a649",
            "2019bedb07da41019b080ab787625e26"
          ]
        },
        "outputId": "e3115fee-d220-4920-dcf3-198cfc03a834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81b078190f2843e4af37469a63d803c4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./t5-summarizer\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    # logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441,
          "referenced_widgets": [
            "726c835ada9f477a9e4e85d64dc42eb1",
            "4b14e0a54e624123b9919de6a3ee4047",
            "13dee946b6a44b59a04d468272cfcc51",
            "868ce64d9b1a40a7a5cbd164af2495f0",
            "8d36c788740f4eaf811d7adc7722b8a2",
            "9f21b0a0781d4fb19787bf45a07c36fd",
            "6aa71f13cd114f53a9502589b3f98b0a",
            "a467ca61a19945659501666c658ea576",
            "ae6320964231418283f1d31fdb17bdc8",
            "af2e735997654cce9b5364ba0720318e",
            "c024a3b34c674eb4978ad5396c50e3e5",
            "c352a70b75a843d89e86bfc10fe0d7b7",
            "82884b2f48b44b5d923636ca0cb3b0d7",
            "be46ea18bea5498e876ecaf9eb66ac65",
            "3cea227cf83e452c83e6c6374a0b7cc1",
            "558ff144f41d4965b88d2228fa8df4d0",
            "bb120e2b6f934d4f92bae1fd798e3392",
            "0777d5a0a92640f881e18607feb3be8c",
            "8d2928e6bf204542971640dc8e88aa63",
            "1e8c87501d794fa49dfb914afc088f24",
            "f55b10e76acc4d508ce16a24a359e760",
            "73fd88cf8b3a424681fcc48ceb0be919",
            "ab70e70b7a754dee82c42ce4a1546427",
            "eb23685b6a3c4886a1d162d646678f00",
            "41a1bc43ae3d41b1a4ca094f3e62194d",
            "6218b2368d90465ba9257d7b86e2942e",
            "9c7f77b30fb34b64b35e4011270a735f",
            "3a04c1aa5e61464692ecaa2b220f66df",
            "6c98c63072274136b4f43af7ae4484e6",
            "62131aeb063941afbdc5ef25ac2611ff",
            "0f1cade3f25049caa002d59f7acf2e31",
            "cf5b0f635c7645e89e2df0f47a5169e5",
            "c388e4c9cb2049e8a913441e03d007b1"
          ]
        },
        "id": "QvAbyy0tySWl",
        "outputId": "559a0c32-9c4f-4172-dbd8-9cea5afe1a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "726c835ada9f477a9e4e85d64dc42eb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c352a70b75a843d89e86bfc10fe0d7b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab70e70b7a754dee82c42ce4a1546427"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-12-0e6a51a66378>:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2700' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2700/2700 06:41, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.916200</td>\n",
              "      <td>0.463798</td>\n",
              "      <td>93.185351</td>\n",
              "      <td>88.413111</td>\n",
              "      <td>92.911248</td>\n",
              "      <td>92.949128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.498800</td>\n",
              "      <td>0.415378</td>\n",
              "      <td>94.081543</td>\n",
              "      <td>89.760538</td>\n",
              "      <td>93.755274</td>\n",
              "      <td>93.824107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.468800</td>\n",
              "      <td>0.406630</td>\n",
              "      <td>94.221716</td>\n",
              "      <td>89.997072</td>\n",
              "      <td>93.849396</td>\n",
              "      <td>93.927438</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2700, training_loss=0.5827742230450665, metrics={'train_runtime': 403.2967, 'train_samples_per_second': 6.695, 'train_steps_per_second': 6.695, 'total_flos': 274067147980800.0, 'train_loss': 0.5827742230450665, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### saving the model\n"
      ],
      "metadata": {
        "id": "z0JOCDWV55-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "trainer.save_model(\"my-t5-summary-model\")\n",
        "tokenizer.save_pretrained(\"my-t5-summary-model\")\n",
        "\n",
        "shutil.make_archive('my-t5-summary-model', 'zip', 'my-t5-summary-model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iihb2dfSyYkv",
        "outputId": "70a9966d-1968-413b-d9b1-9dbf6d4fcd97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/my-t5-summary-model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on a sample"
      ],
      "metadata": {
        "id": "Pq7ZnlGc6BZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = load(\"rouge\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"my-t5-summary-model\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"my-t5-summary-model\")\n",
        "text = dataset['test'][0]['input_text']\n",
        "refer = dataset['test'][0]['target_text']\n",
        "\n",
        "\n",
        "inputs = tokenizer(text, max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "decoded_input_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "\n",
        "# Summarize the input text using the model and tokenizer\n",
        "summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\n",
        "summary = summarizer(decoded_input_text, max_length=1024, min_length=50, do_sample=False)\n",
        "\n",
        "generated_summary = summary[0]['summary_text']\n",
        "\n",
        "rouge_scores = rouge.compute(predictions=[generated_summary], references=[refer])\n",
        "\n",
        "# Print ROUGE scores\n",
        "print(\"ROUGE Scores:\", rouge_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzIQGoQxzoQr",
        "outputId": "ff2d1899-27a2-41af-90d8-ab90ba46c950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
            "Your max_length is set to 1024, but your input_length is only 514. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=257)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores: {'rouge1': np.float64(0.7385892116182573), 'rouge2': np.float64(0.6861924686192469), 'rougeL': np.float64(0.7385892116182573), 'rougeLsum': np.float64(0.7385892116182573)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_9DDYJd3WM2",
        "outputId": "39e91f20-0979-4bff-9168-905b3d841452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': 'The Nystrom method is an efficient technique to speed up large-scale learning applications by generating low-rank approximations. Cru- cial to the performance of this technique is the assumption that a matrix can be well approximated by working exclusively with a subset of its columns. In this work we re- late this assumption to the concept of ma- trix coherence and connect matrix coh\u00e9rence to the performances of the Nystrom Method. Making use of related work in the compressed sensing and the matrix completion literature, we then present empirical results that corroborate these theoretical bounds. We'}]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-cpu  sentence-transformers spacy"
      ],
      "metadata": {
        "id": "1miIc9ksZBF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79818f74-20bc-4673-a206-69dbfa1302d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQe0eYvLoM-z",
        "outputId": "f6b489c6-299d-4a03-b5c8-3c9051edbd32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.4.0\n",
            "\u001b[38;5;2m\u2714 Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m\u26a0 Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Implementation for user query model"
      ],
      "metadata": {
        "id": "HfStJHH86J0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import torch\n",
        "import spacy\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ],
      "metadata": {
        "id": "nCNWUHN6oPqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### uploading the paper"
      ],
      "metadata": {
        "id": "CpDcOtGK6nee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# \u2705 Upload your research paper (PDF)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "l8E6gUZjoRmB",
        "outputId": "10b23a46-cfa0-4137-e96e-36cf244038c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d597f085-f1f0-4536-8126-b8dd5bf4f4fb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d597f085-f1f0-4536-8126-b8dd5bf4f4fb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving An_Ocular_Feature-Based_Novel_Biomarker_Determination_for_Glaucoma_Diagnosis_Using_Supervised_Machine_Learning_and_Fundus_Imaging.pdf to An_Ocular_Feature-Based_Novel_Biomarker_Determination_for_Glaucoma_Diagnosis_Using_Supervised_Machine_Learning_and_Fundus_Imaging.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating summary using the fine tuned model\n"
      ],
      "metadata": {
        "id": "hyZnuLVF6qxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "reader = PdfReader(pdf_path)\n",
        "raw_text = \"\"\n",
        "for page in reader.pages:\n",
        "    raw_text += page.extract_text() + \"\\n\"\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.encode('ascii', 'ignore').decode()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'(References|Bibliography|Appendix).*$', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text.strip()\n",
        "text = preprocess_text(raw_text)\n",
        "\n",
        "summarizer = pipeline('summarization', model = 'my-t5-summary-model')\n",
        "summary = summarizer(text, max_length=1024, min_length=50,do_sample=False)\n",
        "generated_summary = summary[0]['summary_text']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp_g45Jeo-l4",
        "outputId": "06fd1274-e849-4205-849c-87618312ffd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4293 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "CO_y9fVUpvQD",
        "outputId": "fe4382fd-c782-4938-dc27-5802724b89b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The proposed feature set is investigated by combining both structural and nonstructural features, which act as a novel glaucoma biomarker for diagnosis and improving automated systems for ophthalmologists. In this work, a reduced, powerful 60 feature set was found to outperform all SOTA approaches with an accuracy of 85.42% and low computational time of 32s, respectively. However, relying on CDR alone may be considered as the best option for the binary classification of retinal images as healthy or healthy eyes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DIVIDING LONG TEXT INTO CHUNKS OF 512 TOKENS\n"
      ],
      "metadata": {
        "id": "ILwvyo-V7BW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Function to chunk the text into smaller chunks, considering token limits\n",
        "def chunk_text(text, max_tokens=512):\n",
        "    sentences = text.split('. ')  # Split based on sentence end (you can adjust the logic here)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Check if adding the sentence exceeds the max tokens\n",
        "        new_chunk = current_chunk + sentence + \". \"\n",
        "        if len(tokenizer.encode(new_chunk)) <= max_tokens:\n",
        "            current_chunk = new_chunk\n",
        "        else:\n",
        "            # If chunk exceeds max tokens, save the current chunk and start a new one\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \". \"\n",
        "\n",
        "    if current_chunk.strip():\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "chunks = chunk_text(raw_text, max_tokens=512)\n",
        "\n"
      ],
      "metadata": {
        "id": "1L3ExkLgoheQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### USING SENTENCE TRANSFORMER MODEL TO CREATE EMBEDDINGS AND INDEX ASSIGNED IN  FAISS\n",
        "\n"
      ],
      "metadata": {
        "id": "jU_vO2Lh7Uja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embedder.encode(chunks)\n",
        "\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)"
      ],
      "metadata": {
        "id": "6pUAJL7IooTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###USER-QUERY MODEL\n"
      ],
      "metadata": {
        "id": "MijWP-CC72SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "def retrieve_relevant_chunk(question, index, chunks, embedder):\n",
        "\n",
        "    question_embedding = embedder.encode([question])\n",
        "\n",
        "    D, I = index.search(np.array(question_embedding).astype(np.float32), k=1)  # Get the top 1 chunk\n",
        "\n",
        "\n",
        "    relevant_chunk = chunks[I[0][0]]\n",
        "    return relevant_chunk\n",
        "\n",
        "def get_answer_from_question(question, index, chunks, embedder, qa_pipeline):\n",
        "    relevant_chunk = retrieve_relevant_chunk(question, index, chunks, embedder)\n",
        "\n",
        "    input_text = f\"question: {question} context: {relevant_chunk}\"\n",
        "\n",
        "    answer = qa_pipeline(input_text, max_length=1024, min_length=50, num_return_sequences=1, do_sample=True, top_p=0.95, top_k=60)\n",
        "    return answer[0]['generated_text']\n",
        "\n",
        "question = input(\"Ask a question about the document: \")\n",
        "answer = get_answer_from_question(question, index, chunks, embedder, qa_pipeline)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCgnDBHRoqvD",
        "outputId": "1ca65d69-e7fc-4cb6-b162-77158c9ffb5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question about the document: How is the weather today?\n",
            "Answer: Currently, the proposed feature set, using the Extra Tree classifier, achieves the highest accuracy of 85.42% compared to related Authorized licensed use limited to: PES University Bengaluru. Downloaded on February 04,2025 at 08:58:06 UTC from IEEE Xplore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jqT-jtkIpGKZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}